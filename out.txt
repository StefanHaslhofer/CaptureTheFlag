(MultiAgentEnvRunner pid=37492) {'red_0': -80.39999999999974, 'red_1': -56.10000000000051, 'blue_0': -88.59999999999928, 'blue_1': -43.200000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 656
(MultiAgentEnvRunner pid=37492) {'red_0': -51.80000000000046, 'red_1': -69.1000000000003, 'blue_0': -68.70000000000034, 'blue_1': -43.40000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1005
(MultiAgentEnvRunner pid=37492) {'red_0': -54.800000000000495, 'red_1': -42.10000000000036, 'blue_0': -47.600000000000385, 'blue_1': -65.60000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1054
(MultiAgentEnvRunner pid=37492) {'red_0': -40.70000000000034, 'red_1': -44.60000000000036, 'blue_0': -68.70000000000033, 'blue_1': -67.6000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -66.30000000000048, 'red_1': -63.60000000000061, 'blue_0': -74.20000000000007, 'blue_1': -61.10000000000057}
ITERATION 0: reward=-239.64000000000124, metadata={'num_env_steps_sampled_lifetime': 6000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003311248138410567, 'timers': {'connectors': {'batch_individual_items': 9.741915504564421e-05, 'add_states_from_episodes_to_batch': 7.0720367715162555e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2340498594856429e-05, 'numpy_to_tensor': 6.950424614413692e-05, 'agent_to_module_mapping': 8.070468459183475e-06, 'add_observations_from_episodes_to_batch': 3.7995959187525554e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008958428548174612, 'timers': {'connectors': {'get_actions': 0.0004665548197054605, 'un_batch_to_individual_items': 6.570904494882714e-05, 'tensor_to_numpy': 0.00012025702228230722, 'module_to_agent_unmapping': 6.817633897806694e-06, 'normalize_and_clip_actions': 7.308794574131889e-05, 'listify_data_for_vector_env': 2.4529452392676385e-05, 'remove_single_ts_time_rank_from_batch': 2.4820916377271972e-06}}}, 'sample': 91.42771870002616, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -55.100000000000435, 'blue_0': -69.56000000000009, 'blue_1': -56.18000000000045, 'red_0': -58.800000000000296}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 0.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 6000.0, 'blue_0': 6000.0, 'blue_1': 6000.0, 'red_0': 6000.0}, 'connector_pipeline_timer': 0.0005224000196903944, 'env_reset_timer': 0.00036589999217540026, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -55.100000000000435, 'blue_policy': -56.18000000000045}, 'num_module_steps_sampled_lifetime': {'red_policy': 12000.0, 'blue_policy': 12000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': 8.36000544950366e-05, 'add_states_from_episodes_to_batch': 7.900060154497623e-06, 'add_time_dim_to_batch_and_zero_pad': 1.7600017599761486e-05, 'numpy_to_tensor': 9.019998833537102e-05, 'agent_to_module_mapping': 1.009996049106121e-05, 'add_observations_from_episodes_to_batch': 5.140004213899374e-05}}, 'env_step_timer': 0.0003132666459852481, 'episode_return_mean': -239.64000000000124, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -210.10000000000178, 'episode_duration_sec_mean': 18.154580459976568, 'episode_return_min': -268.2999999999999, 'rlmodule_inference_timer': 0.012997154452166076, 'num_episodes_lifetime': 5.0, 'episode_len_min': 1200, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.123947795137415, 'throughput_since_last_restore': 14.12394761892353}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 406.65551970002707, 'restore_env_runners': 1.5199999324977398e-05, 'training_step': 406.65489930007607, 'env_runner_sampling_timer': 91.62589039991144, 'learner_update_timer': 314.978009799961, 'synch_weights': 0.033098500105552375}, 'env_runners': {'num_env_steps_sampled_lifetime': 6000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003311248138410567, 'timers': {'connectors': {'batch_individual_items': 9.741915504564421e-05, 'add_states_from_episodes_to_batch': 7.0720367715162555e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2340498594856429e-05, 'numpy_to_tensor': 6.950424614413692e-05, 'agent_to_module_mapping': 8.070468459183475e-06, 'add_observations_from_episodes_to_batch': 3.7995959187525554e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008958428548174612, 'timers': {'connectors': {'get_actions': 0.0004665548197054605, 'un_batch_to_individual_items': 6.570904494882714e-05, 'tensor_to_numpy': 0.00012025702228230722, 'module_to_agent_unmapping': 6.817633897806694e-06, 'normalize_and_clip_actions': 7.308794574131889e-05, 'listify_data_for_vector_env': 2.4529452392676385e-05, 'remove_single_ts_time_rank_from_batch': 2.4820916377271972e-06}}}, 'sample': 91.42771870002616, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -55.100000000000435, 'blue_0': -69.56000000000009, 'blue_1': -56.18000000000045, 'red_0': -58.800000000000296}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 0.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 6000.0, 'blue_0': 6000.0, 'blue_1': 6000.0, 'red_0': 6000.0}, 'connector_pipeline_timer': 0.0005224000196903944, 'env_reset_timer': 0.00036589999217540026, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -55.100000000000435, 'blue_policy': -56.18000000000045}, 'num_module_steps_sampled_lifetime': {'red_policy': 12000.0, 'blue_policy': 12000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': 8.36000544950366e-05, 'add_states_from_episodes_to_batch': 7.900060154497623e-06, 'add_time_dim_to_batch_and_zero_pad': 1.7600017599761486e-05, 'numpy_to_tensor': 9.019998833537102e-05, 'agent_to_module_mapping': 1.009996049106121e-05, 'add_observations_from_episodes_to_batch': 5.140004213899374e-05}}, 'env_step_timer': 0.0003132666459852481, 'episode_return_mean': -239.64000000000124, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -210.10000000000178, 'episode_duration_sec_mean': 18.154580459976568, 'episode_return_min': -268.2999999999999, 'rlmodule_inference_timer': 0.012997154452166076, 'num_episodes_lifetime': 5.0, 'episode_len_min': 1200, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.123947795137415, 'throughput_since_last_restore': 14.12394761892353}}, 'learners': {'red_policy': {'policy_loss': -0.20272260904312134, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.07678258419036865, 'num_trainable_parameters': 2218855.0, 'mean_kl_loss': 0.04359150677919388, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 1.0, 'num_module_steps_trained_lifetime': 360320.0, 'curr_entropy_coeff': 0.04991, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00029939999999999996, 'vf_explained_var': 0.9817050099372864, 'curr_kl_coeff': 0.30000001192092896, 'total_loss': -0.2046414166688919, 'entropy': 1.7483940124511719, 'vf_loss_unclipped': 0.07678258419036865, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 848.1889691087799, 'throughput_since_last_restore': 848.1889675116988}}, 'blue_policy': {'weights_seq_no': 1.0, 'num_module_steps_trained_lifetime': 360320.0, 'curr_entropy_coeff': 0.04991, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00029939999999999996, 'vf_explained_var': -0.007794976234436035, 'curr_kl_coeff': 0.30000001192092896, 'vf_loss_unclipped': 89.19186401367188, 'total_loss': 9.789759635925293, 'entropy': 1.7437043190002441, 'policy_loss': -0.13266107439994812, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': 2218855.0, 'mean_kl_loss': 0.04803396016359329, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 848.1891214516537, 'throughput_since_last_restore': 848.1891200540039}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 8.70006624609232e-06, 'batch_individual_items': 0.842642999952659, 'add_time_dim_to_batch_and_zero_pad': 3.060000017285347e-05, 'numpy_to_tensor': 0.14241450000554323, 'add_observations_from_episodes_to_batch': 0.0002587999915704131, 'agent_to_module_mapping': 0.02572660008445382, 'add_one_ts_to_episodes_and_truncate': 0.3656352999387309, 'add_columns_from_episodes_to_train_batch': 0.501407100004144, 'general_advantage_estimation': 41.39055020001251}}, 'connector_pipeline_timer': 43.269503099960275}, 'num_module_steps_trained_lifetime': 720640.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 16890000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': 0.0, 'num_trainable_parameters': 4437710.0, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 39758.89695482761, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 39758.87278937372, 'throughput_since_last_restore': 39758.8726302662}, 'num_module_steps_trained_throughput': 1696.378530417567, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1696.3784793039654, 'throughput_since_last_restore': 1696.3784761093364}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 6000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2026-01-25_15-25-55', 'timestamp': 1769351155, 'time_this_iter_s': 406.67349910736084, 'time_total_s': 406.67349910736084, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 406.67349910736084, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 18.235456110154903, 'ram_util_percent': 80.09191049913942}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 616
(MultiAgentEnvRunner pid=37492) {'red_0': -41.300000000000345, 'red_1': -53.500000000000476, 'blue_0': -48.300000000000395, 'blue_1': -40.90000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -54.60000000000049, 'red_1': -59.20000000000056, 'blue_0': -54.400000000000496, 'blue_1': -42.70000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -89.4999999999992, 'red_1': -45.70000000000037, 'blue_0': -49.20000000000043, 'blue_1': -52.200000000000465}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 431
(MultiAgentEnvRunner pid=37492) {'red_0': -68.90000000000032, 'red_1': -71.60000000000016, 'blue_0': -52.7000000000005, 'blue_1': -46.50000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 505
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 925
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 925
(MultiAgentEnvRunner pid=37492) {'red_0': -69.20000000000037, 'red_1': -45.30000000000045, 'blue_0': -66.30000000000047, 'blue_1': -69.70000000000024}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 1: reward=-224.34000000000137, metadata={'num_env_steps_sampled_lifetime': 12000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003654781341257107, 'timers': {'connectors': {'batch_individual_items': 0.00011216482048349479, 'add_states_from_episodes_to_batch': 6.988875621771313e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3699189158946026e-05, 'numpy_to_tensor': 7.588040864438527e-05, 'agent_to_module_mapping': 8.92260730244472e-06, 'add_observations_from_episodes_to_batch': 4.1524848186321564e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009437371427252033, 'timers': {'connectors': {'get_actions': 0.0004816484211553407, 'un_batch_to_individual_items': 7.020462322890943e-05, 'tensor_to_numpy': 0.00012912407633027627, 'module_to_agent_unmapping': 7.200622421753672e-06, 'normalize_and_clip_actions': 7.933766762278048e-05, 'listify_data_for_vector_env': 2.648424352650983e-05, 'remove_single_ts_time_rank_from_batch': 2.5203704401485636e-06}}}, 'sample': 93.95442059997004, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -55.0600000000004, 'blue_0': -54.18000000000046, 'blue_1': -50.400000000000354, 'red_0': -64.70000000000014}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 1.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 12000.0, 'blue_0': 12000.0, 'blue_1': 12000.0, 'red_0': 12000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -55.0600000000004, 'blue_policy': -50.400000000000354}, 'num_module_steps_sampled_lifetime': {'red_policy': 24000.0, 'blue_policy': 24000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003386639120650772, 'episode_return_mean': -224.34000000000137, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -184.00000000000153, 'episode_duration_sec_mean': 18.623135579982772, 'episode_return_min': -250.50000000000153, 'rlmodule_inference_timer': 0.01390337006900556, 'num_episodes_lifetime': 10.0, 'episode_len_min': 1200, 'time_between_sampling': 315.4647602000041, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.461769925780516, 'throughput_since_last_restore': 14.762556710913524}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -44.10000000000034, 'red_1': -38.900000000000276, 'blue_0': -67.10000000000042, 'blue_1': -55.3000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -56.70000000000052, 'red_1': -52.80000000000046, 'blue_0': -45.200000000000365, 'blue_1': -43.200000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -56.4000000000005, 'red_1': -49.00000000000042, 'blue_0': -44.20000000000036, 'blue_1': -41.00000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -46.10000000000037, 'red_1': -48.000000000000405, 'blue_0': -44.000000000000355, 'blue_1': -58.00000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 432
(MultiAgentEnvRunner pid=37492) {'red_0': -99.49999999999865, 'red_1': -88.39999999999922, 'blue_0': -50.200000000000415, 'blue_1': -36.3000000000002}
ITERATION 2: reward=-212.88000000000102, metadata={'num_env_steps_sampled_lifetime': 18000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003675118392637479, 'timers': {'connectors': {'batch_individual_items': 0.0001172800029237112, 'add_states_from_episodes_to_batch': 6.729514756779878e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3700975252871769e-05, 'numpy_to_tensor': 7.621539751696313e-05, 'agent_to_module_mapping': 9.335052082175362e-06, 'add_observations_from_episodes_to_batch': 4.157377180774463e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009103651410507798, 'timers': {'connectors': {'get_actions': 0.000467198639938554, 'un_batch_to_individual_items': 6.845277003092544e-05, 'tensor_to_numpy': 0.00012400922315956324, 'module_to_agent_unmapping': 6.624036028464078e-06, 'normalize_and_clip_actions': 7.531464520640696e-05, 'listify_data_for_vector_env': 2.5192254201693466e-05, 'remove_single_ts_time_rank_from_batch': 2.4737683662937245e-06}}}, 'sample': 89.0459815999493, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -55.42000000000015, 'blue_0': -50.14000000000038, 'blue_1': -46.76000000000038, 'red_0': -60.56000000000007}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 2.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 18000.0, 'blue_0': 18000.0, 'blue_1': 18000.0, 'red_0': 18000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -55.42000000000015, 'blue_policy': -46.76000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 36000.0, 'blue_policy': 36000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00031933402741431436, 'episode_return_mean': -212.88000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -190.6000000000016, 'episode_duration_sec_mean': 17.64778290002141, 'episode_return_min': -274.3999999999985, 'rlmodule_inference_timer': 0.013001448291447979, 'num_episodes_lifetime': 15.0, 'episode_len_min': 1200, 'time_between_sampling': 293.9058796999743, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.331329681239186, 'throughput_since_last_restore': 15.250839824944427}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 923
(MultiAgentEnvRunner pid=37492) {'red_0': -37.700000000000294, 'red_1': -52.70000000000047, 'blue_0': -52.700000000000465, 'blue_1': -58.70000000000055}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 852
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1091
(MultiAgentEnvRunner pid=37492) {'red_0': -52.20000000000046, 'red_1': -53.20000000000049, 'blue_0': -50.40000000000046, 'blue_1': -39.700000000000294}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 561
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 729
(MultiAgentEnvRunner pid=37492) {'red_0': -52.80000000000047, 'red_1': -62.80000000000058, 'blue_0': -45.4000000000004, 'blue_1': -35.80000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 501
(MultiAgentEnvRunner pid=37492) {'red_0': -50.40000000000043, 'red_1': -45.00000000000039, 'blue_0': -52.20000000000045, 'blue_1': -70.40000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 735
(MultiAgentEnvRunner pid=37492) {'red_0': -54.900000000000496, 'red_1': -51.10000000000047, 'blue_0': -49.70000000000042, 'blue_1': -41.40000000000031}
ITERATION 3: reward=-201.84000000000168, metadata={'num_env_steps_sampled_lifetime': 24000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003720096395056129, 'timers': {'connectors': {'batch_individual_items': 0.00011316478771392904, 'add_states_from_episodes_to_batch': 6.831949026416425e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4833181213739554e-05, 'numpy_to_tensor': 8.141009130583332e-05, 'agent_to_module_mapping': 8.845416355072177e-06, 'add_observations_from_episodes_to_batch': 4.229308132151866e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009574172785056157, 'timers': {'connectors': {'get_actions': 0.000494067835072737, 'un_batch_to_individual_items': 7.092642495945286e-05, 'tensor_to_numpy': 0.00013089079387453386, 'module_to_agent_unmapping': 6.951004763009993e-06, 'normalize_and_clip_actions': 7.744624527498662e-05, 'listify_data_for_vector_env': 2.575613188600837e-05, 'remove_single_ts_time_rank_from_batch': 2.5675477383528283e-06}}}, 'sample': 93.5587996999966, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -52.96000000000048, 'blue_0': -50.08000000000044, 'blue_1': -49.200000000000344, 'red_0': -49.60000000000043}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 3.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 24000.0, 'blue_0': 24000.0, 'blue_1': 24000.0, 'red_0': 24000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -52.96000000000048, 'blue_policy': -49.200000000000344}, 'num_module_steps_sampled_lifetime': {'red_policy': 48000.0, 'blue_policy': 48000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033700184605930933, 'episode_return_mean': -201.84000000000168, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -195.5000000000017, 'episode_duration_sec_mean': 18.529412640002555, 'episode_return_min': -218.00000000000153, 'rlmodule_inference_timer': 0.01582623776894675, 'num_episodes_lifetime': 20.0, 'episode_len_min': 1200, 'time_between_sampling': 278.3468405000167, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.487434156718955, 'throughput_since_last_restore': 15.309284100265723}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -47.8000000000004, 'red_1': -43.90000000000035, 'blue_0': -70.60000000000024, 'blue_1': -51.00000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -54.000000000000476, 'red_1': -52.90000000000047, 'blue_0': -45.20000000000037, 'blue_1': -40.400000000000304}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -57.60000000000053, 'red_1': -55.400000000000496, 'blue_0': -58.00000000000053, 'blue_1': -41.50000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -45.60000000000037, 'red_1': -34.900000000000226, 'blue_0': -49.30000000000042, 'blue_1': -59.700000000000564}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -45.70000000000037, 'red_1': -62.6000000000006, 'blue_0': -57.20000000000053, 'blue_1': -46.70000000000039}
ITERATION 4: reward=-204.00000000000168, metadata={'num_env_steps_sampled_lifetime': 30000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00036821518914594345, 'timers': {'connectors': {'batch_individual_items': 0.00010595957419059559, 'add_states_from_episodes_to_batch': 6.84623698477638e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4319196361161316e-05, 'numpy_to_tensor': 7.839420787977368e-05, 'agent_to_module_mapping': 8.842142094586946e-06, 'add_observations_from_episodes_to_batch': 4.487819624530974e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000993662992970332, 'timers': {'connectors': {'get_actions': 0.0005138571887347885, 'un_batch_to_individual_items': 7.448122199399862e-05, 'tensor_to_numpy': 0.00013585085998012031, 'module_to_agent_unmapping': 7.289234460355951e-06, 'normalize_and_clip_actions': 7.972073542632246e-05, 'listify_data_for_vector_env': 2.6494714690986555e-05, 'remove_single_ts_time_rank_from_batch': 2.648963552684943e-06}}}, 'sample': 96.34534719993826, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -49.940000000000424, 'blue_0': -56.06000000000042, 'blue_1': -47.8600000000004, 'red_0': -50.14000000000043}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 4.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 30000.0, 'blue_0': 30000.0, 'blue_1': 30000.0, 'red_0': 30000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -49.940000000000424, 'blue_policy': -47.8600000000004}, 'num_module_steps_sampled_lifetime': {'red_policy': 60000.0, 'blue_policy': 60000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003572740359631742, 'episode_return_mean': -204.00000000000168, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -189.5000000000016, 'episode_duration_sec_mean': 19.145492219994775, 'episode_return_min': -213.30000000000143, 'rlmodule_inference_timer': 0.014450975513998226, 'num_episodes_lifetime': 25.0, 'episode_len_min': 1200, 'time_between_sampling': 293.85387899994384, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.430539482309483, 'throughput_since_last_restore': 15.125058918665285}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 311
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 503
(MultiAgentEnvRunner pid=37492) {'red_0': -51.60000000000046, 'red_1': -70.20000000000026, 'blue_0': -51.20000000000045, 'blue_1': -48.40000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -55.70000000000051, 'red_1': -48.80000000000041, 'blue_0': -36.80000000000025, 'blue_1': -43.40000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -49.80000000000043, 'red_1': -43.700000000000344, 'blue_0': -37.50000000000026, 'blue_1': -46.20000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -39.9000000000003, 'red_1': -40.2000000000003, 'blue_0': -42.70000000000033, 'blue_1': -48.900000000000425}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -63.90000000000061, 'red_1': -54.50000000000049, 'blue_0': -44.20000000000036, 'blue_1': -52.90000000000047}
ITERATION 5: reward=-194.10000000000156, metadata={'num_env_steps_sampled_lifetime': 36000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035012587426648236, 'timers': {'connectors': {'batch_individual_items': 0.00010298580666860496, 'add_states_from_episodes_to_batch': 6.671351267859032e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3141410751745584e-05, 'numpy_to_tensor': 7.540961822083986e-05, 'agent_to_module_mapping': 8.401471884480485e-06, 'add_observations_from_episodes_to_batch': 4.248896320428275e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009191984548757752, 'timers': {'connectors': {'get_actions': 0.00047191910503094754, 'un_batch_to_individual_items': 7.037725478468317e-05, 'tensor_to_numpy': 0.00012581106958370613, 'module_to_agent_unmapping': 6.6533662804273436e-06, 'normalize_and_clip_actions': 7.527290770035171e-05, 'listify_data_for_vector_env': 2.468952128204467e-05, 'remove_single_ts_time_rank_from_batch': 2.4335315759703677e-06}}}, 'sample': 90.93893860001117, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -51.48000000000036, 'blue_0': -42.48000000000033, 'blue_1': -47.96000000000041, 'red_0': -52.18000000000046}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 5.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 36000.0, 'blue_0': 36000.0, 'blue_1': 36000.0, 'red_0': 36000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -51.48000000000036, 'blue_policy': -47.96000000000041}, 'num_module_steps_sampled_lifetime': {'red_policy': 72000.0, 'blue_policy': 72000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033146358553912825, 'episode_return_mean': -194.10000000000156, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -171.70000000000135, 'episode_duration_sec_mean': 18.059342260006815, 'episode_return_min': -221.4000000000016, 'rlmodule_inference_timer': 0.013519098836557527, 'num_episodes_lifetime': 30.0, 'episode_len_min': 1200, 'time_between_sampling': 319.43733300000895, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.667025677646008, 'throughput_since_last_restore': 15.046718720588933}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -50.60000000000045, 'red_1': -55.60000000000051, 'blue_0': -42.80000000000033, 'blue_1': -49.20000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 858
(MultiAgentEnvRunner pid=37492) {'red_0': -69.80000000000022, 'red_1': -68.5000000000003, 'blue_0': -43.80000000000042, 'blue_1': -31.400000000000393}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -50.90000000000044, 'red_1': -38.50000000000028, 'blue_0': -48.400000000000404, 'blue_1': -59.100000000000556}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 587
(MultiAgentEnvRunner pid=37492) {'red_0': -60.200000000000564, 'red_1': -56.400000000000546, 'blue_0': -57.70000000000053, 'blue_1': -59.00000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -52.10000000000045, 'red_1': -68.3000000000004, 'blue_0': -38.400000000000276, 'blue_1': -40.70000000000031}
ITERATION 6: reward=-208.28000000000165, metadata={'num_env_steps_sampled_lifetime': 42000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034150431221026444, 'timers': {'connectors': {'batch_individual_items': 0.00010241686479331465, 'add_states_from_episodes_to_batch': 6.546191095561839e-06, 'add_time_dim_to_batch_and_zero_pad': 1.270291529579417e-05, 'numpy_to_tensor': 7.0577603891722e-05, 'agent_to_module_mapping': 8.215601504242472e-06, 'add_observations_from_episodes_to_batch': 3.968732968585313e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008962644835706397, 'timers': {'connectors': {'get_actions': 0.0004652790452033538, 'un_batch_to_individual_items': 6.705322548212304e-05, 'tensor_to_numpy': 0.00012008759641821259, 'module_to_agent_unmapping': 6.9086084347582225e-06, 'normalize_and_clip_actions': 7.387900167793798e-05, 'listify_data_for_vector_env': 2.400219779536401e-05, 'remove_single_ts_time_rank_from_batch': 2.3580368522248796e-06}}}, 'sample': 93.21885449998081, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -57.46000000000041, 'blue_0': -46.22000000000039, 'blue_1': -47.88000000000044, 'red_0': -56.720000000000425}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 6.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 42000.0, 'blue_0': 42000.0, 'blue_1': 42000.0, 'red_0': 42000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -57.46000000000041, 'blue_policy': -47.88000000000044}, 'num_module_steps_sampled_lifetime': {'red_policy': 84000.0, 'blue_policy': 84000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003211019380928772, 'episode_return_mean': -208.28000000000165, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -196.90000000000168, 'episode_duration_sec_mean': 18.4807770999847, 'episode_return_min': -233.30000000000217, 'rlmodule_inference_timer': 0.01313071056938751, 'num_episodes_lifetime': 35.0, 'episode_len_min': 1200, 'time_between_sampling': 318.1542395000579, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.488944948530671, 'throughput_since_last_restore': 14.964402376597496}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 499
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 657
(MultiAgentEnvRunner pid=37492) {'red_0': -56.300000000000466, 'red_1': -57.00000000000049, 'blue_0': -45.30000000000041, 'blue_1': -32.40000000000018}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 550
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 678
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1104
(MultiAgentEnvRunner pid=37492) {'red_0': -70.90000000000016, 'red_1': -74.09999999999987, 'blue_0': -47.70000000000046, 'blue_1': -55.800000000000544}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1041
(MultiAgentEnvRunner pid=37492) {'red_0': -56.200000000000436, 'red_1': -48.50000000000034, 'blue_0': -29.700000000000358, 'blue_1': -33.60000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 388
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 595
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 939
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 974
(MultiAgentEnvRunner pid=37492) {'red_0': -61.60000000000047, 'red_1': -55.70000000000041, 'blue_0': -26.500000000000153, 'blue_1': -37.80000000000054}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 417
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 618
(MultiAgentEnvRunner pid=37492) {'red_0': -49.00000000000041, 'red_1': -52.700000000000465, 'blue_0': -52.700000000000465, 'blue_1': -47.90000000000042}
ITERATION 7: reward=-198.28000000000148, metadata={'num_env_steps_sampled_lifetime': 48000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033239838804546444, 'timers': {'connectors': {'batch_individual_items': 0.0001003180735567815, 'add_states_from_episodes_to_batch': 6.378432823925157e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2676420569622404e-05, 'numpy_to_tensor': 6.924891473944918e-05, 'agent_to_module_mapping': 8.159192649105631e-06, 'add_observations_from_episodes_to_batch': 3.9017449354778636e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008749902579511595, 'timers': {'connectors': {'get_actions': 0.00045080987270953787, 'un_batch_to_individual_items': 6.546069036339025e-05, 'tensor_to_numpy': 0.00011940891325161475, 'module_to_agent_unmapping': 6.541714779327906e-06, 'normalize_and_clip_actions': 7.150816516627259e-05, 'listify_data_for_vector_env': 2.4056284562026234e-05, 'remove_single_ts_time_rank_from_batch': 2.3859534198363864e-06}}}, 'sample': 92.08075089997146, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -57.60000000000032, 'blue_0': -40.38000000000037, 'blue_1': -41.5000000000004, 'red_0': -58.80000000000039}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 7.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 48000.0, 'blue_0': 48000.0, 'blue_1': 48000.0, 'red_0': 48000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -57.60000000000032, 'blue_policy': -41.5000000000004}, 'num_module_steps_sampled_lifetime': {'red_policy': 96000.0, 'blue_policy': 96000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032395061622526904, 'episode_return_mean': -198.28000000000148, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -168.00000000000142, 'episode_duration_sec_mean': 18.272542240004988, 'episode_return_min': -248.50000000000105, 'rlmodule_inference_timer': 0.012643390645365895, 'num_episodes_lifetime': 40.0, 'episode_len_min': 1200, 'time_between_sampling': 320.8893504999578, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.898073406917156, 'throughput_since_last_restore': 15.075058527499069}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 596
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 690
(MultiAgentEnvRunner pid=37492) {'red_0': -62.10000000000056, 'red_1': -49.30000000000038, 'blue_0': -30.100000000000158, 'blue_1': -39.70000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 485
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 525
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1175
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1175
(MultiAgentEnvRunner pid=37492) {'red_0': -44.60000000000036, 'red_1': -28.400000000000063, 'blue_0': -50.5000000000004, 'blue_1': -53.70000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 331
(MultiAgentEnvRunner pid=37492) {'red_0': -51.90000000000045, 'red_1': -67.30000000000041, 'blue_0': -48.400000000000404, 'blue_1': -45.700000000000365}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 698
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 782
(MultiAgentEnvRunner pid=37492) {'red_0': -50.00000000000041, 'red_1': -48.900000000000404, 'blue_0': -45.60000000000043, 'blue_1': -48.60000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 990
(MultiAgentEnvRunner pid=37492) {'red_0': -43.50000000000037, 'red_1': -44.40000000000036, 'blue_0': -52.90000000000045, 'blue_1': -51.70000000000045}
ITERATION 8: reward=-191.4600000000015, metadata={'num_env_steps_sampled_lifetime': 54000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003417090729078751, 'timers': {'connectors': {'batch_individual_items': 0.00010140159222206944, 'add_states_from_episodes_to_batch': 6.8955540746224106e-06, 'add_time_dim_to_batch_and_zero_pad': 1.299443645090027e-05, 'numpy_to_tensor': 7.188106535811697e-05, 'agent_to_module_mapping': 8.358408056697185e-06, 'add_observations_from_episodes_to_batch': 4.063955472159179e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008943974371669229, 'timers': {'connectors': {'get_actions': 0.00045959328529599457, 'un_batch_to_individual_items': 6.929725733583597e-05, 'tensor_to_numpy': 0.00012145394168227115, 'module_to_agent_unmapping': 6.616543802237023e-06, 'normalize_and_clip_actions': 7.35013282902084e-05, 'listify_data_for_vector_env': 2.428943469770239e-05, 'remove_single_ts_time_rank_from_batch': 2.3836236382650196e-06}}}, 'sample': 97.25665679993108, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -47.66000000000032, 'blue_0': -45.50000000000037, 'blue_1': -47.880000000000386, 'red_0': -50.42000000000043}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 8.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 54000.0, 'blue_0': 54000.0, 'blue_1': 54000.0, 'red_0': 54000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -47.66000000000032, 'blue_policy': -47.880000000000386}, 'num_module_steps_sampled_lifetime': {'red_policy': 108000.0, 'blue_policy': 108000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003290840318755848, 'episode_return_mean': -191.4600000000015, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -177.20000000000124, 'episode_duration_sec_mean': 19.300478659989313, 'episode_return_min': -213.30000000000163, 'rlmodule_inference_timer': 0.01317572267273583, 'num_episodes_lifetime': 45.0, 'episode_len_min': 1200, 'time_between_sampling': 285.33053309994284, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.441225315949737, 'throughput_since_last_restore': 15.11487443882244}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 946
(MultiAgentEnvRunner pid=37492) {'red_0': -81.1999999999996, 'red_1': -77.59999999999982, 'blue_0': -41.600000000000385, 'blue_1': -37.500000000000476}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 835
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 948
(MultiAgentEnvRunner pid=37492) {'red_0': -57.10000000000045, 'red_1': -71.30000000000011, 'blue_0': -38.30000000000034, 'blue_1': -29.300000000000335}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1101
(MultiAgentEnvRunner pid=37492) {'red_0': -72.80000000000013, 'red_1': -65.50000000000045, 'blue_0': -44.10000000000043, 'blue_1': -30.200000000000372}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 892
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 976
(MultiAgentEnvRunner pid=37492) {'red_0': -26.80000000000035, 'red_1': -45.100000000000435, 'blue_0': -57.000000000000455, 'blue_1': -61.4000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -49.900000000000425, 'red_1': -48.80000000000042, 'blue_0': -41.900000000000325, 'blue_1': -50.20000000000044}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -118.49999999999754, 'red_1': -1.2, 'blue_0': -117.99999999999757, 'blue_1': -118.39999999999755}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -116.89999999999763, 'red_1': -116.99999999999763, 'blue_0': -117.69999999999759, 'blue_1': -118.19999999999756}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-25 16:21:59,001	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
2026-01-25 16:21:59,001	WARNING algorithm.py:1537 -- No evaluation results found for this iteration. This can happen if the evaluation worker(s) is/are not healthy.
(MultiAgentEnvRunner pid=41856) {'red_0': -117.99999999999757, 'red_1': -118.89999999999752, 'blue_0': -118.19999999999756, 'blue_1': -117.99999999999757}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -118.99999999999751, 'red_1': -119.59999999999748, 'blue_0': -118.99999999999751, 'blue_1': -119.49999999999748}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.79999999999747, 'red_1': -119.79999999999747, 'blue_0': -119.89999999999746, 'blue_1': -119.89999999999746}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 9: reward=-205.52000000000126, metadata={'num_env_steps_sampled_lifetime': 60000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035375122720261617, 'timers': {'connectors': {'batch_individual_items': 0.00010694780164347926, 'add_states_from_episodes_to_batch': 6.746436674084128e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3802264285889322e-05, 'numpy_to_tensor': 7.535915855202254e-05, 'agent_to_module_mapping': 8.471402948637015e-06, 'add_observations_from_episodes_to_batch': 4.072430915696837e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009407155693112945, 'timers': {'connectors': {'get_actions': 0.0004862207366232612, 'un_batch_to_individual_items': 7.125437624033379e-05, 'tensor_to_numpy': 0.00012661939902524775, 'module_to_agent_unmapping': 6.724973493356993e-06, 'normalize_and_clip_actions': 7.762967577476216e-05, 'listify_data_for_vector_env': 2.5639087378438882e-05, 'remove_single_ts_time_rank_from_batch': 2.5134570963817272e-06}}}, 'sample': 95.14025060005952, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -61.66000000000024, 'blue_0': -44.58000000000038, 'blue_1': -41.720000000000425, 'red_0': -57.560000000000194}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 9.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 60000.0, 'blue_0': 60000.0, 'blue_1': 60000.0, 'red_0': 60000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -61.66000000000024, 'blue_policy': -41.720000000000425}, 'num_module_steps_sampled_lifetime': {'red_policy': 120000.0, 'blue_policy': 120000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035021749705931563, 'episode_return_mean': -205.52000000000126, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -190.30000000000172, 'episode_duration_sec_mean': 18.78259494001977, 'episode_return_min': -237.90000000000032, 'rlmodule_inference_timer': 0.013678659414772644, 'num_episodes_lifetime': 50.0, 'episode_len_min': 1200, 'time_between_sampling': 291.3209867000114, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 11.541853802494353, 'throughput_since_last_restore': 14.661003461816227}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 681
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 857
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 885
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 885
(MultiAgentEnvRunner pid=37492) {'red_0': -47.300000000000374, 'red_1': -50.00000000000039, 'blue_0': -32.500000000000384, 'blue_1': -49.500000000000455}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -47.5000000000004, 'red_1': -62.40000000000059, 'blue_0': -35.80000000000024, 'blue_1': -51.00000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 616
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 682
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 711
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 714
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1016
(MultiAgentEnvRunner pid=37492) {'red_0': -57.800000000000416, 'red_1': -53.20000000000036, 'blue_0': -29.100000000000104, 'blue_1': -28.400000000000087}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 420
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 766
(MultiAgentEnvRunner pid=37492) {'red_0': -35.60000000000019, 'red_1': -48.40000000000041, 'blue_0': -56.20000000000047, 'blue_1': -48.900000000000446}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -72.30000000000013, 'red_1': -66.80000000000045, 'blue_0': -42.20000000000032, 'blue_1': -40.60000000000031}
ITERATION 10: reward=-191.10000000000142, metadata={'num_env_steps_sampled_lifetime': 66000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00042606910154668865, 'timers': {'connectors': {'batch_individual_items': 0.0001864972377393016, 'add_states_from_episodes_to_batch': 7.153203591076588e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2378980725900744e-05, 'numpy_to_tensor': 7.033881018408657e-05, 'agent_to_module_mapping': 8.263649986817423e-06, 'add_observations_from_episodes_to_batch': 3.944387746809935e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008941021929635047, 'timers': {'connectors': {'get_actions': 0.00046102279671786703, 'un_batch_to_individual_items': 6.753727647170306e-05, 'tensor_to_numpy': 0.00012056744583138289, 'module_to_agent_unmapping': 6.521852868430925e-06, 'normalize_and_clip_actions': 7.364400148845715e-05, 'listify_data_for_vector_env': 2.4528581437884818e-05, 'remove_single_ts_time_rank_from_batch': 2.3525638253741405e-06}}}, 'sample': 93.2976833001012, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -56.16000000000044, 'blue_0': -39.16000000000031, 'blue_1': -43.68000000000035, 'red_0': -52.10000000000029}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 10.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 66000.0, 'blue_0': 66000.0, 'blue_1': 66000.0, 'red_0': 66000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -56.16000000000044, 'blue_policy': -43.68000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 132000.0, 'blue_policy': 132000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034125789736948344, 'episode_return_mean': -191.10000000000142, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -168.50000000000097, 'episode_duration_sec_mean': 18.5162984599825, 'episode_return_min': -221.90000000000123, 'rlmodule_inference_timer': 0.012948187817096788, 'num_episodes_lifetime': 55.0, 'episode_len_min': 1200, 'time_between_sampling': 424.7052069999045, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.667906026149234, 'throughput_since_last_restore': 14.747148484626605}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 382.9073734000558, 'restore_env_runners': 1.3399985618889332e-05, 'training_step': 382.9067934999475, 'env_runner_sampling_timer': 93.44643810007256, 'learner_update_timer': 289.3772467999952, 'synch_weights': 0.022184700006619096, 'synch_env_connectors': 0.0023994999937713146, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 66000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00042606910154668865, 'timers': {'connectors': {'batch_individual_items': 0.0001864972377393016, 'add_states_from_episodes_to_batch': 7.153203591076588e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2378980725900744e-05, 'numpy_to_tensor': 7.033881018408657e-05, 'agent_to_module_mapping': 8.263649986817423e-06, 'add_observations_from_episodes_to_batch': 3.944387746809935e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008941021929635047, 'timers': {'connectors': {'get_actions': 0.00046102279671786703, 'un_batch_to_individual_items': 6.753727647170306e-05, 'tensor_to_numpy': 0.00012056744583138289, 'module_to_agent_unmapping': 6.521852868430925e-06, 'normalize_and_clip_actions': 7.364400148845715e-05, 'listify_data_for_vector_env': 2.4528581437884818e-05, 'remove_single_ts_time_rank_from_batch': 2.3525638253741405e-06}}}, 'sample': 93.2976833001012, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -56.16000000000044, 'blue_0': -39.16000000000031, 'blue_1': -43.68000000000035, 'red_0': -52.10000000000029}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 10.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 66000.0, 'blue_0': 66000.0, 'blue_1': 66000.0, 'red_0': 66000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -56.16000000000044, 'blue_policy': -43.68000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 132000.0, 'blue_policy': 132000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034125789736948344, 'episode_return_mean': -191.10000000000142, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -168.50000000000097, 'episode_duration_sec_mean': 18.5162984599825, 'episode_return_min': -221.90000000000123, 'rlmodule_inference_timer': 0.012948187817096788, 'num_episodes_lifetime': 55.0, 'episode_len_min': 1200, 'time_between_sampling': 424.7052069999045, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.667906026149234, 'throughput_since_last_restore': 14.747148484626605}}, 'learners': {'red_policy': {'policy_loss': -0.23375828564167023, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.011964287608861923, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 11.0, 'num_module_steps_trained_lifetime': 3963520.0, 'curr_entropy_coeff': 0.049010000000000005, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002934, 'vf_explained_var': -1.0, 'curr_kl_coeff': 2.278125047683716, 'total_loss': 9.708934783935547, 'entropy': 1.7222607135772705, 'vf_loss_unclipped': 253.43673706054688, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 940.9145461682518, 'throughput_since_last_restore': 885.6153576693497}}, 'blue_policy': {'weights_seq_no': 11.0, 'num_module_steps_trained_lifetime': 3963520.0, 'curr_entropy_coeff': 0.049010000000000005, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002934, 'vf_explained_var': 0.9117231965065002, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.6864086389541626, 'total_loss': -0.2302035242319107, 'entropy': 1.7432527542114258, 'policy_loss': -0.2552555799484253, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.08612346649169922, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.016146361827850342, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 940.9130459025474, 'throughput_since_last_restore': 885.6153720158859}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 7.00005330145359e-06, 'batch_individual_items': 0.9229082999518141, 'add_time_dim_to_batch_and_zero_pad': 2.1999934688210487e-05, 'numpy_to_tensor': 0.14582289999816567, 'add_observations_from_episodes_to_batch': 0.0002666999353095889, 'agent_to_module_mapping': 0.022027500090189278, 'add_one_ts_to_episodes_and_truncate': 0.2186989999609068, 'add_columns_from_episodes_to_train_batch': 0.49135759996715933, 'general_advantage_estimation': 13.387407999951392}}, 'connector_pipeline_timer': 15.18892409990076}, 'num_module_steps_trained_lifetime': 7927040.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 185790000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 44105.260927384894, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 44105.258508744446, 'throughput_since_last_restore': 41513.22125429026}, 'num_module_steps_trained_throughput': 1881.8243546863819, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1881.8243340466317, 'throughput_since_last_restore': 1771.2307659176981}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 66000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 11, 'trial_id': 'default', 'date': '2026-01-25_16-33-26', 'timestamp': 1769355206, 'time_this_iter_s': 382.92484307289124, 'time_total_s': 4456.86651301384, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 4456.86651301384, 'iterations_since_restore': 11, 'perf': {'cpu_util_percent': 19.414835164835164, 'ram_util_percent': 89.90164835164836}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -57.20000000000053, 'red_1': -54.600000000000485, 'blue_0': -49.10000000000043, 'blue_1': -51.00000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -58.60000000000055, 'red_1': -44.30000000000035, 'blue_0': -51.20000000000046, 'blue_1': -42.80000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -41.40000000000031, 'red_1': -74.5, 'blue_0': -55.000000000000504, 'blue_1': -57.000000000000526}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 827
(MultiAgentEnvRunner pid=37492) {'red_0': -44.60000000000035, 'red_1': -58.90000000000054, 'blue_0': -43.800000000000345, 'blue_1': -41.90000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -52.60000000000047, 'red_1': -45.800000000000374, 'blue_0': -53.20000000000047, 'blue_1': -40.60000000000031}
ITERATION 11: reward=-203.62000000000162, metadata={'num_env_steps_sampled_lifetime': 72000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003378412395070256, 'timers': {'connectors': {'batch_individual_items': 0.00010516876319880966, 'add_states_from_episodes_to_batch': 6.469671012430975e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2421019139405518e-05, 'numpy_to_tensor': 6.785102280695378e-05, 'agent_to_module_mapping': 8.345921385213571e-06, 'add_observations_from_episodes_to_batch': 3.965753265844773e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008844775376594383, 'timers': {'connectors': {'get_actions': 0.0004549256632474072, 'un_batch_to_individual_items': 6.664643616442582e-05, 'tensor_to_numpy': 0.00012131817758232683, 'module_to_agent_unmapping': 6.395708478537022e-06, 'normalize_and_clip_actions': 7.330622361365463e-05, 'listify_data_for_vector_env': 2.4190723591154965e-05, 'remove_single_ts_time_rank_from_batch': 2.4255483903876483e-06}}}, 'sample': 92.02367839997169, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -55.620000000000346, 'blue_0': -50.46000000000045, 'blue_1': -46.660000000000394, 'red_0': -50.880000000000436}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 11.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 72000.0, 'blue_0': 72000.0, 'blue_1': 72000.0, 'red_0': 72000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -55.620000000000346, 'blue_policy': -46.660000000000394}, 'num_module_steps_sampled_lifetime': {'red_policy': 144000.0, 'blue_policy': 144000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003371222359497047, 'episode_return_mean': -203.62000000000162, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -189.20000000000158, 'episode_duration_sec_mean': 18.262268039979972, 'episode_return_min': -227.90000000000134, 'rlmodule_inference_timer': 0.012957200749126575, 'num_episodes_lifetime': 60.0, 'episode_len_min': 1200, 'time_between_sampling': 289.7656408000039, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.230113004835257, 'throughput_since_last_restore': 14.860291260560823}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -52.10000000000045, 'red_1': -51.00000000000045, 'blue_0': -46.600000000000385, 'blue_1': -44.20000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 765
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1125
(MultiAgentEnvRunner pid=37492) {'red_0': -55.700000000000436, 'red_1': -44.000000000000284, 'blue_0': -39.500000000000384, 'blue_1': -35.10000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 929
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1048
(MultiAgentEnvRunner pid=37492) {'red_0': -50.90000000000037, 'red_1': -59.000000000000476, 'blue_0': -47.20000000000046, 'blue_1': -24.40000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1073
(MultiAgentEnvRunner pid=37492) {'red_0': -35.2000000000003, 'red_1': -27.80000000000033, 'blue_0': -58.70000000000048, 'blue_1': -53.600000000000406}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 926
(MultiAgentEnvRunner pid=37492) {'red_0': -54.20000000000042, 'red_1': -52.4000000000004, 'blue_0': -37.300000000000324, 'blue_1': -37.30000000000046}
ITERATION 12: reward=-181.24000000000157, metadata={'num_env_steps_sampled_lifetime': 78000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032487900143384765, 'timers': {'connectors': {'batch_individual_items': 9.921043510541414e-05, 'add_states_from_episodes_to_batch': 6.198866818505762e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2097716374636325e-05, 'numpy_to_tensor': 6.77142508465108e-05, 'agent_to_module_mapping': 8.022340958451851e-06, 'add_observations_from_episodes_to_batch': 3.7386817396346434e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008735243434383417, 'timers': {'connectors': {'get_actions': 0.00045341935970178067, 'un_batch_to_individual_items': 6.489198952553794e-05, 'tensor_to_numpy': 0.000117471807832429, 'module_to_agent_unmapping': 6.303129202051929e-06, 'normalize_and_clip_actions': 7.257972029478421e-05, 'listify_data_for_vector_env': 2.3592531367849732e-05, 'remove_single_ts_time_rank_from_batch': 2.3116568041777065e-06}}}, 'sample': 91.73366660007741, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.84000000000039, 'blue_0': -45.8600000000004, 'blue_1': -38.92000000000037, 'red_0': -49.620000000000395}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 12.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 78000.0, 'blue_0': 78000.0, 'blue_1': 78000.0, 'red_0': 78000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.84000000000039, 'blue_policy': -38.92000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 156000.0, 'blue_policy': 156000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032685074235627395, 'episode_return_mean': -181.24000000000157, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -174.30000000000146, 'episode_duration_sec_mean': 18.108289259998127, 'episode_return_min': -193.90000000000165, 'rlmodule_inference_timer': 0.012592555375300052, 'num_episodes_lifetime': 65.0, 'episode_len_min': 1200, 'time_between_sampling': 277.55282169999555, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.10266491476984, 'throughput_since_last_restore': 14.949004231879094}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 760
(MultiAgentEnvRunner pid=37492) {'red_0': -81.19999999999962, 'red_1': -43.800000000000374, 'blue_0': -54.70000000000048, 'blue_1': -54.50000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1099
(MultiAgentEnvRunner pid=37492) {'red_0': -53.500000000000455, 'red_1': -52.00000000000046, 'blue_0': -46.20000000000038, 'blue_1': -44.90000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -47.9000000000004, 'red_1': -52.500000000000455, 'blue_0': -38.80000000000028, 'blue_1': -40.1000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 483
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 846
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 846
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 847
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 847
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 987
(MultiAgentEnvRunner pid=37492) {'red_0': -60.700000000000465, 'red_1': -54.90000000000041, 'blue_0': -20.99999999999999, 'blue_1': -36.00000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 413
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1131
(MultiAgentEnvRunner pid=37492) {'red_0': -72.19999999999999, 'red_1': -64.10000000000055, 'blue_0': -50.800000000000445, 'blue_1': -32.20000000000013}
ITERATION 13: reward=-200.40000000000128, metadata={'num_env_steps_sampled_lifetime': 84000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00030700281044498067, 'timers': {'connectors': {'batch_individual_items': 9.197785672488895e-05, 'add_states_from_episodes_to_batch': 5.8916248859505954e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1380697891399115e-05, 'numpy_to_tensor': 6.349138204178187e-05, 'agent_to_module_mapping': 7.6126677024907335e-06, 'add_observations_from_episodes_to_batch': 3.681801792301697e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008317832550212594, 'timers': {'connectors': {'get_actions': 0.0004297071902259131, 'un_batch_to_individual_items': 6.350558098510288e-05, 'tensor_to_numpy': 0.00011287265310341433, 'module_to_agent_unmapping': 6.058831692097168e-06, 'normalize_and_clip_actions': 6.81003847704014e-05, 'listify_data_for_vector_env': 2.2582480900105495e-05, 'remove_single_ts_time_rank_from_batch': 2.281159607324747e-06}}}, 'sample': 87.85468930006027, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -53.46000000000045, 'blue_0': -42.30000000000031, 'blue_1': -41.54000000000034, 'red_0': -63.10000000000018}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 13.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 84000.0, 'blue_0': 84000.0, 'blue_1': 84000.0, 'red_0': 84000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -53.46000000000045, 'blue_policy': -41.54000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 168000.0, 'blue_policy': 168000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00030939920878124286, 'episode_return_mean': -200.40000000000128, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -172.60000000000124, 'episode_duration_sec_mean': 17.438895219983532, 'episode_return_min': -234.20000000000095, 'rlmodule_inference_timer': 0.01194994638800118, 'num_episodes_lifetime': 70.0, 'episode_len_min': 1200, 'time_between_sampling': 280.8635827000253, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.595723820962895, 'throughput_since_last_restore': 14.993408460689773}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 676
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1010
(MultiAgentEnvRunner pid=37492) {'red_0': -43.6000000000003, 'red_1': -47.20000000000035, 'blue_0': -29.700000000000184, 'blue_1': -45.90000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1100
(MultiAgentEnvRunner pid=37492) {'red_0': -50.30000000000037, 'red_1': -50.000000000000355, 'blue_0': -40.70000000000038, 'blue_1': -23.500000000000277}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1161
(MultiAgentEnvRunner pid=37492) {'red_0': -40.2000000000003, 'red_1': -51.90000000000048, 'blue_0': -52.20000000000044, 'blue_1': -44.10000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -52.90000000000046, 'red_1': -62.200000000000585, 'blue_0': -42.90000000000034, 'blue_1': -45.60000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -44.500000000000355, 'red_1': -50.400000000000425, 'blue_0': -57.20000000000052, 'blue_1': -40.400000000000304}
ITERATION 14: reward=-183.08000000000152, metadata={'num_env_steps_sampled_lifetime': 90000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032940426081804326, 'timers': {'connectors': {'batch_individual_items': 9.905091295999399e-05, 'add_states_from_episodes_to_batch': 6.303918172428142e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2546398146493335e-05, 'numpy_to_tensor': 6.702179776530243e-05, 'agent_to_module_mapping': 8.014948842788058e-06, 'add_observations_from_episodes_to_batch': 3.965077708191506e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008679454440760116, 'timers': {'connectors': {'get_actions': 0.0004457263794550859, 'un_batch_to_individual_items': 6.475906985704866e-05, 'tensor_to_numpy': 0.00011829540964613242, 'module_to_agent_unmapping': 7.204175212445639e-06, 'normalize_and_clip_actions': 7.349898643210061e-05, 'listify_data_for_vector_env': 2.380877190812668e-05, 'remove_single_ts_time_rank_from_batch': 2.3602298685286254e-06}}}, 'sample': 92.05855260009412, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -52.340000000000444, 'blue_0': -44.540000000000376, 'blue_1': -39.90000000000034, 'red_0': -46.30000000000035}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 14.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 90000.0, 'blue_0': 90000.0, 'blue_1': 90000.0, 'red_0': 90000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -52.340000000000444, 'blue_policy': -39.90000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 180000.0, 'blue_policy': 180000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032993857425650915, 'episode_return_mean': -183.08000000000152, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -164.5000000000014, 'episode_duration_sec_mean': 18.27736546001397, 'episode_return_min': -203.60000000000176, 'rlmodule_inference_timer': 0.012606094924165518, 'num_episodes_lifetime': 75.0, 'episode_len_min': 1200, 'time_between_sampling': 296.8918548000511, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.942861059796186, 'throughput_since_last_restore': 15.109282755995348}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 339
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 387
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 935
(MultiAgentEnvRunner pid=37492) {'red_0': -41.800000000000296, 'red_1': -32.00000000000015, 'blue_0': -57.50000000000049, 'blue_1': -55.70000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 634
(MultiAgentEnvRunner pid=37492) {'red_0': -50.300000000000466, 'red_1': -47.30000000000039, 'blue_0': -39.30000000000029, 'blue_1': -56.70000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -54.200000000000486, 'red_1': -57.80000000000054, 'blue_0': -43.50000000000035, 'blue_1': -48.30000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 936
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 937
(MultiAgentEnvRunner pid=37492) {'red_0': -53.300000000000416, 'red_1': -65.90000000000042, 'blue_0': -31.00000000000024, 'blue_1': -36.20000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -40.1000000000003, 'red_1': -40.3000000000003, 'blue_0': -39.50000000000029, 'blue_1': -52.40000000000046}
ITERATION 15: reward=-188.62000000000154, metadata={'num_env_steps_sampled_lifetime': 96000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035952117540037734, 'timers': {'connectors': {'batch_individual_items': 0.00010406380138545582, 'add_states_from_episodes_to_batch': 7.138719306902032e-06, 'add_time_dim_to_batch_and_zero_pad': 1.425603875157771e-05, 'numpy_to_tensor': 7.403719633985888e-05, 'agent_to_module_mapping': 9.085213264492565e-06, 'add_observations_from_episodes_to_batch': 4.2978555620540666e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009616966619481797, 'timers': {'connectors': {'get_actions': 0.0004900635350696657, 'un_batch_to_individual_items': 7.351052620985682e-05, 'tensor_to_numpy': 0.00013341285560958022, 'module_to_agent_unmapping': 7.04457098076891e-06, 'normalize_and_clip_actions': 7.951207354022414e-05, 'listify_data_for_vector_env': 2.6735563342089906e-05, 'remove_single_ts_time_rank_from_batch': 2.922658185590294e-06}}}, 'sample': 89.20875850005541, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -48.66000000000035, 'blue_0': -42.16000000000033, 'blue_1': -49.860000000000454, 'red_0': -47.940000000000396}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 15.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 96000.0, 'blue_0': 96000.0, 'blue_1': 96000.0, 'red_0': 96000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -48.66000000000035, 'blue_policy': -49.860000000000454}, 'num_module_steps_sampled_lifetime': {'red_policy': 192000.0, 'blue_policy': 192000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003654192186076543, 'episode_return_mean': -188.62000000000154, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -172.30000000000135, 'episode_duration_sec_mean': 17.720674480032176, 'episode_return_min': -203.80000000000177, 'rlmodule_inference_timer': 0.014528044139482834, 'num_episodes_lifetime': 80.0, 'episode_len_min': 1200, 'time_between_sampling': 262.0640103999758, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.926907301214726, 'throughput_since_last_restore': 15.211365418730486}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 405
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 752
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1127
(MultiAgentEnvRunner pid=37492) {'red_0': -27.20000000000033, 'red_1': -37.20000000000031, 'blue_0': -41.90000000000028, 'blue_1': -74.50000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 351
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 926
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1048
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1050
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1127
(MultiAgentEnvRunner pid=37492) {'red_0': -52.200000000000465, 'red_1': -42.50000000000036, 'blue_0': -57.00000000000054, 'blue_1': -38.500000000000284}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -41.40000000000032, 'red_1': -52.200000000000465, 'blue_0': -40.2000000000003, 'blue_1': -45.70000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 343
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 607
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1119
(MultiAgentEnvRunner pid=37492) {'red_0': -62.000000000000625, 'red_1': -48.200000000000365, 'blue_0': -65.0000000000005, 'blue_1': -46.100000000000335}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 762
(MultiAgentEnvRunner pid=37492) {'red_0': -48.900000000000404, 'red_1': -39.30000000000028, 'blue_0': -38.8000000000003, 'blue_1': -41.800000000000324}
ITERATION 16: reward=-188.12000000000148, metadata={'num_env_steps_sampled_lifetime': 102000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00037298152218478455, 'timers': {'connectors': {'batch_individual_items': 0.00012218487488018808, 'add_states_from_episodes_to_batch': 6.960649289997295e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3494716895022336e-05, 'numpy_to_tensor': 7.334583280738382e-05, 'agent_to_module_mapping': 8.514336669444998e-06, 'add_observations_from_episodes_to_batch': 4.2171525150973515e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000929754517274218, 'timers': {'connectors': {'get_actions': 0.0004862587904639204, 'un_batch_to_individual_items': 6.860995995625843e-05, 'tensor_to_numpy': 0.00012595884473536335, 'module_to_agent_unmapping': 6.657208004210074e-06, 'normalize_and_clip_actions': 7.463960370642517e-05, 'listify_data_for_vector_env': 2.4447230445131898e-05, 'remove_single_ts_time_rank_from_batch': 2.4611978402803485e-06}}}, 'sample': 99.47321190009825, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -43.88000000000036, 'blue_0': -48.58000000000038, 'blue_1': -49.32000000000032, 'red_0': -46.34000000000043}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 16.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 102000.0, 'blue_0': 102000.0, 'blue_1': 102000.0, 'red_0': 102000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -43.88000000000036, 'blue_policy': -49.32000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 204000.0, 'blue_policy': 204000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003522181925252622, 'episode_return_mean': -188.12000000000148, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -168.80000000000132, 'episode_duration_sec_mean': 19.749354259995744, 'episode_return_min': -221.3000000000018, 'rlmodule_inference_timer': 0.013999657898632842, 'num_episodes_lifetime': 85.0, 'episode_len_min': 1200, 'time_between_sampling': 265.26342159998603, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.822311504997083, 'throughput_since_last_restore': 15.121966871152885}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 947
(MultiAgentEnvRunner pid=37492) {'red_0': -44.60000000000036, 'red_1': -45.000000000000384, 'blue_0': -41.2000000000003, 'blue_1': -50.60000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -45.700000000000365, 'red_1': -49.40000000000043, 'blue_0': -57.50000000000053, 'blue_1': -36.300000000000246}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 603
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 777
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 906
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 987
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 987
(MultiAgentEnvRunner pid=37492) {'red_0': -51.0000000000004, 'red_1': -44.40000000000037, 'blue_0': -36.100000000000435, 'blue_1': -50.100000000000456}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 638
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 811
(MultiAgentEnvRunner pid=37492) {'red_0': -45.10000000000038, 'red_1': -43.60000000000035, 'blue_0': -56.0000000000005, 'blue_1': -46.8000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 267
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 280
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1011
(MultiAgentEnvRunner pid=37492) {'red_0': -50.90000000000048, 'red_1': -50.00000000000047, 'blue_0': -47.90000000000039, 'blue_1': -50.00000000000042}
ITERATION 17: reward=-188.4400000000016, metadata={'num_env_steps_sampled_lifetime': 108000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003645766711191262, 'timers': {'connectors': {'batch_individual_items': 0.00010496794649885524, 'add_states_from_episodes_to_batch': 7.6016375186889894e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3832557657550235e-05, 'numpy_to_tensor': 7.459621850568906e-05, 'agent_to_module_mapping': 8.70737169616378e-06, 'add_observations_from_episodes_to_batch': 4.476486121319395e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009337331314573717, 'timers': {'connectors': {'get_actions': 0.00048104858567842105, 'un_batch_to_individual_items': 6.853660422545177e-05, 'tensor_to_numpy': 0.0001282602368954194, 'module_to_agent_unmapping': 6.729216022668552e-06, 'normalize_and_clip_actions': 7.749991114976149e-05, 'listify_data_for_vector_env': 2.5569744488921384e-05, 'remove_single_ts_time_rank_from_batch': 2.430993421111276e-06}}}, 'sample': 99.67645979998633, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.480000000000395, 'blue_0': -47.74000000000043, 'blue_1': -46.760000000000396, 'red_0': -47.4600000000004}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 17.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 108000.0, 'blue_0': 108000.0, 'blue_1': 108000.0, 'red_0': 108000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.480000000000395, 'blue_policy': -46.760000000000396}, 'num_module_steps_sampled_lifetime': {'red_policy': 216000.0, 'blue_policy': 216000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035115876705501804, 'episode_return_mean': -188.4400000000016, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -181.40000000000148, 'episode_duration_sec_mean': 19.738693039980717, 'episode_return_min': -198.80000000000175, 'rlmodule_inference_timer': 0.013971373189817385, 'num_episodes_lifetime': 90.0, 'episode_len_min': 1200, 'time_between_sampling': 334.61987630010117, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.30893680174161, 'throughput_since_last_restore': 15.132225545918702}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 585
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 693
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 956
(MultiAgentEnvRunner pid=37492) {'red_0': -46.20000000000034, 'red_1': -56.00000000000051, 'blue_0': -39.900000000000325, 'blue_1': -28.30000000000015}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 859
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1029
(MultiAgentEnvRunner pid=37492) {'red_0': -44.00000000000031, 'red_1': -49.40000000000038, 'blue_0': -39.100000000000456, 'blue_1': -33.80000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 404
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1152
(MultiAgentEnvRunner pid=37492) {'red_0': -32.50000000000016, 'red_1': -56.600000000000584, 'blue_0': -39.10000000000035, 'blue_1': -47.60000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -43.000000000000334, 'red_1': -43.80000000000035, 'blue_0': -44.70000000000036, 'blue_1': -42.20000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -50.00000000000044, 'red_1': -45.70000000000038, 'blue_0': -53.60000000000048, 'blue_1': -49.00000000000041}
ITERATION 18: reward=-176.90000000000146, metadata={'num_env_steps_sampled_lifetime': 114000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000360197049036728, 'timers': {'connectors': {'batch_individual_items': 0.00010628943572265239, 'add_states_from_episodes_to_batch': 7.018669246252862e-06, 'add_time_dim_to_batch_and_zero_pad': 1.459483800560013e-05, 'numpy_to_tensor': 7.624982023039364e-05, 'agent_to_module_mapping': 9.000711898694388e-06, 'add_observations_from_episodes_to_batch': 4.157327142207724e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009490969208356669, 'timers': {'connectors': {'get_actions': 0.0004861303486393865, 'un_batch_to_individual_items': 7.156723868826155e-05, 'tensor_to_numpy': 0.00012866905299517095, 'module_to_agent_unmapping': 7.292412836959665e-06, 'normalize_and_clip_actions': 7.818434556482256e-05, 'listify_data_for_vector_env': 2.592776144194111e-05, 'remove_single_ts_time_rank_from_batch': 2.6156946638536237e-06}}}, 'sample': 95.92378499999177, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -50.300000000000445, 'blue_0': -43.28000000000039, 'blue_1': -40.180000000000305, 'red_0': -43.14000000000031}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 18.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 114000.0, 'blue_0': 114000.0, 'blue_1': 114000.0, 'red_0': 114000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -50.300000000000445, 'blue_policy': -40.180000000000305}, 'num_module_steps_sampled_lifetime': {'red_policy': 228000.0, 'blue_policy': 228000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035200198228823605, 'episode_return_mean': -176.90000000000146, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -166.30000000000138, 'episode_duration_sec_mean': 19.053942039981486, 'episode_return_min': -198.30000000000172, 'rlmodule_inference_timer': 0.013713207630530973, 'num_episodes_lifetime': 95.0, 'episode_len_min': 1200, 'time_between_sampling': 292.25033409998287, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.749300236544139, 'throughput_since_last_restore': 15.163490507260407}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -42.800000000000324, 'red_1': -75.1, 'blue_0': -50.30000000000043, 'blue_1': -56.700000000000514}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 574
(MultiAgentEnvRunner pid=37492) {'red_0': -43.20000000000034, 'red_1': -45.700000000000365, 'blue_0': -44.500000000000355, 'blue_1': -40.3000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 729
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1032
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1058
(MultiAgentEnvRunner pid=37492) {'red_0': -51.400000000000404, 'red_1': -72.1000000000001, 'blue_0': -45.20000000000042, 'blue_1': -31.200000000000415}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -35.90000000000024, 'red_1': -44.70000000000036, 'blue_0': -42.60000000000033, 'blue_1': -47.50000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 341
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 597
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 794
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1170
(MultiAgentEnvRunner pid=37492) {'red_0': -35.60000000000031, 'red_1': -46.200000000000436, 'blue_0': -49.10000000000034, 'blue_1': -40.40000000000038}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -112.99999999999785, 'red_1': -113.59999999999782, 'blue_0': -114.39999999999777, 'blue_1': -112.99999999999785}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -114.59999999999776, 'red_1': -114.49999999999777, 'blue_0': -114.79999999999775, 'blue_1': -115.29999999999772}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-25 17:27:45,371	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -116.29999999999767, 'red_1': -115.89999999999769, 'blue_0': -116.29999999999767, 'blue_1': -116.79999999999764}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.7999999999999999, 'red_1': -0.1, 'blue_0': -115.99999999999768, 'blue_1': -115.99999999999768}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -115.89999999999769, 'red_1': -115.39999999999772, 'blue_0': -1.0999999999999999, 'blue_1': -0.7999999999999999}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 19: reward=-188.10000000000133, metadata={'num_env_steps_sampled_lifetime': 120000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00036433278005093555, 'timers': {'connectors': {'batch_individual_items': 0.0001062504443695093, 'add_states_from_episodes_to_batch': 6.999421353328461e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4167302062197458e-05, 'numpy_to_tensor': 7.762642383000018e-05, 'agent_to_module_mapping': 9.129197750128694e-06, 'add_observations_from_episodes_to_batch': 4.195706627914952e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009682803516675585, 'timers': {'connectors': {'get_actions': 0.000498229922532209, 'un_batch_to_individual_items': 7.490596471306272e-05, 'tensor_to_numpy': 0.0001294353187356992, 'module_to_agent_unmapping': 6.963634641714697e-06, 'normalize_and_clip_actions': 7.917767928533762e-05, 'listify_data_for_vector_env': 2.6366633539680926e-05, 'remove_single_ts_time_rank_from_batch': 2.4832351373223626e-06}}}, 'sample': 96.02928880008403, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -56.76000000000024, 'blue_0': -46.34000000000037, 'blue_1': -43.220000000000404, 'red_0': -41.78000000000032}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 19.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 120000.0, 'blue_0': 120000.0, 'blue_1': 120000.0, 'red_0': 120000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -56.76000000000024, 'blue_policy': -43.220000000000404}, 'num_module_steps_sampled_lifetime': {'red_policy': 240000.0, 'blue_policy': 240000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003749925411218465, 'episode_return_mean': -188.10000000000133, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -170.70000000000132, 'episode_duration_sec_mean': 19.037899079988712, 'episode_return_min': -224.90000000000128, 'rlmodule_inference_timer': 0.014650454916751151, 'num_episodes_lifetime': 100.0, 'episode_len_min': 1200, 'time_between_sampling': 285.04519189998973, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.117355969733529, 'throughput_since_last_restore': 14.975256634111739}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -45.00000000000036, 'red_1': -53.400000000000475, 'blue_0': -49.40000000000042, 'blue_1': -37.600000000000264}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -64.50000000000058, 'red_1': -42.00000000000033, 'blue_0': -40.80000000000031, 'blue_1': -42.30000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 639
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 720
(MultiAgentEnvRunner pid=37492) {'red_0': -72.10000000000007, 'red_1': -48.50000000000034, 'blue_0': -33.70000000000026, 'blue_1': -28.800000000000118}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 442
(MultiAgentEnvRunner pid=37492) {'red_0': -48.800000000000416, 'red_1': -48.90000000000042, 'blue_0': -39.000000000000284, 'blue_1': -56.4000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 722
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1187
(MultiAgentEnvRunner pid=37492) {'red_0': -22.500000000000263, 'red_1': -54.60000000000055, 'blue_0': -46.30000000000031, 'blue_1': -51.1000000000004}
ITERATION 20: reward=-185.1400000000014, metadata={'num_env_steps_sampled_lifetime': 126000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003526183808517368, 'timers': {'connectors': {'batch_individual_items': 0.00010492434932737893, 'add_states_from_episodes_to_batch': 6.834515661588421e-06, 'add_time_dim_to_batch_and_zero_pad': 1.391426973291954e-05, 'numpy_to_tensor': 7.517257945742741e-05, 'agent_to_module_mapping': 8.64728218457003e-06, 'add_observations_from_episodes_to_batch': 4.093503403088631e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009200941127997681, 'timers': {'connectors': {'get_actions': 0.0004737643729124349, 'un_batch_to_individual_items': 6.915733617237659e-05, 'tensor_to_numpy': 0.00012088350327622223, 'module_to_agent_unmapping': 6.607379702818575e-06, 'normalize_and_clip_actions': 7.583558424551e-05, 'listify_data_for_vector_env': 2.6049761923629187e-05, 'remove_single_ts_time_rank_from_batch': 2.9688001273867315e-06}}}, 'sample': 95.99425869993865, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -49.480000000000416, 'blue_0': -41.840000000000316, 'blue_1': -43.24000000000032, 'red_0': -50.58000000000033}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 20.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 126000.0, 'blue_0': 126000.0, 'blue_1': 126000.0, 'red_0': 126000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -49.480000000000416, 'blue_policy': -43.24000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 252000.0, 'blue_policy': 252000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035508802061023426, 'episode_return_mean': -185.1400000000014, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -174.50000000000153, 'episode_duration_sec_mean': 19.06804063997697, 'episode_return_min': -193.10000000000161, 'rlmodule_inference_timer': 0.014023532605179282, 'num_episodes_lifetime': 105.0, 'episode_len_min': 1200, 'time_between_sampling': 399.13310079998337, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.902635838066074, 'throughput_since_last_restore': 14.97177425618688}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 402.57142650010064, 'restore_env_runners': 1.1699972674250603e-05, 'training_step': 402.571173000033, 'env_runner_sampling_timer': 96.13526919996366, 'learner_update_timer': 306.3753613999579, 'synch_weights': 0.0150562000926584, 'synch_env_connectors': 0.0026425999822095037, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 126000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003526183808517368, 'timers': {'connectors': {'batch_individual_items': 0.00010492434932737893, 'add_states_from_episodes_to_batch': 6.834515661588421e-06, 'add_time_dim_to_batch_and_zero_pad': 1.391426973291954e-05, 'numpy_to_tensor': 7.517257945742741e-05, 'agent_to_module_mapping': 8.64728218457003e-06, 'add_observations_from_episodes_to_batch': 4.093503403088631e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009200941127997681, 'timers': {'connectors': {'get_actions': 0.0004737643729124349, 'un_batch_to_individual_items': 6.915733617237659e-05, 'tensor_to_numpy': 0.00012088350327622223, 'module_to_agent_unmapping': 6.607379702818575e-06, 'normalize_and_clip_actions': 7.583558424551e-05, 'listify_data_for_vector_env': 2.6049761923629187e-05, 'remove_single_ts_time_rank_from_batch': 2.9688001273867315e-06}}}, 'sample': 95.99425869993865, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -49.480000000000416, 'blue_0': -41.840000000000316, 'blue_1': -43.24000000000032, 'red_0': -50.58000000000033}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 20.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 126000.0, 'blue_0': 126000.0, 'blue_1': 126000.0, 'red_0': 126000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -49.480000000000416, 'blue_policy': -43.24000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 252000.0, 'blue_policy': 252000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035508802061023426, 'episode_return_mean': -185.1400000000014, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -174.50000000000153, 'episode_duration_sec_mean': 19.06804063997697, 'episode_return_min': -193.10000000000161, 'rlmodule_inference_timer': 0.014023532605179282, 'num_episodes_lifetime': 105.0, 'episode_len_min': 1200, 'time_between_sampling': 399.13310079998337, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.902635838066074, 'throughput_since_last_restore': 14.97177425618688}}, 'learners': {'red_policy': {'policy_loss': -0.1262335479259491, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.011639600619673729, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 21.0, 'num_module_steps_trained_lifetime': 7566720.0, 'curr_entropy_coeff': 0.04811, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002874, 'vf_explained_var': -0.1784437894821167, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.829597473144531, 'entropy': 1.741566777229309, 'vf_loss_unclipped': 584.5611572265625, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 894.9591322310323, 'throughput_since_last_restore': 899.1049129906087}}, 'blue_policy': {'weights_seq_no': 21.0, 'num_module_steps_trained_lifetime': 7566720.0, 'curr_entropy_coeff': 0.04811, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002874, 'vf_explained_var': 0.7824676036834717, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.9493539333343506, 'total_loss': -0.16341376304626465, 'entropy': 1.754906415939331, 'policy_loss': -0.18964830040931702, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.08954387903213501, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014009634032845497, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 894.9571551966837, 'throughput_since_last_restore': 899.1049211848446}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.9999368861317635e-06, 'batch_individual_items': 0.8425137000158429, 'add_time_dim_to_batch_and_zero_pad': 2.2300053387880325e-05, 'numpy_to_tensor': 0.16184970003087074, 'add_observations_from_episodes_to_batch': 0.0002432999899610877, 'agent_to_module_mapping': 0.02123860002029687, 'add_one_ts_to_episodes_and_truncate': 0.18036330002360046, 'add_columns_from_episodes_to_train_batch': 0.5494035000447184, 'general_advantage_estimation': 13.342087100027129}}, 'connector_pipeline_timer': 15.098185900016688}, 'num_module_steps_trained_lifetime': 15133440.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 354690000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 41951.063238255105, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 41951.06193580375, 'throughput_since_last_restore': 42145.543577665}, 'num_module_steps_trained_throughput': 1789.9119848186758, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1789.911964812651, 'throughput_since_last_restore': 1798.2098547625305}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 126000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 21, 'trial_id': 'default', 'date': '2026-01-25_17-39-06', 'timestamp': 1769359146, 'time_this_iter_s': 402.5874447822571, 'time_total_s': 8396.887152671814, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 8396.887152671814, 'iterations_since_restore': 21, 'perf': {'cpu_util_percent': 18.890940766550525, 'ram_util_percent': 90.0369337979094}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 860
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1070
(MultiAgentEnvRunner pid=37492) {'red_0': -40.00000000000029, 'red_1': -51.10000000000046, 'blue_0': -45.800000000000395, 'blue_1': -41.100000000000314}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 340
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 904
(MultiAgentEnvRunner pid=37492) {'red_0': -40.800000000000324, 'red_1': -44.00000000000035, 'blue_0': -42.00000000000032, 'blue_1': -51.00000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 475
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 890
(MultiAgentEnvRunner pid=37492) {'red_0': -45.900000000000475, 'red_1': -23.10000000000018, 'blue_0': -62.30000000000052, 'blue_1': -62.50000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 926
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1034
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1076
(MultiAgentEnvRunner pid=37492) {'red_0': -18.400000000000215, 'red_1': -35.20000000000033, 'blue_0': -59.60000000000047, 'blue_1': -70.70000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -48.700000000000415, 'red_1': -53.60000000000048, 'blue_0': -57.80000000000053, 'blue_1': -45.60000000000037}
ITERATION 21: reward=-187.84000000000154, metadata={'num_env_steps_sampled_lifetime': 132000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032906465142569795, 'timers': {'connectors': {'batch_individual_items': 9.862203598496084e-05, 'add_states_from_episodes_to_batch': 6.330084176226069e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2432174596397944e-05, 'numpy_to_tensor': 6.773654947338474e-05, 'agent_to_module_mapping': 8.023931738725322e-06, 'add_observations_from_episodes_to_batch': 3.864233078463842e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008641195188953837, 'timers': {'connectors': {'get_actions': 0.0004448544874380981, 'un_batch_to_individual_items': 6.519151331741432e-05, 'tensor_to_numpy': 0.00011773448998754862, 'module_to_agent_unmapping': 6.259850960599319e-06, 'normalize_and_clip_actions': 7.176654314199413e-05, 'listify_data_for_vector_env': 2.3714872781592245e-05, 'remove_single_ts_time_rank_from_batch': 2.8547513318763233e-06}}}, 'sample': 97.20150450000074, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -41.40000000000036, 'blue_0': -53.500000000000455, 'blue_1': -54.18000000000037, 'red_0': -38.760000000000346}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 21.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 132000.0, 'blue_0': 132000.0, 'blue_1': 132000.0, 'red_0': 132000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -41.40000000000036, 'blue_policy': -54.18000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 264000.0, 'blue_policy': 264000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000339029476129613, 'episode_return_mean': -187.84000000000154, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -177.8000000000014, 'episode_duration_sec_mean': 19.31508427998051, 'episode_return_min': -205.70000000000178, 'rlmodule_inference_timer': 0.01311027046108718, 'num_episodes_lifetime': 110.0, 'episode_len_min': 1200, 'time_between_sampling': 306.74936370004434, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.671783674886537, 'throughput_since_last_restore': 14.957868521037012}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 941
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1023
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1068
(MultiAgentEnvRunner pid=37492) {'red_0': -44.80000000000032, 'red_1': -61.700000000000536, 'blue_0': -36.90000000000032, 'blue_1': -41.00000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 548
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 984
(MultiAgentEnvRunner pid=37492) {'red_0': -53.20000000000042, 'red_1': -49.20000000000037, 'blue_0': -44.500000000000405, 'blue_1': -34.10000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 390
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 554
(MultiAgentEnvRunner pid=37492) {'red_0': -52.300000000000495, 'red_1': -46.10000000000035, 'blue_0': -27.100000000000094, 'blue_1': -35.200000000000216}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1053
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1098
(MultiAgentEnvRunner pid=37492) {'red_0': -58.10000000000046, 'red_1': -75.30000000000024, 'blue_0': -19.200000000000244, 'blue_1': -36.700000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 731
(MultiAgentEnvRunner pid=37492) {'red_0': -42.500000000000334, 'red_1': -35.700000000000266, 'blue_0': -43.40000000000033, 'blue_1': -47.100000000000385}
ITERATION 22: reward=-176.82000000000136, metadata={'num_env_steps_sampled_lifetime': 138000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033035923004987337, 'timers': {'connectors': {'batch_individual_items': 9.820907894149744e-05, 'add_states_from_episodes_to_batch': 6.477400815251735e-06, 'add_time_dim_to_batch_and_zero_pad': 1.274013081421732e-05, 'numpy_to_tensor': 6.782669022311205e-05, 'agent_to_module_mapping': 8.479475305928042e-06, 'add_observations_from_episodes_to_batch': 3.920750684854461e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008793049195969615, 'timers': {'connectors': {'get_actions': 0.0004548021195006086, 'un_batch_to_individual_items': 6.566792100360672e-05, 'tensor_to_numpy': 0.00011826022352298968, 'module_to_agent_unmapping': 6.504119355557151e-06, 'normalize_and_clip_actions': 7.26235086864112e-05, 'listify_data_for_vector_env': 2.4414406856065016e-05, 'remove_single_ts_time_rank_from_batch': 2.311220448739626e-06}}}, 'sample': 93.41646390000824, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -53.60000000000035, 'blue_0': -34.220000000000276, 'blue_1': -38.82000000000034, 'red_0': -50.180000000000405}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 22.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 138000.0, 'blue_0': 138000.0, 'blue_1': 138000.0, 'red_0': 138000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -53.60000000000035, 'blue_policy': -38.82000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 276000.0, 'blue_policy': 276000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003358083761308366, 'episode_return_mean': -176.82000000000136, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -160.70000000000115, 'episode_duration_sec_mean': 18.506633220007643, 'episode_return_min': -189.30000000000126, 'rlmodule_inference_timer': 0.013220611685473482, 'num_episodes_lifetime': 115.0, 'episode_len_min': 1200, 'time_between_sampling': 311.6173319000518, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.980501768394177, 'throughput_since_last_restore': 14.958846085547266}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 350
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 831
(MultiAgentEnvRunner pid=37492) {'red_0': -52.90000000000044, 'red_1': -52.20000000000049, 'blue_0': -43.9000000000003, 'blue_1': -30.400000000000112}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 673
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1136
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1154
(MultiAgentEnvRunner pid=37492) {'red_0': -52.2000000000004, 'red_1': -58.90000000000049, 'blue_0': -51.100000000000506, 'blue_1': -23.7000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1049
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1159
(MultiAgentEnvRunner pid=37492) {'red_0': -50.200000000000365, 'red_1': -51.00000000000036, 'blue_0': -41.70000000000039, 'blue_1': -23.400000000000304}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 573
(MultiAgentEnvRunner pid=37492) {'red_0': -50.40000000000043, 'red_1': -53.000000000000504, 'blue_0': -46.20000000000037, 'blue_1': -51.50000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 722
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 805
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 813
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 813
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1052
(MultiAgentEnvRunner pid=37492) {'red_0': -40.40000000000035, 'red_1': -26.600000000000307, 'blue_0': -60.800000000000544, 'blue_1': -40.500000000000405}
ITERATION 23: reward=-180.20000000000155, metadata={'num_env_steps_sampled_lifetime': 144000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035222882146975377, 'timers': {'connectors': {'batch_individual_items': 0.00010258656584667476, 'add_states_from_episodes_to_batch': 6.714681006096467e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3317831102764617e-05, 'numpy_to_tensor': 7.423975482332504e-05, 'agent_to_module_mapping': 8.657302350251825e-06, 'add_observations_from_episodes_to_batch': 4.124951730240976e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009292039004510362, 'timers': {'connectors': {'get_actions': 0.00047901954046362325, 'un_batch_to_individual_items': 6.974441163779267e-05, 'tensor_to_numpy': 0.0001253425635732097, 'module_to_agent_unmapping': 6.762741850674121e-06, 'normalize_and_clip_actions': 7.592610700403441e-05, 'listify_data_for_vector_env': 2.4930111606074012e-05, 'remove_single_ts_time_rank_from_batch': 2.466680176113812e-06}}}, 'sample': 94.58582150004804, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -48.34000000000044, 'blue_0': -48.74000000000042, 'blue_1': -33.90000000000031, 'red_0': -49.2200000000004}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 23.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 144000.0, 'blue_0': 144000.0, 'blue_1': 144000.0, 'red_0': 144000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -48.34000000000044, 'blue_policy': -33.90000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 288000.0, 'blue_policy': 288000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003530378288491925, 'episode_return_mean': -180.20000000000155, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -166.30000000000143, 'episode_duration_sec_mean': 18.794910480012184, 'episode_return_min': -201.10000000000178, 'rlmodule_inference_timer': 0.013877895179395345, 'num_episodes_lifetime': 120.0, 'episode_len_min': 1200, 'time_between_sampling': 307.10349400003906, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.022307815392136, 'throughput_since_last_restore': 14.961475706955627}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1052
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1198
(MultiAgentEnvRunner pid=37492) {'red_0': -40.80000000000031, 'red_1': -44.60000000000038, 'blue_0': -39.9000000000003, 'blue_1': -35.60000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 514
(MultiAgentEnvRunner pid=37492) {'red_0': -47.800000000000395, 'red_1': -42.80000000000034, 'blue_0': -32.10000000000018, 'blue_1': -42.90000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 171
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 786
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 819
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 853
(MultiAgentEnvRunner pid=37492) {'red_0': -48.900000000000375, 'red_1': -46.50000000000037, 'blue_0': -36.400000000000276, 'blue_1': -41.50000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -34.800000000000225, 'red_1': -65.30000000000054, 'blue_0': -52.40000000000045, 'blue_1': -44.40000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 172
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 707
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 726
(MultiAgentEnvRunner pid=37492) {'red_0': -39.50000000000039, 'red_1': -34.600000000000406, 'blue_0': -43.80000000000027, 'blue_1': -49.40000000000035}
ITERATION 24: reward=-172.80000000000138, metadata={'num_env_steps_sampled_lifetime': 150000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003473840256731991, 'timers': {'connectors': {'batch_individual_items': 9.966459920995268e-05, 'add_states_from_episodes_to_batch': 6.878618982013153e-06, 'add_time_dim_to_batch_and_zero_pad': 1.309887623676779e-05, 'numpy_to_tensor': 7.44373240675448e-05, 'agent_to_module_mapping': 8.692671624807423e-06, 'add_observations_from_episodes_to_batch': 4.117906216679576e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009104869079714235, 'timers': {'connectors': {'get_actions': 0.00046719379173897, 'un_batch_to_individual_items': 6.70770985254469e-05, 'tensor_to_numpy': 0.00012509224581091724, 'module_to_agent_unmapping': 6.6411222230011855e-06, 'normalize_and_clip_actions': 7.507938380750028e-05, 'listify_data_for_vector_env': 2.5371016238410476e-05, 'remove_single_ts_time_rank_from_batch': 2.3872462824337717e-06}}}, 'sample': 93.12187319993973, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.76000000000041, 'blue_0': -40.92000000000029, 'blue_1': -42.760000000000346, 'red_0': -42.36000000000034}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 24.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 150000.0, 'blue_0': 150000.0, 'blue_1': 150000.0, 'red_0': 150000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.76000000000041, 'blue_policy': -42.760000000000346}, 'num_module_steps_sampled_lifetime': {'red_policy': 300000.0, 'blue_policy': 300000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003566185881069264, 'episode_return_mean': -172.80000000000138, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -160.90000000000123, 'episode_duration_sec_mean': 18.4797467000084, 'episode_return_min': -196.90000000000157, 'rlmodule_inference_timer': 0.014133035259697679, 'num_episodes_lifetime': 125.0, 'episode_len_min': 1200, 'time_between_sampling': 304.82486279995646, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.98440699756689, 'throughput_since_last_restore': 14.962388173737441}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 369
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 376
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 601
(MultiAgentEnvRunner pid=37492) {'red_0': -44.500000000000384, 'red_1': -72.00000000000007, 'blue_0': -24.800000000000058, 'blue_1': -42.50000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 692
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1060
(MultiAgentEnvRunner pid=37492) {'red_0': -52.10000000000045, 'red_1': -51.60000000000044, 'blue_0': -36.90000000000031, 'blue_1': -37.600000000000264}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 254
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 438
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 515
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 898
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 912
(MultiAgentEnvRunner pid=37492) {'red_0': -54.00000000000045, 'red_1': -49.20000000000038, 'blue_0': -37.30000000000027, 'blue_1': -27.800000000000082}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1135
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1178
(MultiAgentEnvRunner pid=37492) {'red_0': -49.00000000000035, 'red_1': -50.70000000000036, 'blue_0': -40.200000000000365, 'blue_1': -21.300000000000274}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1014
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1020
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1037
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1127
(MultiAgentEnvRunner pid=37492) {'red_0': -58.900000000000496, 'red_1': -52.0000000000004, 'blue_0': -64.80000000000061, 'blue_1': -29.80000000000038}
ITERATION 25: reward=-179.4000000000013, metadata={'num_env_steps_sampled_lifetime': 156000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003706763180103228, 'timers': {'connectors': {'batch_individual_items': 0.00011162491265104713, 'add_states_from_episodes_to_batch': 7.0632766370954865e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4233997680652793e-05, 'numpy_to_tensor': 7.607238459311025e-05, 'agent_to_module_mapping': 8.911491960463994e-06, 'add_observations_from_episodes_to_batch': 4.325218347232542e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009966539137954318, 'timers': {'connectors': {'get_actions': 0.0005161041491880142, 'un_batch_to_individual_items': 7.853749371236919e-05, 'tensor_to_numpy': 0.0001337692908841903, 'module_to_agent_unmapping': 7.230043710092976e-06, 'normalize_and_clip_actions': 7.937577181886496e-05, 'listify_data_for_vector_env': 2.6762404907144196e-05, 'remove_single_ts_time_rank_from_batch': 2.7375682495380194e-06}}}, 'sample': 91.8116149000125, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -55.10000000000033, 'blue_0': -40.800000000000324, 'blue_1': -31.80000000000026, 'red_0': -51.70000000000042}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 25.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 156000.0, 'blue_0': 156000.0, 'blue_1': 156000.0, 'red_0': 156000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -55.10000000000033, 'blue_policy': -31.80000000000026}, 'num_module_steps_sampled_lifetime': {'red_policy': 312000.0, 'blue_policy': 312000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00037691320294065374, 'episode_return_mean': -179.4000000000013, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -161.20000000000132, 'episode_duration_sec_mean': 18.22555645997636, 'episode_return_min': -205.50000000000188, 'rlmodule_inference_timer': 0.014813452729872662, 'num_episodes_lifetime': 130.0, 'episode_len_min': 1200, 'time_between_sampling': 307.2930223000003, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.343198067015072, 'throughput_since_last_restore': 14.937583521265923}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 337
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1010
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1039
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1180
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1197
(MultiAgentEnvRunner pid=37492) {'red_0': -19.10000000000023, 'red_1': -34.50000000000026, 'blue_0': -50.600000000000385, 'blue_1': -58.50000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -42.300000000000324, 'red_1': -56.10000000000051, 'blue_0': -44.80000000000036, 'blue_1': -62.7000000000006}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 710
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 822
(MultiAgentEnvRunner pid=37492) {'red_0': -63.10000000000054, 'red_1': -55.50000000000043, 'blue_0': -33.200000000000294, 'blue_1': -43.90000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 564
(MultiAgentEnvRunner pid=37492) {'red_0': -37.30000000000026, 'red_1': -46.70000000000037, 'blue_0': -38.60000000000028, 'blue_1': -38.60000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 557
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1102
(MultiAgentEnvRunner pid=37492) {'red_0': -62.100000000000584, 'red_1': -37.000000000000284, 'blue_0': -54.000000000000476, 'blue_1': -46.700000000000415}
ITERATION 26: reward=-185.06000000000157, metadata={'num_env_steps_sampled_lifetime': 162000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003474841354369646, 'timers': {'connectors': {'batch_individual_items': 0.00010915236859317203, 'add_states_from_episodes_to_batch': 6.65174130279239e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2566828550020832e-05, 'numpy_to_tensor': 7.055501909392679e-05, 'agent_to_module_mapping': 8.20165308260503e-06, 'add_observations_from_episodes_to_batch': 3.976600876637006e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008909538408153359, 'timers': {'connectors': {'get_actions': 0.00045815116836832494, 'un_batch_to_individual_items': 6.916370139359192e-05, 'tensor_to_numpy': 0.00011969626892455836, 'module_to_agent_unmapping': 6.543640344101947e-06, 'normalize_and_clip_actions': 7.274939862784185e-05, 'listify_data_for_vector_env': 2.540557811472145e-05, 'remove_single_ts_time_rank_from_batch': 2.458080026878461e-06}}}, 'sample': 95.02004520001356, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -45.96000000000038, 'blue_0': -44.240000000000364, 'blue_1': -50.080000000000446, 'red_0': -44.780000000000385}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 26.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 162000.0, 'blue_0': 162000.0, 'blue_1': 162000.0, 'red_0': 162000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -45.96000000000038, 'blue_policy': -50.080000000000446}, 'num_module_steps_sampled_lifetime': {'red_policy': 324000.0, 'blue_policy': 324000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034667275919555496, 'episode_return_mean': -185.06000000000157, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -161.2000000000012, 'episode_duration_sec_mean': 18.86823551999405, 'episode_return_min': -205.9000000000018, 'rlmodule_inference_timer': 0.01354695562634269, 'num_episodes_lifetime': 135.0, 'episode_len_min': 1200, 'time_between_sampling': 326.5121341000777, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.519965933253527, 'throughput_since_last_restore': 14.921684834641058}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 119
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 675
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 999
(MultiAgentEnvRunner pid=37492) {'red_0': -75.1, 'red_1': -49.40000000000044, 'blue_0': -39.90000000000028, 'blue_1': -36.400000000000276}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 307
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 850
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1093
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1162
(MultiAgentEnvRunner pid=37492) {'red_0': -68.00000000000045, 'red_1': -66.00000000000041, 'blue_0': -48.500000000000426, 'blue_1': -20.100000000000172}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -36.30000000000024, 'red_1': -49.40000000000043, 'blue_0': -47.60000000000039, 'blue_1': -49.10000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 359
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 603
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 858
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 991
(MultiAgentEnvRunner pid=37492) {'red_0': -53.600000000000364, 'red_1': -57.300000000000416, 'blue_0': -32.20000000000042, 'blue_1': -30.600000000000367}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 476
(MultiAgentEnvRunner pid=37492) {'red_0': -46.000000000000334, 'red_1': -45.60000000000033, 'blue_0': -33.900000000000176, 'blue_1': -46.000000000000405}
ITERATION 27: reward=-186.20000000000135, metadata={'num_env_steps_sampled_lifetime': 168000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003418623023288361, 'timers': {'connectors': {'batch_individual_items': 9.947321989005465e-05, 'add_states_from_episodes_to_batch': 6.562653778849325e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3037906120328822e-05, 'numpy_to_tensor': 7.130620291110224e-05, 'agent_to_module_mapping': 8.354508137075038e-06, 'add_observations_from_episodes_to_batch': 4.11804630880231e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008788312656483463, 'timers': {'connectors': {'get_actions': 0.0004517651378201605, 'un_batch_to_individual_items': 6.505666991740041e-05, 'tensor_to_numpy': 0.00011900335761970299, 'module_to_agent_unmapping': 6.363095965980258e-06, 'normalize_and_clip_actions': 7.280005876803386e-05, 'listify_data_for_vector_env': 2.435951184426757e-05, 'remove_single_ts_time_rank_from_batch': 2.6226217455443183e-06}}}, 'sample': 93.02041209989693, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -53.540000000000404, 'blue_0': -40.42000000000034, 'blue_1': -36.440000000000325, 'red_0': -55.800000000000274}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 27.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 168000.0, 'blue_0': 168000.0, 'blue_1': 168000.0, 'red_0': 168000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -53.540000000000404, 'blue_policy': -36.440000000000325}, 'num_module_steps_sampled_lifetime': {'red_policy': 336000.0, 'blue_policy': 336000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034077861078288897, 'episode_return_mean': -186.20000000000135, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -171.50000000000125, 'episode_duration_sec_mean': 18.478346379986032, 'episode_return_min': -202.60000000000144, 'rlmodule_inference_timer': 0.013146520186789693, 'num_episodes_lifetime': 140.0, 'episode_len_min': 1200, 'time_between_sampling': 318.2113662000047, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.6016400102111, 'throughput_since_last_restore': 14.91000800045312}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 485
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 625
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 790
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1084
(MultiAgentEnvRunner pid=37492) {'red_0': -42.40000000000039, 'red_1': -32.30000000000018, 'blue_0': -48.000000000000334, 'blue_1': -32.50000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1141
(MultiAgentEnvRunner pid=37492) {'red_0': -55.50000000000043, 'red_1': -42.500000000000256, 'blue_0': -32.40000000000026, 'blue_1': -34.90000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 614
(MultiAgentEnvRunner pid=37492) {'red_0': -43.40000000000034, 'red_1': -49.300000000000445, 'blue_0': -49.30000000000041, 'blue_1': -33.80000000000021}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 372
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 697
(MultiAgentEnvRunner pid=37492) {'red_0': -48.20000000000043, 'red_1': -58.50000000000052, 'blue_0': -47.5000000000004, 'blue_1': -41.2000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 458
(MultiAgentEnvRunner pid=37492) {'red_0': -39.100000000000286, 'red_1': -36.000000000000234, 'blue_0': -51.100000000000435, 'blue_1': -54.70000000000048}
ITERATION 28: reward=-174.5200000000014, metadata={'num_env_steps_sampled_lifetime': 174000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003353653722483693, 'timers': {'connectors': {'batch_individual_items': 0.0001020645749452551, 'add_states_from_episodes_to_batch': 7.378636019019224e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2602590601275172e-05, 'numpy_to_tensor': 6.864460759987824e-05, 'agent_to_module_mapping': 8.15340581358618e-06, 'add_observations_from_episodes_to_batch': 3.936680002496285e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008748977858116823, 'timers': {'connectors': {'get_actions': 0.00045527188118650087, 'un_batch_to_individual_items': 6.466518760807101e-05, 'tensor_to_numpy': 0.00011719589453413266, 'module_to_agent_unmapping': 6.38840295974983e-06, 'normalize_and_clip_actions': 7.138918506525825e-05, 'listify_data_for_vector_env': 2.3789049745629974e-05, 'remove_single_ts_time_rank_from_batch': 2.3440193296737017e-06}}}, 'sample': 99.82259399991017, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -43.720000000000326, 'blue_0': -45.660000000000366, 'blue_1': -39.42000000000034, 'red_0': -45.720000000000375}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 28.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 174000.0, 'blue_0': 174000.0, 'blue_1': 174000.0, 'red_0': 174000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -43.720000000000326, 'blue_policy': -39.42000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 348000.0, 'blue_policy': 348000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033646885670988156, 'episode_return_mean': -174.5200000000014, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -155.20000000000118, 'episode_duration_sec_mean': 19.825744519988074, 'episode_return_min': -195.40000000000165, 'rlmodule_inference_timer': 0.012864768038725745, 'num_episodes_lifetime': 145.0, 'episode_len_min': 1200, 'time_between_sampling': 317.8868344000075, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.608959689682093, 'throughput_since_last_restore': 14.899416368005024}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 951
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 956
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 983
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1109
(MultiAgentEnvRunner pid=37492) {'red_0': -41.80000000000027, 'red_1': -50.10000000000037, 'blue_0': -33.00000000000024, 'blue_1': -29.400000000000368}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 543
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 705
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 786
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 872
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1089
(MultiAgentEnvRunner pid=37492) {'red_0': -54.900000000000446, 'red_1': -47.70000000000039, 'blue_0': -51.3000000000005, 'blue_1': -35.80000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1027
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1058
(MultiAgentEnvRunner pid=37492) {'red_0': -35.00000000000028, 'red_1': -33.90000000000021, 'blue_0': -42.200000000000315, 'blue_1': -62.50000000000059}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 700
(MultiAgentEnvRunner pid=37492) {'red_0': -41.60000000000031, 'red_1': -39.90000000000029, 'blue_0': -61.20000000000057, 'blue_1': -64.6000000000006}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 457
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 462
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 674
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 895
(MultiAgentEnvRunner pid=37492) {'red_0': -44.600000000000364, 'red_1': -56.200000000000436, 'blue_0': -37.600000000000264, 'blue_1': -24.700000000000053}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -111.09999999999796, 'red_1': -112.59999999999788, 'blue_0': -2.0000000000000004, 'blue_1': -1.9000000000000006}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.30000000000000004, 'blue_1': -0.2}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-25 18:37:04,862	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -118.59999999999754, 'red_1': -118.69999999999753, 'blue_0': -118.69999999999753, 'blue_1': -118.59999999999754}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.2, 'red_1': 0, 'blue_0': -0.30000000000000004, 'blue_1': -59.80000000000058}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.5, 'red_1': 0, 'blue_0': -1.8000000000000005, 'blue_1': -2.0000000000000004}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 29: reward=-177.60000000000144, metadata={'num_env_steps_sampled_lifetime': 180000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003505076707206969, 'timers': {'connectors': {'batch_individual_items': 0.00010360007607931571, 'add_states_from_episodes_to_batch': 6.987452652670468e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3354691901481633e-05, 'numpy_to_tensor': 7.202259027462362e-05, 'agent_to_module_mapping': 9.142833266075123e-06, 'add_observations_from_episodes_to_batch': 4.238542634483569e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009123650856807103, 'timers': {'connectors': {'get_actions': 0.0004714396550971089, 'un_batch_to_individual_items': 6.976076650422382e-05, 'tensor_to_numpy': 0.00012359667597675102, 'module_to_agent_unmapping': 6.645708041983635e-06, 'normalize_and_clip_actions': 7.498869531632395e-05, 'listify_data_for_vector_env': 2.4534228988378643e-05, 'remove_single_ts_time_rank_from_batch': 2.4113468434731224e-06}}}, 'sample': 95.2004213000182, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -45.560000000000336, 'blue_0': -45.06000000000038, 'blue_1': -43.40000000000041, 'red_0': -43.58000000000033}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 29.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 180000.0, 'blue_0': 180000.0, 'blue_1': 180000.0, 'red_0': 180000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -45.560000000000336, 'blue_policy': -43.40000000000041}, 'num_module_steps_sampled_lifetime': {'red_policy': 360000.0, 'blue_policy': 360000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003646952761761599, 'episode_return_mean': -177.60000000000144, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -154.30000000000123, 'episode_duration_sec_mean': 18.90973456003703, 'episode_return_min': -207.30000000000177, 'rlmodule_inference_timer': 0.013553903318118536, 'num_episodes_lifetime': 150.0, 'episode_len_min': 1200, 'time_between_sampling': 310.89477050001733, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.294780556131329, 'throughput_since_last_restore': 14.794936824894705}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 247
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 417
(MultiAgentEnvRunner pid=37492) {'red_0': -27.800000000000097, 'red_1': -31.400000000000155, 'blue_0': -47.500000000000334, 'blue_1': -52.8000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 242
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 460
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 511
(MultiAgentEnvRunner pid=37492) {'red_0': -42.20000000000031, 'red_1': -27.9000000000001, 'blue_0': -51.00000000000038, 'blue_1': -49.40000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 718
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1100
(MultiAgentEnvRunner pid=37492) {'red_0': -44.20000000000036, 'red_1': -53.60000000000046, 'blue_0': -45.00000000000042, 'blue_1': -48.50000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -48.400000000000404, 'red_1': -44.90000000000037, 'blue_0': -51.30000000000045, 'blue_1': -36.50000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 655
(MultiAgentEnvRunner pid=37492) {'red_0': -49.70000000000044, 'red_1': -43.20000000000034, 'blue_0': -56.800000000000516, 'blue_1': -47.60000000000039}
ITERATION 30: reward=-179.9400000000014, metadata={'num_env_steps_sampled_lifetime': 186000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000335843488277114, 'timers': {'connectors': {'batch_individual_items': 0.00010158141148840799, 'add_states_from_episodes_to_batch': 6.478352535353817e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2388465083495435e-05, 'numpy_to_tensor': 6.9805926456326e-05, 'agent_to_module_mapping': 8.26207581473614e-06, 'add_observations_from_episodes_to_batch': 3.916323867821e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008804283567190476, 'timers': {'connectors': {'get_actions': 0.00045524861372228453, 'un_batch_to_individual_items': 6.580787477009679e-05, 'tensor_to_numpy': 0.00011850142830253425, 'module_to_agent_unmapping': 6.5174048102720136e-06, 'normalize_and_clip_actions': 7.320408165518238e-05, 'listify_data_for_vector_env': 2.391035512852008e-05, 'remove_single_ts_time_rank_from_batch': 2.3868343495067423e-06}}}, 'sample': 92.92485599999782, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -40.20000000000029, 'blue_0': -50.32000000000043, 'blue_1': -46.96000000000037, 'red_0': -42.46000000000032}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 30.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 186000.0, 'blue_0': 186000.0, 'blue_1': 186000.0, 'red_0': 186000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -40.20000000000029, 'blue_policy': -46.96000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 372000.0, 'blue_policy': 372000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003372324313596775, 'episode_return_mean': -179.9400000000014, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -159.500000000001, 'episode_duration_sec_mean': 18.39834151996765, 'episode_return_min': -197.3000000000017, 'rlmodule_inference_timer': 0.01327325980021854, 'num_episodes_lifetime': 155.0, 'episode_len_min': 1200, 'time_between_sampling': 392.8245159999933, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.808455131794554, 'throughput_since_last_restore': 14.795366785862134}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 405.1247019000584, 'restore_env_runners': 1.1600088328123093e-05, 'training_step': 405.1244572000578, 'env_runner_sampling_timer': 93.06458709994331, 'learner_update_timer': 311.9962920000544, 'synch_weights': 0.01542790001258254, 'synch_env_connectors': 0.003072399995289743, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 186000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000335843488277114, 'timers': {'connectors': {'batch_individual_items': 0.00010158141148840799, 'add_states_from_episodes_to_batch': 6.478352535353817e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2388465083495435e-05, 'numpy_to_tensor': 6.9805926456326e-05, 'agent_to_module_mapping': 8.26207581473614e-06, 'add_observations_from_episodes_to_batch': 3.916323867821e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008804283567190476, 'timers': {'connectors': {'get_actions': 0.00045524861372228453, 'un_batch_to_individual_items': 6.580787477009679e-05, 'tensor_to_numpy': 0.00011850142830253425, 'module_to_agent_unmapping': 6.5174048102720136e-06, 'normalize_and_clip_actions': 7.320408165518238e-05, 'listify_data_for_vector_env': 2.391035512852008e-05, 'remove_single_ts_time_rank_from_batch': 2.3868343495067423e-06}}}, 'sample': 92.92485599999782, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -40.20000000000029, 'blue_0': -50.32000000000043, 'blue_1': -46.96000000000037, 'red_0': -42.46000000000032}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 30.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 186000.0, 'blue_0': 186000.0, 'blue_1': 186000.0, 'red_0': 186000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -40.20000000000029, 'blue_policy': -46.96000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 372000.0, 'blue_policy': 372000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003372324313596775, 'episode_return_mean': -179.9400000000014, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -159.500000000001, 'episode_duration_sec_mean': 18.39834151996765, 'episode_return_min': -197.3000000000017, 'rlmodule_inference_timer': 0.01327325980021854, 'num_episodes_lifetime': 155.0, 'episode_len_min': 1200, 'time_between_sampling': 392.8245159999933, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.808455131794554, 'throughput_since_last_restore': 14.795366785862134}}, 'learners': {'red_policy': {'policy_loss': -0.19792193174362183, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.01024395041167736, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 31.0, 'num_module_steps_trained_lifetime': 11169920.0, 'curr_entropy_coeff': 0.04721, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00028139999999999996, 'vf_explained_var': -0.06171917915344238, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.754592895507812, 'entropy': 1.7439892292022705, 'vf_loss_unclipped': 625.9713134765625, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 889.302379393571, 'throughput_since_last_restore': 888.5110698237991}}, 'blue_policy': {'weights_seq_no': 31.0, 'num_module_steps_trained_lifetime': 11169920.0, 'curr_entropy_coeff': 0.04721, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00028139999999999996, 'vf_explained_var': 0.9880465865135193, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.026210196316242218, 'total_loss': -0.4010757505893707, 'entropy': 1.7537877559661865, 'policy_loss': -0.36636078357696533, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.026210196316242218, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.01450465526431799, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 889.3005411864488, 'throughput_since_last_restore': 888.5110750326627}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.900052540004253e-06, 'batch_individual_items': 0.8033319000387564, 'add_time_dim_to_batch_and_zero_pad': 2.159993164241314e-05, 'numpy_to_tensor': 0.13794919999781996, 'add_observations_from_episodes_to_batch': 0.0003907999489456415, 'agent_to_module_mapping': 0.021929899929091334, 'add_one_ts_to_episodes_and_truncate': 0.19560389989055693, 'add_columns_from_episodes_to_train_batch': 0.5771014000056311, 'general_advantage_estimation': 16.073102099937387}}, 'connector_pipeline_timer': 17.809896099963225}, 'num_module_steps_trained_lifetime': 22339840.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 523590000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 41685.91694051441, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 41685.91545898137, 'throughput_since_last_restore': 41648.956889634544}, 'num_module_steps_trained_throughput': 1778.5990512426558, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1778.5990292943532, 'throughput_since_last_restore': 1777.0221579245735}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 186000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 31, 'trial_id': 'default', 'date': '2026-01-25_18-48-22', 'timestamp': 1769363302, 'time_this_iter_s': 405.13996839523315, 'time_total_s': 12552.189219474792, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 12552.189219474792, 'iterations_since_restore': 31, 'perf': {'cpu_util_percent': 15.513148788927337, 'ram_util_percent': 89.1749134948097}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 378
(MultiAgentEnvRunner pid=37492) {'red_0': -65.50000000000053, 'red_1': -53.400000000000475, 'blue_0': -48.60000000000041, 'blue_1': -44.300000000000345}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 358
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 375
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 925
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 995
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1097
(MultiAgentEnvRunner pid=37492) {'red_0': -36.50000000000046, 'red_1': -62.1000000000006, 'blue_0': -35.90000000000015, 'blue_1': -46.300000000000296}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 304
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 754
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 784
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 869
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1183
(MultiAgentEnvRunner pid=37492) {'red_0': -50.70000000000043, 'red_1': -46.9000000000004, 'blue_0': -51.40000000000046, 'blue_1': -47.50000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 347
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 790
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 880
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1022
(MultiAgentEnvRunner pid=37492) {'red_0': -54.60000000000037, 'red_1': -41.50000000000019, 'blue_0': -38.600000000000385, 'blue_1': -29.500000000000355}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 522
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1098
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1178
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1179
(MultiAgentEnvRunner pid=37492) {'red_0': -26.100000000000385, 'red_1': -28.300000000000196, 'blue_0': -73.70000000000014, 'blue_1': -59.60000000000045}
ITERATION 31: reward=-188.20000000000147, metadata={'num_env_steps_sampled_lifetime': 192000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033242076793695995, 'timers': {'connectors': {'batch_individual_items': 0.00010218772009673425, 'add_states_from_episodes_to_batch': 6.3892251252156175e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3792410105677468e-05, 'numpy_to_tensor': 6.819635973286979e-05, 'agent_to_module_mapping': 8.035754219258584e-06, 'add_observations_from_episodes_to_batch': 3.7816049266898956e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008586724054488018, 'timers': {'connectors': {'get_actions': 0.00044205584797228105, 'un_batch_to_individual_items': 6.43646500353474e-05, 'tensor_to_numpy': 0.00011550919591891488, 'module_to_agent_unmapping': 6.2719358310778805e-06, 'normalize_and_clip_actions': 7.130071179875867e-05, 'listify_data_for_vector_env': 2.3667882196150064e-05, 'remove_single_ts_time_rank_from_batch': 2.3385151681073446e-06}}}, 'sample': 91.48667750007007, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.440000000000374, 'blue_0': -49.64000000000031, 'blue_1': -45.44000000000038, 'red_0': -46.68000000000043}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 31.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 192000.0, 'blue_0': 192000.0, 'blue_1': 192000.0, 'red_0': 192000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.440000000000374, 'blue_policy': -45.44000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 384000.0, 'blue_policy': 384000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034206498915041375, 'episode_return_mean': -188.20000000000147, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -164.2000000000013, 'episode_duration_sec_mean': 18.17507702000439, 'episode_return_min': -211.80000000000177, 'rlmodule_inference_timer': 0.012774358345441151, 'num_episodes_lifetime': 160.0, 'episode_len_min': 1200, 'time_between_sampling': 312.3606085999636, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.094183364616816, 'throughput_since_last_restore': 14.80452296142067}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 611
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 833
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1125
(MultiAgentEnvRunner pid=37492) {'red_0': -36.2000000000003, 'red_1': -21.200000000000244, 'blue_0': -40.000000000000256, 'blue_1': -56.10000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 970
(MultiAgentEnvRunner pid=37492) {'red_0': -34.900000000000254, 'red_1': -45.10000000000036, 'blue_0': -35.00000000000023, 'blue_1': -47.700000000000394}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 623
(MultiAgentEnvRunner pid=37492) {'red_0': -41.200000000000344, 'red_1': -42.40000000000033, 'blue_0': -49.0000000000004, 'blue_1': -40.900000000000304}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 474
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 884
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 884
(MultiAgentEnvRunner pid=37492) {'red_0': -46.200000000000365, 'red_1': -51.000000000000455, 'blue_0': -36.20000000000022, 'blue_1': -40.700000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 228
(MultiAgentEnvRunner pid=37492) {'red_0': -48.20000000000041, 'red_1': -53.00000000000046, 'blue_0': -36.00000000000024, 'blue_1': -39.30000000000028}
ITERATION 32: reward=-168.0600000000013, metadata={'num_env_steps_sampled_lifetime': 198000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000343229679311416, 'timers': {'connectors': {'batch_individual_items': 0.00010359031995977301, 'add_states_from_episodes_to_batch': 6.875811148789299e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3532872266719866e-05, 'numpy_to_tensor': 7.037422830691412e-05, 'agent_to_module_mapping': 8.339440842358926e-06, 'add_observations_from_episodes_to_batch': 4.0072715051716526e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009055424339487301, 'timers': {'connectors': {'get_actions': 0.00046686672303141336, 'un_batch_to_individual_items': 6.92049831476814e-05, 'tensor_to_numpy': 0.00012220826989361771, 'module_to_agent_unmapping': 6.674968456620368e-06, 'normalize_and_clip_actions': 7.258478402380986e-05, 'listify_data_for_vector_env': 2.7223108342141756e-05, 'remove_single_ts_time_rank_from_batch': 2.5471762830436467e-06}}}, 'sample': 93.27674230001867, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -42.540000000000376, 'blue_0': -39.24000000000027, 'blue_1': -44.94000000000035, 'red_0': -41.34000000000034}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 32.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 198000.0, 'blue_0': 198000.0, 'blue_1': 198000.0, 'red_0': 198000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -42.540000000000376, 'blue_policy': -44.94000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 396000.0, 'blue_policy': 396000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003374725542558155, 'episode_return_mean': -168.0600000000013, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -153.50000000000125, 'episode_duration_sec_mean': 18.512911699991673, 'episode_return_min': -176.50000000000142, 'rlmodule_inference_timer': 0.013421861855361811, 'num_episodes_lifetime': 165.0, 'episode_len_min': 1200, 'time_between_sampling': 305.89643540000543, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.992451237082978, 'throughput_since_last_restore': 14.810145238741198}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 639
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 951
(MultiAgentEnvRunner pid=37492) {'red_0': -36.9000000000002, 'red_1': -45.300000000000296, 'blue_0': -53.80000000000052, 'blue_1': -43.70000000000054}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 367
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 400
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 876
(MultiAgentEnvRunner pid=37492) {'red_0': -51.400000000000354, 'red_1': -56.40000000000039, 'blue_0': -20.80000000000005, 'blue_1': -23.70000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1054
(MultiAgentEnvRunner pid=37492) {'red_0': -45.20000000000036, 'red_1': -40.200000000000294, 'blue_0': -56.00000000000053, 'blue_1': -41.000000000000306}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 214
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 291
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 972
(MultiAgentEnvRunner pid=37492) {'red_0': -45.20000000000041, 'red_1': -55.600000000000506, 'blue_0': -44.20000000000029, 'blue_1': -40.90000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 430
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 604
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1178
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1178
(MultiAgentEnvRunner pid=37492) {'red_0': -40.900000000000254, 'red_1': -36.300000000000196, 'blue_0': -34.100000000000264, 'blue_1': -40.900000000000375}
ITERATION 33: reward=-170.5000000000013, metadata={'num_env_steps_sampled_lifetime': 204000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034255624143966784, 'timers': {'connectors': {'batch_individual_items': 9.887779968034326e-05, 'add_states_from_episodes_to_batch': 6.750108594689477e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2828598137495163e-05, 'numpy_to_tensor': 7.273560678646668e-05, 'agent_to_module_mapping': 8.394740221091261e-06, 'add_observations_from_episodes_to_batch': 4.1977334765123576e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009064683457687372, 'timers': {'connectors': {'get_actions': 0.0004672537455724764, 'un_batch_to_individual_items': 6.788032402827175e-05, 'tensor_to_numpy': 0.0001226452753813576, 'module_to_agent_unmapping': 6.606098692537029e-06, 'normalize_and_clip_actions': 7.429311201122982e-05, 'listify_data_for_vector_env': 2.5377350201084777e-05, 'remove_single_ts_time_rank_from_batch': 2.9364965397905964e-06}}}, 'sample': 94.37308300007135, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.76000000000033, 'blue_0': -41.78000000000033, 'blue_1': -38.040000000000305, 'red_0': -43.920000000000314}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 33.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 204000.0, 'blue_0': 204000.0, 'blue_1': 204000.0, 'red_0': 204000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.76000000000033, 'blue_policy': -38.040000000000305}, 'num_module_steps_sampled_lifetime': {'red_policy': 408000.0, 'blue_policy': 408000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003507243977434866, 'episode_return_mean': -170.5000000000013, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -152.20000000000107, 'episode_duration_sec_mean': 18.73778285998851, 'episode_return_min': -185.90000000000146, 'rlmodule_inference_timer': 0.01401402588987621, 'num_episodes_lifetime': 170.0, 'episode_len_min': 1200, 'time_between_sampling': 306.9251220000442, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.8894084736516, 'throughput_since_last_restore': 14.863962545291079}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 698
(MultiAgentEnvRunner pid=37492) {'red_0': -42.30000000000032, 'red_1': -49.700000000000415, 'blue_0': -51.20000000000047, 'blue_1': -55.500000000000504}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 360
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 564
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 638
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 651
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1085
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1139
(MultiAgentEnvRunner pid=37492) {'red_0': -53.50000000000036, 'red_1': -61.600000000000435, 'blue_0': -8.699999999999987, 'blue_1': -40.000000000000426}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1005
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1124
(MultiAgentEnvRunner pid=37492) {'red_0': -39.700000000000294, 'red_1': -34.00000000000026, 'blue_0': -52.70000000000046, 'blue_1': -56.90000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 221
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 970
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 984
(MultiAgentEnvRunner pid=37492) {'red_0': -54.40000000000054, 'red_1': -36.00000000000045, 'blue_0': -47.90000000000033, 'blue_1': -49.60000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1101
(MultiAgentEnvRunner pid=37492) {'red_0': -47.800000000000395, 'red_1': -43.90000000000034, 'blue_0': -37.30000000000029, 'blue_1': -42.60000000000033}
ITERATION 34: reward=-181.0600000000015, metadata={'num_env_steps_sampled_lifetime': 210000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003247100588219024, 'timers': {'connectors': {'batch_individual_items': 9.660005057236742e-05, 'add_states_from_episodes_to_batch': 6.254910747997638e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2326609349428087e-05, 'numpy_to_tensor': 6.740189094266186e-05, 'agent_to_module_mapping': 8.244297741824496e-06, 'add_observations_from_episodes_to_batch': 3.769081799755901e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008555847135290061, 'timers': {'connectors': {'get_actions': 0.0004395662528935551, 'un_batch_to_individual_items': 6.416003367059982e-05, 'tensor_to_numpy': 0.00011641405383968137, 'module_to_agent_unmapping': 6.197723943834381e-06, 'normalize_and_clip_actions': 7.206885871010583e-05, 'listify_data_for_vector_env': 2.4076714061140523e-05, 'remove_single_ts_time_rank_from_batch': 2.324483567087705e-06}}}, 'sample': 88.35390690003987, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -45.04000000000038, 'blue_0': -39.56000000000031, 'blue_1': -48.92000000000043, 'red_0': -47.54000000000038}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 34.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 210000.0, 'blue_0': 210000.0, 'blue_1': 210000.0, 'red_0': 210000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -45.04000000000038, 'blue_policy': -48.92000000000043}, 'num_module_steps_sampled_lifetime': {'red_policy': 420000.0, 'blue_policy': 420000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032923416933266467, 'episode_return_mean': -181.0600000000015, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -163.8000000000012, 'episode_duration_sec_mean': 17.5356424999889, 'episode_return_min': -198.70000000000172, 'rlmodule_inference_timer': 0.012455047278736172, 'num_episodes_lifetime': 175.0, 'episode_len_min': 1200, 'time_between_sampling': 260.8836006000638, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.113707975269634, 'throughput_since_last_restore': 14.91999862220293}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 461
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 716
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 884
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 937
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 977
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1086
(MultiAgentEnvRunner pid=37492) {'red_0': -47.40000000000035, 'red_1': -57.90000000000053, 'blue_0': -28.400000000000162, 'blue_1': -39.60000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 803
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 812
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1131
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1131
(MultiAgentEnvRunner pid=37492) {'red_0': -52.800000000000445, 'red_1': -53.80000000000042, 'blue_0': -27.500000000000185, 'blue_1': -33.10000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -43.20000000000034, 'red_1': -41.50000000000031, 'blue_0': -53.10000000000047, 'blue_1': -37.30000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 489
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 606
(MultiAgentEnvRunner pid=37492) {'red_0': -37.20000000000029, 'red_1': -54.40000000000049, 'blue_0': -56.6000000000005, 'blue_1': -42.200000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 859
(MultiAgentEnvRunner pid=37492) {'red_0': -54.400000000000475, 'red_1': -51.80000000000046, 'blue_0': -36.900000000000254, 'blue_1': -44.40000000000038}
ITERATION 35: reward=-178.70000000000147, metadata={'num_env_steps_sampled_lifetime': 216000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003275710612298054, 'timers': {'connectors': {'batch_individual_items': 9.918401429765763e-05, 'add_states_from_episodes_to_batch': 6.368930256800542e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2409382821401479e-05, 'numpy_to_tensor': 6.769524726004475e-05, 'agent_to_module_mapping': 8.101503379815728e-06, 'add_observations_from_episodes_to_batch': 3.802284855037302e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008683963572674076, 'timers': {'connectors': {'get_actions': 0.0004473267636295598, 'un_batch_to_individual_items': 6.796539084978891e-05, 'tensor_to_numpy': 0.00011665771849698812, 'module_to_agent_unmapping': 6.4883789782977534e-06, 'normalize_and_clip_actions': 7.118196643117899e-05, 'listify_data_for_vector_env': 2.3839147392659414e-05, 'remove_single_ts_time_rank_from_batch': 2.31860103652911e-06}}}, 'sample': 89.0067700999789, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -51.880000000000436, 'blue_0': -40.50000000000032, 'blue_1': -39.32000000000031, 'red_0': -47.00000000000038}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 35.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 216000.0, 'blue_0': 216000.0, 'blue_1': 216000.0, 'red_0': 216000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -51.880000000000436, 'blue_policy': -39.32000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 432000.0, 'blue_policy': 432000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033360003430488443, 'episode_return_mean': -178.70000000000147, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -167.2000000000013, 'episode_duration_sec_mean': 17.64995998002123, 'episode_return_min': -190.40000000000157, 'rlmodule_inference_timer': 0.01271831787496924, 'num_episodes_lifetime': 180.0, 'episode_len_min': 1200, 'time_between_sampling': 262.2447660000762, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.015549499678844, 'throughput_since_last_restore': 14.971211991410518}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 558
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 799
(MultiAgentEnvRunner pid=37492) {'red_0': -42.30000000000029, 'red_1': -48.400000000000375, 'blue_0': -38.30000000000023, 'blue_1': -39.000000000000256}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -45.10000000000036, 'red_1': -44.80000000000035, 'blue_0': -49.60000000000041, 'blue_1': -52.100000000000456}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1091
(MultiAgentEnvRunner pid=37492) {'red_0': -45.60000000000038, 'red_1': -45.30000000000036, 'blue_0': -46.700000000000394, 'blue_1': -37.20000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 235
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 838
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 958
(MultiAgentEnvRunner pid=37492) {'red_0': -52.600000000000485, 'red_1': -51.100000000000435, 'blue_0': -43.90000000000033, 'blue_1': -42.90000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 268
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 909
(MultiAgentEnvRunner pid=37492) {'red_0': -59.70000000000049, 'red_1': -39.600000000000215, 'blue_0': -25.100000000000218, 'blue_1': -46.80000000000045}
ITERATION 36: reward=-179.22000000000145, metadata={'num_env_steps_sampled_lifetime': 222000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003348690318824799, 'timers': {'connectors': {'batch_individual_items': 9.782069409682172e-05, 'add_states_from_episodes_to_batch': 6.662495688983492e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2793500219026447e-05, 'numpy_to_tensor': 6.996259772297252e-05, 'agent_to_module_mapping': 8.572595096545852e-06, 'add_observations_from_episodes_to_batch': 3.952713220045094e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009065489936274826, 'timers': {'connectors': {'get_actions': 0.0004660589955133985, 'un_batch_to_individual_items': 6.844671175350928e-05, 'tensor_to_numpy': 0.00012349918963965327, 'module_to_agent_unmapping': 6.7501795431644195e-06, 'normalize_and_clip_actions': 7.505346737935668e-05, 'listify_data_for_vector_env': 2.53841973969877e-05, 'remove_single_ts_time_rank_from_batch': 2.4522251972102986e-06}}}, 'sample': 90.81205429998226, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -45.840000000000344, 'blue_0': -40.72000000000032, 'blue_1': -43.600000000000364, 'red_0': -49.0600000000004}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 36.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 222000.0, 'blue_0': 222000.0, 'blue_1': 222000.0, 'red_0': 222000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -45.840000000000344, 'blue_policy': -43.600000000000364}, 'num_module_steps_sampled_lifetime': {'red_policy': 444000.0, 'blue_policy': 444000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034167735167886016, 'episode_return_mean': -179.22000000000145, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -168.00000000000117, 'episode_duration_sec_mean': 18.03378009998705, 'episode_return_min': -191.6000000000016, 'rlmodule_inference_timer': 0.013197602422566113, 'num_episodes_lifetime': 185.0, 'episode_len_min': 1200, 'time_between_sampling': 263.61943960003555, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.985721962063916, 'throughput_since_last_restore': 15.019352918783255}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 944
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1128
(MultiAgentEnvRunner pid=37492) {'red_0': -41.20000000000027, 'red_1': -46.80000000000031, 'blue_0': -41.70000000000037, 'blue_1': -25.300000000000303}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 952
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 975
(MultiAgentEnvRunner pid=37492) {'red_0': -43.40000000000026, 'red_1': -45.70000000000031, 'blue_0': -33.30000000000044, 'blue_1': -31.800000000000253}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 550
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 841
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1064
(MultiAgentEnvRunner pid=37492) {'red_0': -42.30000000000027, 'red_1': -45.80000000000033, 'blue_0': -36.30000000000023, 'blue_1': -32.80000000000014}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 501
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1072
(MultiAgentEnvRunner pid=37492) {'red_0': -59.600000000000485, 'red_1': -37.00000000000033, 'blue_0': -60.30000000000062, 'blue_1': -38.1000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 523
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 830
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1102
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1139
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1180
(MultiAgentEnvRunner pid=37492) {'red_0': -35.10000000000039, 'red_1': -48.90000000000043, 'blue_0': -48.20000000000028, 'blue_1': -23.200000000000095}
ITERATION 37: reward=-163.3600000000013, metadata={'num_env_steps_sampled_lifetime': 228000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003362012844862335, 'timers': {'connectors': {'batch_individual_items': 9.624275386516898e-05, 'add_states_from_episodes_to_batch': 6.718254643492786e-06, 'add_time_dim_to_batch_and_zero_pad': 1.284250284885406e-05, 'numpy_to_tensor': 7.052381423240143e-05, 'agent_to_module_mapping': 8.341525990488332e-06, 'add_observations_from_episodes_to_batch': 3.927481266907243e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008980105671908946, 'timers': {'connectors': {'get_actions': 0.0004605904717582055, 'un_batch_to_individual_items': 6.840537788275796e-05, 'tensor_to_numpy': 0.00012214933648812333, 'module_to_agent_unmapping': 6.492257695816166e-06, 'normalize_and_clip_actions': 7.470819077244533e-05, 'listify_data_for_vector_env': 2.5063580041993312e-05, 'remove_single_ts_time_rank_from_batch': 2.409085164182534e-06}}}, 'sample': 89.64058379991911, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -44.84000000000034, 'blue_0': -43.96000000000039, 'blue_1': -30.24000000000022, 'red_0': -44.320000000000334}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 37.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 228000.0, 'blue_0': 228000.0, 'blue_1': 228000.0, 'red_0': 228000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -44.84000000000034, 'blue_policy': -30.24000000000022}, 'num_module_steps_sampled_lifetime': {'red_policy': 456000.0, 'blue_policy': 456000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035194357601381945, 'episode_return_mean': -163.3600000000013, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -154.20000000000124, 'episode_duration_sec_mean': 17.775220660003832, 'episode_return_min': -195.00000000000176, 'rlmodule_inference_timer': 0.01320031297561674, 'num_episodes_lifetime': 190.0, 'episode_len_min': 1200, 'time_between_sampling': 262.4315149000613, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.032709365820608, 'throughput_since_last_restore': 15.06621666374083}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -51.50000000000044, 'red_1': -34.60000000000022, 'blue_0': -39.30000000000029, 'blue_1': -42.00000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -44.400000000000354, 'red_1': -37.400000000000254, 'blue_0': -46.30000000000038, 'blue_1': -51.80000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 343
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 345
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 345
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 879
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 951
(MultiAgentEnvRunner pid=37492) {'red_0': -60.70000000000049, 'red_1': -40.30000000000038, 'blue_0': -18.699999999999978, 'blue_1': -48.000000000000455}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 849
(MultiAgentEnvRunner pid=37492) {'red_0': -41.00000000000031, 'red_1': -37.30000000000029, 'blue_0': -44.20000000000034, 'blue_1': -46.70000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 306
(MultiAgentEnvRunner pid=37492) {'red_0': -43.300000000000345, 'red_1': -40.20000000000029, 'blue_0': -65.60000000000052, 'blue_1': -66.20000000000049}
ITERATION 38: reward=-179.9000000000014, metadata={'num_env_steps_sampled_lifetime': 234000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031808961528013465, 'timers': {'connectors': {'batch_individual_items': 9.484983627465362e-05, 'add_states_from_episodes_to_batch': 6.241054945791786e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2057104036701617e-05, 'numpy_to_tensor': 6.614565669683989e-05, 'agent_to_module_mapping': 7.863185256920121e-06, 'add_observations_from_episodes_to_batch': 3.7456821530754756e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008365106334666661, 'timers': {'connectors': {'get_actions': 0.0004300296963987931, 'un_batch_to_individual_items': 6.296661166899933e-05, 'tensor_to_numpy': 0.00011370415302442073, 'module_to_agent_unmapping': 6.321275366314613e-06, 'normalize_and_clip_actions': 6.965367196154547e-05, 'listify_data_for_vector_env': 2.3033528973993783e-05, 'remove_single_ts_time_rank_from_batch': 2.2611519328644807e-06}}}, 'sample': 88.17979359999299, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -37.960000000000285, 'blue_0': -42.8200000000003, 'blue_1': -50.94000000000042, 'red_0': -48.18000000000039}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 38.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 234000.0, 'blue_0': 234000.0, 'blue_1': 234000.0, 'red_0': 234000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -37.960000000000285, 'blue_policy': -50.94000000000042}, 'num_module_steps_sampled_lifetime': {'red_policy': 468000.0, 'blue_policy': 468000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032215435691236084, 'episode_return_mean': -179.9000000000014, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -167.40000000000126, 'episode_duration_sec_mean': 17.512399279978126, 'episode_return_min': -215.30000000000166, 'rlmodule_inference_timer': 0.012377205658663407, 'num_episodes_lifetime': 195.0, 'episode_len_min': 1200, 'time_between_sampling': 262.61697730002925, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.22293064154099, 'throughput_since_last_restore': 15.11474584381403}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 426
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 468
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 620
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 872
(MultiAgentEnvRunner pid=37492) {'red_0': -68.30000000000024, 'red_1': -46.80000000000031, 'blue_0': -29.100000000000094, 'blue_1': -37.00000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -50.400000000000425, 'red_1': -43.300000000000345, 'blue_0': -55.100000000000485, 'blue_1': -44.60000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 136
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1038
(MultiAgentEnvRunner pid=37492) {'red_0': -25.300000000000143, 'red_1': -22.20000000000021, 'blue_0': -63.600000000000556, 'blue_1': -59.90000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 805
(MultiAgentEnvRunner pid=37492) {'red_0': -40.00000000000029, 'red_1': -37.40000000000028, 'blue_0': -41.50000000000032, 'blue_1': -67.00000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 668
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 782
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 819
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1178
(MultiAgentEnvRunner pid=37492) {'red_0': -33.400000000000354, 'red_1': -37.00000000000045, 'blue_0': -62.60000000000052, 'blue_1': -67.50000000000031}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.6, 'blue_1': -0.1}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -118.39999999999755, 'blue_1': -118.59999999999754}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-25 19:40:20,757	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -112.79999999999787, 'red_1': -114.69999999999776, 'blue_0': -112.89999999999786, 'blue_1': -112.49999999999788}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': -0.30000000000000004, 'blue_0': -0.7999999999999999, 'blue_1': -0.4}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -119.59999999999748, 'blue_1': -119.39999999999749}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 39: reward=-186.4000000000014, metadata={'num_env_steps_sampled_lifetime': 240000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031956792826977077, 'timers': {'connectors': {'batch_individual_items': 9.587639877824515e-05, 'add_states_from_episodes_to_batch': 6.265621732567289e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2180415988873551e-05, 'numpy_to_tensor': 6.566017942966628e-05, 'agent_to_module_mapping': 7.83493103707171e-06, 'add_observations_from_episodes_to_batch': 3.761907754565646e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008408365706319498, 'timers': {'connectors': {'get_actions': 0.0004326925725146831, 'un_batch_to_individual_items': 6.308839298843429e-05, 'tensor_to_numpy': 0.00011440960877993324, 'module_to_agent_unmapping': 6.334634984177355e-06, 'normalize_and_clip_actions': 6.972360676148955e-05, 'listify_data_for_vector_env': 2.3116998458363003e-05, 'remove_single_ts_time_rank_from_batch': 2.2823506578552335e-06}}}, 'sample': 87.7799878999358, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -37.34000000000032, 'blue_0': -50.38000000000039, 'blue_1': -55.200000000000365, 'red_0': -43.48000000000029}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 39.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 240000.0, 'blue_0': 240000.0, 'blue_1': 240000.0, 'red_0': 240000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -37.34000000000032, 'blue_policy': -55.200000000000365}, 'num_module_steps_sampled_lifetime': {'red_policy': 480000.0, 'blue_policy': 480000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033178200178686165, 'episode_return_mean': -186.4000000000014, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -171.00000000000142, 'episode_duration_sec_mean': 17.431202679988928, 'episode_return_min': -200.50000000000162, 'rlmodule_inference_timer': 0.012282904563775435, 'num_episodes_lifetime': 200.0, 'episode_len_min': 1200, 'time_between_sampling': 260.19766169996, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.732426573090821, 'throughput_since_last_restore': 15.076802053324155}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 834
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 844
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 844
(MultiAgentEnvRunner pid=37492) {'red_0': -58.700000000000536, 'red_1': -48.60000000000037, 'blue_0': -27.90000000000025, 'blue_1': -53.80000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 828
(MultiAgentEnvRunner pid=37492) {'red_0': -46.30000000000032, 'red_1': -56.900000000000446, 'blue_0': -33.70000000000028, 'blue_1': -28.70000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 391
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 785
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 856
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 873
(MultiAgentEnvRunner pid=37492) {'red_0': -55.50000000000043, 'red_1': -54.90000000000041, 'blue_0': -45.10000000000056, 'blue_1': -41.200000000000465}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 775
(MultiAgentEnvRunner pid=37492) {'red_0': -43.60000000000034, 'red_1': -70.20000000000024, 'blue_0': -46.20000000000039, 'blue_1': -39.30000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 923
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 936
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 936
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1010
(MultiAgentEnvRunner pid=37492) {'red_0': -49.70000000000032, 'red_1': -53.2000000000004, 'blue_0': -38.90000000000052, 'blue_1': -34.30000000000046}
ITERATION 40: reward=-185.3400000000016, metadata={'num_env_steps_sampled_lifetime': 246000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035239470562320333, 'timers': {'connectors': {'batch_individual_items': 0.00010039823862337752, 'add_states_from_episodes_to_batch': 7.030278447490201e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3751601597714515e-05, 'numpy_to_tensor': 7.256169857627868e-05, 'agent_to_module_mapping': 8.743264675494287e-06, 'add_observations_from_episodes_to_batch': 4.334925898441315e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009364768389174838, 'timers': {'connectors': {'get_actions': 0.00047967785450429113, 'un_batch_to_individual_items': 7.033677674487951e-05, 'tensor_to_numpy': 0.00012607417442317022, 'module_to_agent_unmapping': 6.921853950729604e-06, 'normalize_and_clip_actions': 7.856217456607027e-05, 'listify_data_for_vector_env': 2.634696160738489e-05, 'remove_single_ts_time_rank_from_batch': 2.522063181851013e-06}}}, 'sample': 89.1249877999071, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -56.760000000000375, 'blue_0': -38.3600000000004, 'blue_1': -39.46000000000042, 'red_0': -50.76000000000039}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 40.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 246000.0, 'blue_0': 246000.0, 'blue_1': 246000.0, 'red_0': 246000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -56.760000000000375, 'blue_policy': -39.46000000000042}, 'num_module_steps_sampled_lifetime': {'red_policy': 492000.0, 'blue_policy': 492000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003608520970615314, 'episode_return_mean': -185.3400000000016, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -165.6000000000014, 'episode_duration_sec_mean': 17.67900993998628, 'episode_return_min': -199.30000000000126, 'rlmodule_inference_timer': 0.014157407640578679, 'num_episodes_lifetime': 205.0, 'episode_len_min': 1200, 'time_between_sampling': 349.1450126999989, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.140231812731265, 'throughput_since_last_restore': 15.121198538931774}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 350.0166046000086, 'restore_env_runners': 1.1699972674250603e-05, 'training_step': 350.0163542999653, 'env_runner_sampling_timer': 89.2525583000388, 'learner_update_timer': 260.7096147000557, 'synch_weights': 0.013004999957047403, 'synch_env_connectors': 0.0022409999510273337, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 246000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035239470562320333, 'timers': {'connectors': {'batch_individual_items': 0.00010039823862337752, 'add_states_from_episodes_to_batch': 7.030278447490201e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3751601597714515e-05, 'numpy_to_tensor': 7.256169857627868e-05, 'agent_to_module_mapping': 8.743264675494287e-06, 'add_observations_from_episodes_to_batch': 4.334925898441315e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009364768389174838, 'timers': {'connectors': {'get_actions': 0.00047967785450429113, 'un_batch_to_individual_items': 7.033677674487951e-05, 'tensor_to_numpy': 0.00012607417442317022, 'module_to_agent_unmapping': 6.921853950729604e-06, 'normalize_and_clip_actions': 7.856217456607027e-05, 'listify_data_for_vector_env': 2.634696160738489e-05, 'remove_single_ts_time_rank_from_batch': 2.522063181851013e-06}}}, 'sample': 89.1249877999071, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -56.760000000000375, 'blue_0': -38.3600000000004, 'blue_1': -39.46000000000042, 'red_0': -50.76000000000039}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 40.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 246000.0, 'blue_0': 246000.0, 'blue_1': 246000.0, 'red_0': 246000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -56.760000000000375, 'blue_policy': -39.46000000000042}, 'num_module_steps_sampled_lifetime': {'red_policy': 492000.0, 'blue_policy': 492000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003608520970615314, 'episode_return_mean': -185.3400000000016, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -165.6000000000014, 'episode_duration_sec_mean': 17.67900993998628, 'episode_return_min': -199.30000000000126, 'rlmodule_inference_timer': 0.014157407640578679, 'num_episodes_lifetime': 205.0, 'episode_len_min': 1200, 'time_between_sampling': 349.1450126999989, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.140231812731265, 'throughput_since_last_restore': 15.121198538931774}}, 'learners': {'red_policy': {'policy_loss': -0.1248912364244461, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.010349133983254433, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 41.0, 'num_module_steps_trained_lifetime': 14773120.0, 'curr_entropy_coeff': 0.046310000000000004, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00027539999999999997, 'vf_explained_var': -0.06580531597137451, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.829926490783691, 'entropy': 1.7359323501586914, 'vf_loss_unclipped': 569.80810546875, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1029.333267087739, 'throughput_since_last_restore': 908.078357804541}}, 'blue_policy': {'weights_seq_no': 41.0, 'num_module_steps_trained_lifetime': 14773120.0, 'curr_entropy_coeff': 0.046310000000000004, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00027539999999999997, 'vf_explained_var': 0.8545747995376587, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 1.0640158653259277, 'total_loss': -0.17833459377288818, 'entropy': 1.751393437385559, 'policy_loss': -0.3009975552558899, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.18140888214111328, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014827141538262367, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1029.3315236587623, 'throughput_since_last_restore': 908.0783618457705}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 7.00005330145359e-06, 'batch_individual_items': 0.8060436999658123, 'add_time_dim_to_batch_and_zero_pad': 2.3699947632849216e-05, 'numpy_to_tensor': 0.13628480001352727, 'add_observations_from_episodes_to_batch': 0.0002943000290542841, 'agent_to_module_mapping': 0.022652899962849915, 'add_one_ts_to_episodes_and_truncate': 0.17212370003107935, 'add_columns_from_episodes_to_train_batch': 0.48771610006224364, 'general_advantage_estimation': 13.172093800036237}}, 'connector_pipeline_timer': 14.797687500016764}, 'num_module_steps_trained_lifetime': 29546240.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 692490000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48249.86927206335, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48249.86699776833, 'throughput_since_last_restore': 42566.17340435425}, 'num_module_steps_trained_throughput': 2058.660981318899, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2058.660956031084, 'throughput_since_last_restore': 1816.1567297756997}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 246000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 41, 'trial_id': 'default', 'date': '2026-01-25_19-49-59', 'timestamp': 1769366999, 'time_this_iter_s': 350.0300235748291, 'time_total_s': 16248.886445999146, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 16248.886445999146, 'iterations_since_restore': 41, 'perf': {'cpu_util_percent': 15.071142284569136, 'ram_util_percent': 91.68416833667334}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -47.800000000000395, 'red_1': -37.30000000000025, 'blue_0': -46.20000000000038, 'blue_1': -45.00000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 787
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1104
(MultiAgentEnvRunner pid=37492) {'red_0': -56.900000000000524, 'red_1': -45.500000000000384, 'blue_0': -62.50000000000061, 'blue_1': -45.200000000000365}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1050
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1102
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1108
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1108
(MultiAgentEnvRunner pid=37492) {'red_0': -22.30000000000024, 'red_1': -37.4000000000003, 'blue_0': -49.60000000000038, 'blue_1': -51.50000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 409
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1171
(MultiAgentEnvRunner pid=37492) {'red_0': -48.90000000000035, 'red_1': -68.5000000000004, 'blue_0': -36.100000000000314, 'blue_1': -21.700000000000244}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1045
(MultiAgentEnvRunner pid=37492) {'red_0': -36.80000000000028, 'red_1': -44.30000000000036, 'blue_0': -62.90000000000058, 'blue_1': -40.4000000000003}
ITERATION 41: reward=-181.3600000000015, metadata={'num_env_steps_sampled_lifetime': 252000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032113907014630946, 'timers': {'connectors': {'batch_individual_items': 9.52448730112288e-05, 'add_states_from_episodes_to_batch': 6.2307398860982545e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2810388944606187e-05, 'numpy_to_tensor': 6.589144886671675e-05, 'agent_to_module_mapping': 7.80461179427126e-06, 'add_observations_from_episodes_to_batch': 3.792801409249498e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008510913073975354, 'timers': {'connectors': {'get_actions': 0.00043946624821584025, 'un_batch_to_individual_items': 6.407660036273938e-05, 'tensor_to_numpy': 0.00011494838012577425, 'module_to_agent_unmapping': 6.221988561302279e-06, 'normalize_and_clip_actions': 7.014660345214713e-05, 'listify_data_for_vector_env': 2.3321271702874666e-05, 'remove_single_ts_time_rank_from_batch': 2.2594186064915775e-06}}}, 'sample': 89.9185548000969, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.600000000000335, 'blue_0': -51.460000000000456, 'blue_1': -40.76000000000034, 'red_0': -42.54000000000036}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 41.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 252000.0, 'blue_0': 252000.0, 'blue_1': 252000.0, 'red_0': 252000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.600000000000335, 'blue_policy': -40.76000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 504000.0, 'blue_policy': 504000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032733914883194005, 'episode_return_mean': -181.3600000000015, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -160.80000000000135, 'episode_duration_sec_mean': 17.866709899995477, 'episode_return_min': -210.10000000000187, 'rlmodule_inference_timer': 0.012527617342288373, 'num_episodes_lifetime': 210.0, 'episode_len_min': 1200, 'time_between_sampling': 261.01264380000066, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.09455808077567, 'throughput_since_last_restore': 15.162871971410402}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 498
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 721
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1167
(MultiAgentEnvRunner pid=37492) {'red_0': -43.100000000000364, 'red_1': -35.10000000000026, 'blue_0': -40.30000000000028, 'blue_1': -66.50000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 268
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 427
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 476
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 476
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 825
(MultiAgentEnvRunner pid=37492) {'red_0': -58.90000000000047, 'red_1': -65.50000000000043, 'blue_0': -29.200000000000294, 'blue_1': -24.300000000000058}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 459
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 605
(MultiAgentEnvRunner pid=37492) {'red_0': -31.80000000000016, 'red_1': -28.20000000000012, 'blue_0': -49.00000000000038, 'blue_1': -53.30000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 553
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 911
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1130
(MultiAgentEnvRunner pid=37492) {'red_0': -45.30000000000037, 'red_1': -48.70000000000046, 'blue_0': -53.50000000000046, 'blue_1': -43.300000000000324}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1049
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1171
(MultiAgentEnvRunner pid=37492) {'red_0': -41.00000000000032, 'red_1': -48.3000000000004, 'blue_0': -52.70000000000046, 'blue_1': -45.50000000000039}
ITERATION 42: reward=-180.70000000000135, metadata={'num_env_steps_sampled_lifetime': 258000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032539132608570755, 'timers': {'connectors': {'batch_individual_items': 9.403383244206024e-05, 'add_states_from_episodes_to_batch': 6.348563403023229e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2986881680234319e-05, 'numpy_to_tensor': 6.680805593332614e-05, 'agent_to_module_mapping': 8.0980922616297e-06, 'add_observations_from_episodes_to_batch': 4.107333029959638e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008708418161305144, 'timers': {'connectors': {'get_actions': 0.0004454085819932398, 'un_batch_to_individual_items': 6.629006712575893e-05, 'tensor_to_numpy': 0.00011934725980323727, 'module_to_agent_unmapping': 6.377166517708736e-06, 'normalize_and_clip_actions': 7.272176675676806e-05, 'listify_data_for_vector_env': 2.4200863003478235e-05, 'remove_single_ts_time_rank_from_batch': 2.3281637113746094e-06}}}, 'sample': 89.37854669999797, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -45.16000000000033, 'blue_0': -44.940000000000374, 'blue_1': -46.58000000000033, 'red_0': -44.02000000000033}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 42.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 258000.0, 'blue_0': 258000.0, 'blue_1': 258000.0, 'red_0': 258000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -45.16000000000033, 'blue_policy': -46.58000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 516000.0, 'blue_policy': 516000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032744012703333987, 'episode_return_mean': -180.70000000000135, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -162.3000000000011, 'episode_duration_sec_mean': 17.75008288002573, 'episode_return_min': -190.8000000000016, 'rlmodule_inference_timer': 0.012634724777156926, 'num_episodes_lifetime': 215.0, 'episode_len_min': 1200, 'time_between_sampling': 260.9876756001031, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.12733192113583, 'throughput_since_last_restore': 15.203422814741451}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 808
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 814
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1172
(MultiAgentEnvRunner pid=37492) {'red_0': -52.900000000000475, 'red_1': -56.7000000000005, 'blue_0': -37.2000000000003, 'blue_1': -45.800000000000374}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 836
(MultiAgentEnvRunner pid=37492) {'red_0': -40.00000000000023, 'red_1': -55.10000000000043, 'blue_0': -45.800000000000445, 'blue_1': -45.700000000000564}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 667
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 987
(MultiAgentEnvRunner pid=37492) {'red_0': -42.30000000000032, 'red_1': -49.00000000000045, 'blue_0': -40.900000000000304, 'blue_1': -55.500000000000526}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 616
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 694
(MultiAgentEnvRunner pid=37492) {'red_0': -33.200000000000244, 'red_1': -33.3000000000002, 'blue_0': -56.10000000000044, 'blue_1': -66.80000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 383
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1170
(MultiAgentEnvRunner pid=37492) {'red_0': -49.40000000000035, 'red_1': -47.300000000000324, 'blue_0': -25.900000000000297, 'blue_1': -39.50000000000036}
ITERATION 43: reward=-183.68000000000148, metadata={'num_env_steps_sampled_lifetime': 264000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033779485767101053, 'timers': {'connectors': {'batch_individual_items': 9.850873484211496e-05, 'add_states_from_episodes_to_batch': 6.657749968761804e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2982110098430207e-05, 'numpy_to_tensor': 7.005915636434815e-05, 'agent_to_module_mapping': 8.4159022264685e-06, 'add_observations_from_episodes_to_batch': 4.071736145136297e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008878235398301024, 'timers': {'connectors': {'get_actions': 0.00045428567069527527, 'un_batch_to_individual_items': 6.709946043412685e-05, 'tensor_to_numpy': 0.00012066632575154016, 'module_to_agent_unmapping': 6.598006440439295e-06, 'normalize_and_clip_actions': 7.359369343948115e-05, 'listify_data_for_vector_env': 2.534913881943124e-05, 'remove_single_ts_time_rank_from_batch': 2.4124497974972313e-06}}}, 'sample': 89.51812689995859, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -48.280000000000385, 'blue_0': -41.180000000000355, 'blue_1': -50.660000000000444, 'red_0': -43.56000000000032}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 43.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 264000.0, 'blue_0': 264000.0, 'blue_1': 264000.0, 'red_0': 264000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -48.280000000000385, 'blue_policy': -50.660000000000444}, 'num_module_steps_sampled_lifetime': {'red_policy': 528000.0, 'blue_policy': 528000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003458500009919283, 'episode_return_mean': -183.68000000000148, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -162.10000000000133, 'episode_duration_sec_mean': 17.778263779985718, 'episode_return_min': -192.60000000000164, 'rlmodule_inference_timer': 0.013111580359927296, 'num_episodes_lifetime': 220.0, 'episode_len_min': 1200, 'time_between_sampling': 260.94285530003253, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.08704583732158, 'throughput_since_last_restore': 15.241606524275912}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 612
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 649
(MultiAgentEnvRunner pid=37492) {'red_0': -46.0000000000004, 'red_1': -32.50000000000022, 'blue_0': -53.800000000000466, 'blue_1': -55.50000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 449
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 882
(MultiAgentEnvRunner pid=37492) {'red_0': -47.100000000000385, 'red_1': -49.00000000000045, 'blue_0': -43.90000000000038, 'blue_1': -47.80000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 897
(MultiAgentEnvRunner pid=37492) {'red_0': -48.80000000000041, 'red_1': -42.50000000000035, 'blue_0': -65.70000000000051, 'blue_1': -46.10000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 636
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 679
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1072
(MultiAgentEnvRunner pid=37492) {'red_0': -51.40000000000041, 'red_1': -52.500000000000405, 'blue_0': -35.30000000000029, 'blue_1': -35.00000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -34.80000000000022, 'red_1': -54.30000000000048, 'blue_0': -41.600000000000314, 'blue_1': -41.30000000000031}
ITERATION 44: reward=-184.98000000000152, metadata={'num_env_steps_sampled_lifetime': 270000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003245422684487949, 'timers': {'connectors': {'batch_individual_items': 9.946376515489565e-05, 'add_states_from_episodes_to_batch': 6.267542428673719e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2158768073289148e-05, 'numpy_to_tensor': 6.65795343727752e-05, 'agent_to_module_mapping': 7.855542326324579e-06, 'add_observations_from_episodes_to_batch': 3.806376326284718e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008538145599013792, 'timers': {'connectors': {'get_actions': 0.0004391097335947643, 'un_batch_to_individual_items': 6.41526533428679e-05, 'tensor_to_numpy': 0.00011593325919609974, 'module_to_agent_unmapping': 6.342503485149537e-06, 'normalize_and_clip_actions': 7.119335648386286e-05, 'listify_data_for_vector_env': 2.3518727102569947e-05, 'remove_single_ts_time_rank_from_batch': 2.3078103259855803e-06}}}, 'sample': 88.30686130002141, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.16000000000038, 'blue_0': -48.060000000000386, 'blue_1': -45.14000000000037, 'red_0': -45.62000000000036}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 44.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 270000.0, 'blue_0': 270000.0, 'blue_1': 270000.0, 'red_0': 270000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.16000000000038, 'blue_policy': -45.14000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 540000.0, 'blue_policy': 540000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003297755562849375, 'episode_return_mean': -184.98000000000152, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -172.0000000000013, 'episode_duration_sec_mean': 17.523844400024974, 'episode_return_min': -203.10000000000161, 'rlmodule_inference_timer': 0.012393528483204979, 'num_episodes_lifetime': 225.0, 'episode_len_min': 1200, 'time_between_sampling': 261.6273820999777, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.176422330772546, 'throughput_since_last_restore': 15.279851818804032}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 528
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 775
(MultiAgentEnvRunner pid=37492) {'red_0': -33.600000000000364, 'red_1': -32.000000000000256, 'blue_0': -51.90000000000039, 'blue_1': -48.300000000000345}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 226
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 477
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 477
(MultiAgentEnvRunner pid=37492) {'red_0': -49.40000000000049, 'red_1': -43.80000000000038, 'blue_0': -34.900000000000205, 'blue_1': -47.00000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 640
(MultiAgentEnvRunner pid=37492) {'red_0': -36.000000000000234, 'red_1': -46.30000000000038, 'blue_0': -42.300000000000324, 'blue_1': -48.80000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 562
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 592
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 737
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 830
(MultiAgentEnvRunner pid=37492) {'red_0': -63.900000000000475, 'red_1': -70.60000000000008, 'blue_0': -19.400000000000038, 'blue_1': -38.30000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 720
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 798
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1029
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1047
(MultiAgentEnvRunner pid=37492) {'red_0': -47.40000000000032, 'red_1': -38.90000000000019, 'blue_0': -55.20000000000056, 'blue_1': -26.400000000000368}
ITERATION 45: reward=-174.88000000000133, metadata={'num_env_steps_sampled_lifetime': 276000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003279971821305167, 'timers': {'connectors': {'batch_individual_items': 9.893034093094289e-05, 'add_states_from_episodes_to_batch': 6.457844209722842e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2543348144983887e-05, 'numpy_to_tensor': 6.68855467428551e-05, 'agent_to_module_mapping': 8.32409534564389e-06, 'add_observations_from_episodes_to_batch': 3.848522115650759e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008685425053534107, 'timers': {'connectors': {'get_actions': 0.0004474491528835823, 'un_batch_to_individual_items': 6.524741812756573e-05, 'tensor_to_numpy': 0.00011812048946932258, 'module_to_agent_unmapping': 6.618901328854763e-06, 'normalize_and_clip_actions': 7.163671162761478e-05, 'listify_data_for_vector_env': 2.405722355137954e-05, 'remove_single_ts_time_rank_from_batch': 2.341784226283519e-06}}}, 'sample': 89.01081929996144, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -46.320000000000256, 'blue_0': -40.74000000000031, 'blue_1': -41.7600000000004, 'red_0': -46.06000000000038}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 45.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 276000.0, 'blue_0': 276000.0, 'blue_1': 276000.0, 'red_0': 276000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -46.320000000000256, 'blue_policy': -41.7600000000004}, 'num_module_steps_sampled_lifetime': {'red_policy': 552000.0, 'blue_policy': 552000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003339801794208557, 'episode_return_mean': -174.88000000000133, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -165.80000000000135, 'episode_duration_sec_mean': 17.682619599998, 'episode_return_min': -192.2000000000011, 'rlmodule_inference_timer': 0.012754402454833306, 'num_episodes_lifetime': 230.0, 'episode_len_min': 1200, 'time_between_sampling': 261.0108404000057, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.175167494871257, 'throughput_since_last_restore': 15.316593825728756}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 428
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 786
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 819
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1036
(MultiAgentEnvRunner pid=37492) {'red_0': -47.500000000000355, 'red_1': -42.900000000000375, 'blue_0': -37.300000000000225, 'blue_1': -38.80000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -40.5000000000003, 'red_1': -65.30000000000054, 'blue_0': -41.30000000000031, 'blue_1': -45.60000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 319
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 777
(MultiAgentEnvRunner pid=37492) {'red_0': -50.10000000000043, 'red_1': -53.000000000000455, 'blue_0': -45.30000000000039, 'blue_1': -48.800000000000395}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 495
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 785
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1077
(MultiAgentEnvRunner pid=37492) {'red_0': -59.500000000000476, 'red_1': -40.50000000000026, 'blue_0': -35.40000000000044, 'blue_1': -40.50000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 687
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 872
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 901
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 901
(MultiAgentEnvRunner pid=37492) {'red_0': -45.200000000000315, 'red_1': -40.200000000000294, 'blue_0': -28.500000000000288, 'blue_1': -49.00000000000048}
ITERATION 46: reward=-179.04000000000144, metadata={'num_env_steps_sampled_lifetime': 282000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003200171855233591, 'timers': {'connectors': {'batch_individual_items': 9.559927490311852e-05, 'add_states_from_episodes_to_batch': 6.275769071935862e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2259961918806262e-05, 'numpy_to_tensor': 6.580943471886833e-05, 'agent_to_module_mapping': 8.183206209185526e-06, 'add_observations_from_episodes_to_batch': 3.8139051912476476e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008641580978007272, 'timers': {'connectors': {'get_actions': 0.00044615000836589606, 'un_batch_to_individual_items': 6.464792374426952e-05, 'tensor_to_numpy': 0.00011651247673385119, 'module_to_agent_unmapping': 6.340891444531607e-06, 'normalize_and_clip_actions': 7.165257094935352e-05, 'listify_data_for_vector_env': 2.3949235064377848e-05, 'remove_single_ts_time_rank_from_batch': 2.330525157607106e-06}}}, 'sample': 88.64092549995985, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -48.38000000000038, 'blue_0': -37.56000000000033, 'blue_1': -44.54000000000036, 'red_0': -48.56000000000038}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 46.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 282000.0, 'blue_0': 282000.0, 'blue_1': 282000.0, 'red_0': 282000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -48.38000000000038, 'blue_policy': -44.54000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 564000.0, 'blue_policy': 564000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033085367096235955, 'episode_return_mean': -179.04000000000144, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -162.90000000000137, 'episode_duration_sec_mean': 17.59233113997616, 'episode_return_min': -197.20000000000164, 'rlmodule_inference_timer': 0.012533679713403738, 'num_episodes_lifetime': 235.0, 'episode_len_min': 1200, 'time_between_sampling': 260.3357430999167, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.15097610132585, 'throughput_since_last_restore': 15.351526084698792}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -53.200000000000465, 'red_1': -34.20000000000021, 'blue_0': -40.5000000000003, 'blue_1': -53.900000000000475}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 941
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1174
(MultiAgentEnvRunner pid=37492) {'red_0': -38.10000000000029, 'red_1': -42.80000000000035, 'blue_0': -41.0000000000003, 'blue_1': -47.50000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 726
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 838
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 848
(MultiAgentEnvRunner pid=37492) {'red_0': -54.30000000000041, 'red_1': -56.10000000000043, 'blue_0': -25.10000000000018, 'blue_1': -33.00000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 455
(MultiAgentEnvRunner pid=37492) {'red_0': -36.900000000000254, 'red_1': -41.60000000000031, 'blue_0': -47.700000000000394, 'blue_1': -60.100000000000556}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 402
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 549
(MultiAgentEnvRunner pid=37492) {'red_0': -36.50000000000028, 'red_1': -38.60000000000028, 'blue_0': -51.70000000000044, 'blue_1': -38.20000000000027}
ITERATION 47: reward=-174.20000000000138, metadata={'num_env_steps_sampled_lifetime': 288000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031538796964326307, 'timers': {'connectors': {'batch_individual_items': 9.51786279607709e-05, 'add_states_from_episodes_to_batch': 6.152579211122542e-06, 'add_time_dim_to_batch_and_zero_pad': 1.192919769047762e-05, 'numpy_to_tensor': 6.557846661520463e-05, 'agent_to_module_mapping': 7.708525000546928e-06, 'add_observations_from_episodes_to_batch': 3.7243001420100544e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008500300198800749, 'timers': {'connectors': {'get_actions': 0.0004411856187798995, 'un_batch_to_individual_items': 6.345973236570088e-05, 'tensor_to_numpy': 0.00011493349312837476, 'module_to_agent_unmapping': 6.315609286970279e-06, 'normalize_and_clip_actions': 7.007984223713575e-05, 'listify_data_for_vector_env': 2.3434545503033776e-05, 'remove_single_ts_time_rank_from_batch': 2.236765614376704e-06}}}, 'sample': 89.2665840999689, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -42.660000000000316, 'blue_0': -41.20000000000032, 'blue_1': -46.540000000000404, 'red_0': -43.80000000000034}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 47.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 288000.0, 'blue_0': 288000.0, 'blue_1': 288000.0, 'red_0': 288000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -42.660000000000316, 'blue_policy': -46.540000000000404}, 'num_module_steps_sampled_lifetime': {'red_policy': 576000.0, 'blue_policy': 576000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00031662915254983953, 'episode_return_mean': -174.20000000000138, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -165.00000000000128, 'episode_duration_sec_mean': 17.732916199974717, 'episode_return_min': -186.30000000000152, 'rlmodule_inference_timer': 0.012445009161199946, 'num_episodes_lifetime': 240.0, 'episode_len_min': 1200, 'time_between_sampling': 261.194270899985, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.135504772659374, 'throughput_since_last_restore': 15.384893422266694}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1060
(MultiAgentEnvRunner pid=37492) {'red_0': -51.10000000000044, 'red_1': -37.900000000000254, 'blue_0': -45.0000000000004, 'blue_1': -39.20000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -37.00000000000025, 'red_1': -54.400000000000475, 'blue_0': -39.50000000000029, 'blue_1': -57.90000000000054}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 528
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 624
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 909
(MultiAgentEnvRunner pid=37492) {'red_0': -43.4000000000003, 'red_1': -58.70000000000049, 'blue_0': -37.000000000000384, 'blue_1': -37.80000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 944
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1117
(MultiAgentEnvRunner pid=37492) {'red_0': -35.100000000000186, 'red_1': -46.000000000000306, 'blue_0': -41.40000000000037, 'blue_1': -29.20000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 809
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1148
(MultiAgentEnvRunner pid=37492) {'red_0': -41.20000000000027, 'red_1': -53.500000000000426, 'blue_0': -61.80000000000063, 'blue_1': -30.900000000000258}
ITERATION 48: reward=-175.60000000000142, metadata={'num_env_steps_sampled_lifetime': 294000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003375666861167964, 'timers': {'connectors': {'batch_individual_items': 9.565009884813325e-05, 'add_states_from_episodes_to_batch': 7.058365967392333e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3063096155197882e-05, 'numpy_to_tensor': 7.04964055141054e-05, 'agent_to_module_mapping': 9.47882766918907e-06, 'add_observations_from_episodes_to_batch': 4.089139287052156e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008896336786731804, 'timers': {'connectors': {'get_actions': 0.00045387430060630883, 'un_batch_to_individual_items': 6.715356421741117e-05, 'tensor_to_numpy': 0.00012209061634974074, 'module_to_agent_unmapping': 6.667283754013321e-06, 'normalize_and_clip_actions': 7.43514755110295e-05, 'listify_data_for_vector_env': 2.4587027250683e-05, 'remove_single_ts_time_rank_from_batch': 2.454792920962992e-06}}}, 'sample': 89.02793980005663, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -50.10000000000039, 'blue_0': -44.94000000000041, 'blue_1': -39.000000000000334, 'red_0': -41.560000000000294}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 48.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 294000.0, 'blue_0': 294000.0, 'blue_1': 294000.0, 'red_0': 294000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -50.10000000000039, 'blue_policy': -39.000000000000334}, 'num_module_steps_sampled_lifetime': {'red_policy': 588000.0, 'blue_policy': 588000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003474309044950886, 'episode_return_mean': -175.60000000000142, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -151.7000000000012, 'episode_duration_sec_mean': 17.68210091998335, 'episode_return_min': -188.80000000000155, 'rlmodule_inference_timer': 0.013242931383675804, 'num_episodes_lifetime': 245.0, 'episode_len_min': 1200, 'time_between_sampling': 260.89152549998835, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.152699685389354, 'throughput_since_last_restore': 15.417318832865819}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1077
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -53.90000000000047, 'red_1': -35.600000000000236, 'blue_0': -45.50000000000036, 'blue_1': -39.00000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 273
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 441
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 555
(MultiAgentEnvRunner pid=41856) {'red_0': -118.99999999999751, 'red_1': -118.89999999999752, 'blue_0': -119.2999999999975, 'blue_1': -118.99999999999751}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1034
(MultiAgentEnvRunner pid=37492) {'red_0': -36.40000000000026, 'red_1': -43.3000000000003, 'blue_0': -48.90000000000045, 'blue_1': -52.8000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 352
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 404
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1067
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1072
(MultiAgentEnvRunner pid=37492) {'red_0': -54.60000000000048, 'red_1': -41.00000000000031, 'blue_0': -30.40000000000014, 'blue_1': -42.40000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.2, 'blue_0': -0.1, 'blue_1': -0.30000000000000004}
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 746
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -42.50000000000033, 'red_1': -44.60000000000035, 'blue_0': -48.500000000000405, 'blue_1': -43.60000000000037}
(MultiAgentEnvRunner pid=37492) RESET
2026-01-25 20:38:40,814	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': -0.1, 'blue_0': -0.7999999999999999, 'blue_1': -0.6}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) PICKED UP by blue_0 at STEP 155
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1147
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1185
(MultiAgentEnvRunner pid=37492) {'red_0': -33.30000000000023, 'red_1': -41.20000000000034, 'blue_0': -55.70000000000049, 'blue_1': -55.60000000000049}
(MultiAgentEnvRunner pid=41856) {'red_0': -7.200000000000001, 'red_1': -5.9, 'blue_0': 13.5, 'blue_1': 1.799999999999999}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) PICKED UP by red_0 at STEP 90
(MultiAgentEnvRunner pid=41856) {'red_0': 7.500000000000023, 'red_1': 5, 'blue_0': -105.69999999999827, 'blue_1': -8.599999999999994}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 49: reward=-177.76000000000144, metadata={'num_env_steps_sampled_lifetime': 300000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003275451258214608, 'timers': {'connectors': {'batch_individual_items': 9.682812222837048e-05, 'add_states_from_episodes_to_batch': 6.48221394494987e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2818462307937416e-05, 'numpy_to_tensor': 6.941466836668265e-05, 'agent_to_module_mapping': 7.972347227122832e-06, 'add_observations_from_episodes_to_batch': 3.8663345472366046e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.00088395425895314, 'timers': {'connectors': {'get_actions': 0.00045677022515659655, 'un_batch_to_individual_items': 6.530717621610292e-05, 'tensor_to_numpy': 0.00012094877787639545, 'module_to_agent_unmapping': 6.386455138399934e-06, 'normalize_and_clip_actions': 7.399134513367452e-05, 'listify_data_for_vector_env': 2.3725375930170793e-05, 'remove_single_ts_time_rank_from_batch': 2.3241359359033407e-06}}}, 'sample': 127.51941059995443, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -41.140000000000306, 'blue_0': -45.80000000000037, 'blue_1': -46.680000000000405, 'red_0': -44.14000000000035}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 49.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 300000.0, 'blue_0': 300000.0, 'blue_1': 300000.0, 'red_0': 300000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -41.140000000000306, 'blue_policy': -46.680000000000405}, 'num_module_steps_sampled_lifetime': {'red_policy': 600000.0, 'blue_policy': 600000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033813340281549, 'episode_return_mean': -177.76000000000144, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -168.40000000000128, 'episode_duration_sec_mean': 25.316666059987618, 'episode_return_min': -185.80000000000155, 'rlmodule_inference_timer': 0.01337671826709399, 'num_episodes_lifetime': 250.0, 'episode_len_min': 1200, 'time_between_sampling': 260.781785500003, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.036861735020064, 'throughput_since_last_restore': 15.409518981186928}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 436
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 472
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1010
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1018
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1180
(MultiAgentEnvRunner pid=37492) {'red_0': -50.20000000000033, 'red_1': -46.90000000000035, 'blue_0': -23.600000000000072, 'blue_1': -15.200000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 393
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 544
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 598
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 598
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 599
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 775
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 775
(MultiAgentEnvRunner pid=37492) {'red_0': -37.40000000000019, 'red_1': -48.60000000000038, 'blue_0': -24.60000000000004, 'blue_1': -38.90000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 354
(MultiAgentEnvRunner pid=37492) {'red_0': -51.00000000000044, 'red_1': -38.20000000000027, 'blue_0': -50.800000000000445, 'blue_1': -46.200000000000365}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 348
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 562
(MultiAgentEnvRunner pid=37492) {'red_0': -46.300000000000416, 'red_1': -50.90000000000048, 'blue_0': -31.500000000000156, 'blue_1': -33.30000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -42.00000000000032, 'red_1': -38.70000000000027, 'blue_0': -36.300000000000246, 'blue_1': -42.90000000000033}
ITERATION 50: reward=-158.70000000000113, metadata={'num_env_steps_sampled_lifetime': 306000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032025587118686405, 'timers': {'connectors': {'batch_individual_items': 9.566119929708204e-05, 'add_states_from_episodes_to_batch': 6.287129227606579e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2133684951318812e-05, 'numpy_to_tensor': 6.582576252392215e-05, 'agent_to_module_mapping': 7.978597328453079e-06, 'add_observations_from_episodes_to_batch': 3.8025291496953524e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008574784844168928, 'timers': {'connectors': {'get_actions': 0.00044476912570228167, 'un_batch_to_individual_items': 6.382262087034679e-05, 'tensor_to_numpy': 0.0001159393416812388, 'module_to_agent_unmapping': 6.248537828524051e-06, 'normalize_and_clip_actions': 6.990516546001795e-05, 'listify_data_for_vector_env': 2.352195794729429e-05, 'remove_single_ts_time_rank_from_batch': 2.3001884812500366e-06}}}, 'sample': 88.2021883999696, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -44.66000000000035, 'blue_0': -33.3600000000002, 'blue_1': -35.30000000000025, 'red_0': -45.38000000000034}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 50.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 306000.0, 'blue_0': 306000.0, 'blue_1': 306000.0, 'red_0': 306000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -44.66000000000035, 'blue_policy': -35.30000000000025}, 'num_module_steps_sampled_lifetime': {'red_policy': 612000.0, 'blue_policy': 612000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032871828642059794, 'episode_return_mean': -158.70000000000113, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -135.9000000000008, 'episode_duration_sec_mean': 17.521994960028678, 'episode_return_min': -186.20000000000152, 'rlmodule_inference_timer': 0.012423696867615531, 'num_episodes_lifetime': 255.0, 'episode_len_min': 1200, 'time_between_sampling': 271.4965458000079, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.16760876843043, 'throughput_since_last_restore': 15.440521220280953}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 349.4538980999496, 'restore_env_runners': 1.0100076906383038e-05, 'training_step': 349.4536339000333, 'env_runner_sampling_timer': 88.32228930003475, 'learner_update_timer': 261.0780102000572, 'synch_weights': 0.01272689993493259, 'synch_env_connectors': 0.0034010999370366335, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 306000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032025587118686405, 'timers': {'connectors': {'batch_individual_items': 9.566119929708204e-05, 'add_states_from_episodes_to_batch': 6.287129227606579e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2133684951318812e-05, 'numpy_to_tensor': 6.582576252392215e-05, 'agent_to_module_mapping': 7.978597328453079e-06, 'add_observations_from_episodes_to_batch': 3.8025291496953524e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008574784844168928, 'timers': {'connectors': {'get_actions': 0.00044476912570228167, 'un_batch_to_individual_items': 6.382262087034679e-05, 'tensor_to_numpy': 0.0001159393416812388, 'module_to_agent_unmapping': 6.248537828524051e-06, 'normalize_and_clip_actions': 6.990516546001795e-05, 'listify_data_for_vector_env': 2.352195794729429e-05, 'remove_single_ts_time_rank_from_batch': 2.3001884812500366e-06}}}, 'sample': 88.2021883999696, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -44.66000000000035, 'blue_0': -33.3600000000002, 'blue_1': -35.30000000000025, 'red_0': -45.38000000000034}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 50.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 306000.0, 'blue_0': 306000.0, 'blue_1': 306000.0, 'red_0': 306000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -44.66000000000035, 'blue_policy': -35.30000000000025}, 'num_module_steps_sampled_lifetime': {'red_policy': 612000.0, 'blue_policy': 612000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032871828642059794, 'episode_return_mean': -158.70000000000113, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -135.9000000000008, 'episode_duration_sec_mean': 17.521994960028678, 'episode_return_min': -186.20000000000152, 'rlmodule_inference_timer': 0.012423696867615531, 'num_episodes_lifetime': 255.0, 'episode_len_min': 1200, 'time_between_sampling': 271.4965458000079, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.16760876843043, 'throughput_since_last_restore': 15.440521220280953}}, 'learners': {'red_policy': {'policy_loss': -0.10364224016666412, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.011733784340322018, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 51.0, 'num_module_steps_trained_lifetime': 18376320.0, 'curr_entropy_coeff': 0.045410000000000006, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002694, 'vf_explained_var': -0.5400080680847168, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.859993934631348, 'entropy': 1.6804380416870117, 'vf_loss_unclipped': 594.5516967773438, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1030.9760104337497, 'throughput_since_last_restore': 927.2547486998208}}, 'blue_policy': {'weights_seq_no': 51.0, 'num_module_steps_trained_lifetime': 18376320.0, 'curr_entropy_coeff': 0.045410000000000006, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002694, 'vf_explained_var': 0.9065563678741455, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.5137986540794373, 'total_loss': -0.25822922587394714, 'entropy': 1.7504539489746094, 'policy_loss': -0.319317102432251, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.1224338561296463, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.01204918883740902, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1030.9746832708488, 'throughput_since_last_restore': 927.2547529014319}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.7999353632330894e-06, 'batch_individual_items': 0.9254110000329092, 'add_time_dim_to_batch_and_zero_pad': 2.2600055672228336e-05, 'numpy_to_tensor': 0.12000130000524223, 'add_observations_from_episodes_to_batch': 0.00024670001585036516, 'agent_to_module_mapping': 0.02275910007301718, 'add_one_ts_to_episodes_and_truncate': 0.1776504999725148, 'add_columns_from_episodes_to_train_batch': 0.4694137000478804, 'general_advantage_estimation': 13.059378099977039}}, 'connector_pipeline_timer': 14.775305000017397}, 'num_module_steps_trained_lifetime': 36752640.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 861390000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48326.90720757805, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48326.9044696989, 'throughput_since_last_restore': 43465.066740959985}, 'num_module_steps_trained_throughput': 2061.9478963119195, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2061.9478520631124, 'throughput_since_last_restore': 1854.5095121286804}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 306000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 51, 'trial_id': 'default', 'date': '2026-01-25_20-49-09', 'timestamp': 1769370549, 'time_this_iter_s': 349.46920919418335, 'time_total_s': 19798.00700545311, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 19798.00700545311, 'iterations_since_restore': 51, 'perf': {'cpu_util_percent': 14.974749498997996, 'ram_util_percent': 90.38336673346693}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 257
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 422
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 721
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1061
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1178
(MultiAgentEnvRunner pid=37492) {'red_0': -60.8000000000005, 'red_1': -32.200000000000394, 'blue_0': -49.300000000000374, 'blue_1': -23.70000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 362
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 644
(MultiAgentEnvRunner pid=37492) {'red_0': -31.400000000000187, 'red_1': -42.30000000000032, 'blue_0': -45.000000000000355, 'blue_1': -51.00000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 328
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 591
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 652
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 772
(MultiAgentEnvRunner pid=37492) {'red_0': -53.8000000000004, 'red_1': -49.30000000000034, 'blue_0': -53.40000000000057, 'blue_1': -32.40000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1188
(MultiAgentEnvRunner pid=37492) {'red_0': -34.200000000000216, 'red_1': -36.70000000000027, 'blue_0': -46.90000000000038, 'blue_1': -41.100000000000314}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 498
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 616
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 691
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1069
(MultiAgentEnvRunner pid=37492) {'red_0': -48.70000000000034, 'red_1': -62.10000000000055, 'blue_0': -23.800000000000274, 'blue_1': -32.700000000000266}
ITERATION 51: reward=-170.16000000000133, metadata={'num_env_steps_sampled_lifetime': 312000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032285063165741877, 'timers': {'connectors': {'batch_individual_items': 9.901091255867453e-05, 'add_states_from_episodes_to_batch': 6.247420742372744e-06, 'add_time_dim_to_batch_and_zero_pad': 1.220398553722864e-05, 'numpy_to_tensor': 6.560411750869198e-05, 'agent_to_module_mapping': 7.910373012394155e-06, 'add_observations_from_episodes_to_batch': 3.680541374628577e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008468295762270057, 'timers': {'connectors': {'get_actions': 0.0004359938895562727, 'un_batch_to_individual_items': 6.300286384572328e-05, 'tensor_to_numpy': 0.00011655482422875314, 'module_to_agent_unmapping': 6.2088598115239635e-06, 'normalize_and_clip_actions': 6.915690398125736e-05, 'listify_data_for_vector_env': 2.3169659687875684e-05, 'remove_single_ts_time_rank_from_batch': 2.2686590401594853e-06}}}, 'sample': 88.98197329998948, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -44.520000000000366, 'blue_0': -43.68000000000039, 'blue_1': -36.180000000000256, 'red_0': -45.78000000000033}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 51.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 312000.0, 'blue_0': 312000.0, 'blue_1': 312000.0, 'red_0': 312000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -44.520000000000366, 'blue_policy': -36.180000000000256}, 'num_module_steps_sampled_lifetime': {'red_policy': 624000.0, 'blue_policy': 624000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032741305581108007, 'episode_return_mean': -170.16000000000133, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -158.90000000000117, 'episode_duration_sec_mean': 17.661673240037636, 'episode_return_min': -188.9000000000015, 'rlmodule_inference_timer': 0.012405349923019619, 'num_episodes_lifetime': 260.0, 'episode_len_min': 1200, 'time_between_sampling': 261.3637183000101, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.15036781323641, 'throughput_since_last_restore': 15.470179456161157}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 695
(MultiAgentEnvRunner pid=37492) {'red_0': -37.700000000000195, 'red_1': -37.50000000000019, 'blue_0': -48.700000000000486, 'blue_1': -30.100000000000257}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 221
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 758
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 987
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 997
(MultiAgentEnvRunner pid=37492) {'red_0': -38.70000000000034, 'red_1': -49.9000000000004, 'blue_0': -42.00000000000034, 'blue_1': -45.80000000000054}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 264
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 649
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 990
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1072
(MultiAgentEnvRunner pid=37492) {'red_0': -41.900000000000304, 'red_1': -39.000000000000334, 'blue_0': -49.700000000000394, 'blue_1': -34.70000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 770
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1117
(MultiAgentEnvRunner pid=37492) {'red_0': -33.300000000000196, 'red_1': -30.30000000000022, 'blue_0': -46.10000000000037, 'blue_1': -43.10000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 404
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 804
(MultiAgentEnvRunner pid=37492) {'red_0': -44.70000000000036, 'red_1': -34.40000000000024, 'blue_0': -43.500000000000334, 'blue_1': -61.10000000000057}
ITERATION 52: reward=-166.44000000000133, metadata={'num_env_steps_sampled_lifetime': 318000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003195594023086022, 'timers': {'connectors': {'batch_individual_items': 9.696858296640449e-05, 'add_states_from_episodes_to_batch': 6.230557303314877e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2152721851447573e-05, 'numpy_to_tensor': 6.612065578449581e-05, 'agent_to_module_mapping': 7.771142864398635e-06, 'add_observations_from_episodes_to_batch': 3.8308586839416735e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008514525677097659, 'timers': {'connectors': {'get_actions': 0.0004404470789274801, 'un_batch_to_individual_items': 6.308750669665276e-05, 'tensor_to_numpy': 0.00011521345094139352, 'module_to_agent_unmapping': 6.224092165582839e-06, 'normalize_and_clip_actions': 6.98836026226883e-05, 'listify_data_for_vector_env': 2.3158615324958735e-05, 'remove_single_ts_time_rank_from_batch': 2.2551270498545084e-06}}}, 'sample': 89.53344919998199, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -38.22000000000027, 'blue_0': -46.000000000000384, 'blue_1': -42.96000000000039, 'red_0': -39.26000000000028}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 52.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 318000.0, 'blue_0': 318000.0, 'blue_1': 318000.0, 'red_0': 318000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -38.22000000000027, 'blue_policy': -42.96000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 636000.0, 'blue_policy': 636000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003252187278349392, 'episode_return_mean': -166.44000000000133, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -152.80000000000112, 'episode_duration_sec_mean': 17.789307660004123, 'episode_return_min': -183.7000000000015, 'rlmodule_inference_timer': 0.012426920214640723, 'num_episodes_lifetime': 265.0, 'episode_len_min': 1200, 'time_between_sampling': 260.7931064000586, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.123641945896797, 'throughput_since_last_restore': 15.498414195143884}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 462
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 478
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 822
(MultiAgentEnvRunner pid=37492) {'red_0': -38.80000000000024, 'red_1': -47.90000000000043, 'blue_0': -23.900000000000052, 'blue_1': -52.400000000000496}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1162
(MultiAgentEnvRunner pid=37492) {'red_0': -39.80000000000029, 'red_1': -38.0000000000003, 'blue_0': -52.40000000000047, 'blue_1': -55.70000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 844
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 876
(MultiAgentEnvRunner pid=37492) {'red_0': -42.50000000000032, 'red_1': -52.70000000000045, 'blue_0': -51.50000000000044, 'blue_1': -41.50000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 205
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 237
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 685
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 759
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 792
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1151
(MultiAgentEnvRunner pid=37492) {'red_0': -44.20000000000024, 'red_1': -44.700000000000266, 'blue_0': -43.300000000000544, 'blue_1': -53.60000000000056}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 607
(MultiAgentEnvRunner pid=37492) {'red_0': -36.700000000000216, 'red_1': -27.00000000000011, 'blue_0': -47.50000000000033, 'blue_1': -61.60000000000051}
ITERATION 53: reward=-179.14000000000144, metadata={'num_env_steps_sampled_lifetime': 324000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003169151301828933, 'timers': {'connectors': {'batch_individual_items': 9.475843498195668e-05, 'add_states_from_episodes_to_batch': 6.395338945236822e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2015914447881821e-05, 'numpy_to_tensor': 6.562263074039245e-05, 'agent_to_module_mapping': 8.102894376384347e-06, 'add_observations_from_episodes_to_batch': 3.7106048116950976e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008473344264883838, 'timers': {'connectors': {'get_actions': 0.00043563611624188237, 'un_batch_to_individual_items': 6.548650885373247e-05, 'tensor_to_numpy': 0.00011450423362521473, 'module_to_agent_unmapping': 6.2563342008997625e-06, 'normalize_and_clip_actions': 7.024982025597809e-05, 'listify_data_for_vector_env': 2.304232316197645e-05, 'remove_single_ts_time_rank_from_batch': 2.3805040974891707e-06}}}, 'sample': 88.4144290999975, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -42.060000000000315, 'blue_0': -43.72000000000037, 'blue_1': -52.96000000000049, 'red_0': -40.40000000000026}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 53.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 324000.0, 'blue_0': 324000.0, 'blue_1': 324000.0, 'red_0': 324000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -42.060000000000315, 'blue_policy': -52.96000000000049}, 'num_module_steps_sampled_lifetime': {'red_policy': 648000.0, 'blue_policy': 648000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003320568627045449, 'episode_return_mean': -179.14000000000144, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -163.00000000000122, 'episode_duration_sec_mean': 17.565053039975464, 'episode_return_min': -188.20000000000158, 'rlmodule_inference_timer': 0.012371376081441784, 'num_episodes_lifetime': 270.0, 'episode_len_min': 1200, 'time_between_sampling': 260.85915179993026, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.21220145352806, 'throughput_since_last_restore': 15.52704236002995}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 986
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1026
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1026
(MultiAgentEnvRunner pid=37492) {'red_0': -39.100000000000236, 'red_1': -33.500000000000185, 'blue_0': -31.500000000000348, 'blue_1': -52.00000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 203
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 657
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 730
(MultiAgentEnvRunner pid=37492) {'red_0': -33.30000000000016, 'red_1': -33.00000000000021, 'blue_0': -45.10000000000041, 'blue_1': -48.800000000000466}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 172
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 847
(MultiAgentEnvRunner pid=37492) {'red_0': -64.90000000000055, 'red_1': -35.20000000000022, 'blue_0': -55.20000000000049, 'blue_1': -41.10000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 359
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 493
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 618
(MultiAgentEnvRunner pid=37492) {'red_0': -18.89999999999997, 'red_1': -20.099999999999973, 'blue_0': -47.90000000000037, 'blue_1': -62.900000000000446}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1182
(MultiAgentEnvRunner pid=37492) {'red_0': -39.200000000000216, 'red_1': -66.00000000000054, 'blue_0': -33.10000000000041, 'blue_1': -33.10000000000027}
ITERATION 54: reward=-166.78000000000128, metadata={'num_env_steps_sampled_lifetime': 330000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035590583219887806, 'timers': {'connectors': {'batch_individual_items': 9.855017810053582e-05, 'add_states_from_episodes_to_batch': 7.261812389047706e-06, 'add_time_dim_to_batch_and_zero_pad': 1.433834911751833e-05, 'numpy_to_tensor': 7.524001978861026e-05, 'agent_to_module_mapping': 8.911951362480956e-06, 'add_observations_from_episodes_to_batch': 4.294772856018889e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009507749774037528, 'timers': {'connectors': {'get_actions': 0.000487409595793838, 'un_batch_to_individual_items': 7.23639431811709e-05, 'tensor_to_numpy': 0.00012711535297046315, 'module_to_agent_unmapping': 7.131325945650796e-06, 'normalize_and_clip_actions': 7.848758647645744e-05, 'listify_data_for_vector_env': 2.6470605848762762e-05, 'remove_single_ts_time_rank_from_batch': 2.641107652205151e-06}}}, 'sample': 92.82499839994125, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -37.56000000000022, 'blue_0': -42.560000000000414, 'blue_1': -47.580000000000396, 'red_0': -39.080000000000226}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 54.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 330000.0, 'blue_0': 330000.0, 'blue_1': 330000.0, 'red_0': 330000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -37.56000000000022, 'blue_policy': -47.580000000000396}, 'num_module_steps_sampled_lifetime': {'red_policy': 660000.0, 'blue_policy': 660000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003640658088693514, 'episode_return_mean': -166.78000000000128, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -149.80000000000075, 'episode_duration_sec_mean': 18.428363260021435, 'episode_return_min': -196.4000000000016, 'rlmodule_inference_timer': 0.014217260464244378, 'num_episodes_lifetime': 275.0, 'episode_len_min': 1200, 'time_between_sampling': 260.18371519993525, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.933951575047445, 'throughput_since_last_restore': 15.550530747467908}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 227
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 731
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 906
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1066
(MultiAgentEnvRunner pid=37492) {'red_0': -50.30000000000039, 'red_1': -68.90000000000008, 'blue_0': -29.40000000000009, 'blue_1': -26.100000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -41.50000000000031, 'red_1': -39.800000000000296, 'blue_0': -42.70000000000034, 'blue_1': -44.60000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 290
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 305
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 605
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 733
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1063
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1078
(MultiAgentEnvRunner pid=37492) {'red_0': -41.3000000000003, 'red_1': -61.100000000000534, 'blue_0': -46.40000000000043, 'blue_1': -38.70000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 857
(MultiAgentEnvRunner pid=37492) {'red_0': -35.00000000000025, 'red_1': -45.40000000000037, 'blue_0': -48.80000000000041, 'blue_1': -56.900000000000524}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 556
(MultiAgentEnvRunner pid=37492) {'red_0': -31.600000000000165, 'red_1': -40.200000000000294, 'blue_0': -42.40000000000033, 'blue_1': -58.50000000000052}
ITERATION 55: reward=-177.92000000000124, metadata={'num_env_steps_sampled_lifetime': 336000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032117565447995546, 'timers': {'connectors': {'batch_individual_items': 9.579629062630296e-05, 'add_states_from_episodes_to_batch': 6.345408521822107e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2268579731006363e-05, 'numpy_to_tensor': 6.599043649572538e-05, 'agent_to_module_mapping': 7.980997304922493e-06, 'add_observations_from_episodes_to_batch': 3.761063741252023e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008511547347468573, 'timers': {'connectors': {'get_actions': 0.00043631027105606125, 'un_batch_to_individual_items': 6.397898067926877e-05, 'tensor_to_numpy': 0.00011767379736612503, 'module_to_agent_unmapping': 6.2585649122762796e-06, 'normalize_and_clip_actions': 7.009229751326032e-05, 'listify_data_for_vector_env': 2.3405620484243203e-05, 'remove_single_ts_time_rank_from_batch': 2.2868235865736824e-06}}}, 'sample': 88.43159639998339, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -51.08000000000032, 'blue_0': -41.94000000000032, 'blue_1': -44.960000000000335, 'red_0': -39.94000000000028}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 55.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 336000.0, 'blue_0': 336000.0, 'blue_1': 336000.0, 'red_0': 336000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -51.08000000000032, 'blue_policy': -44.960000000000335}, 'num_module_steps_sampled_lifetime': {'red_policy': 672000.0, 'blue_policy': 672000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000327693541711097, 'episode_return_mean': -177.92000000000124, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -168.60000000000127, 'episode_duration_sec_mean': 17.567162699997425, 'episode_return_min': -187.50000000000153, 'rlmodule_inference_timer': 0.012556718651389687, 'num_episodes_lifetime': 280.0, 'episode_len_min': 1200, 'time_between_sampling': 261.49147740006447, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.180011755437135, 'throughput_since_last_restore': 15.576911829076833}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 880
(MultiAgentEnvRunner pid=37492) {'red_0': -33.40000000000022, 'red_1': -31.100000000000296, 'blue_0': -59.100000000000485, 'blue_1': -45.70000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 615
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1086
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1122
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1122
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1122
(MultiAgentEnvRunner pid=37492) {'red_0': -43.40000000000043, 'red_1': -39.400000000000354, 'blue_0': -48.00000000000033, 'blue_1': -55.50000000000056}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 496
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 915
(MultiAgentEnvRunner pid=37492) {'red_0': -42.50000000000032, 'red_1': -34.20000000000021, 'blue_0': -42.60000000000033, 'blue_1': -47.00000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 384
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 936
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1040
(MultiAgentEnvRunner pid=37492) {'red_0': -21.000000000000078, 'red_1': -35.200000000000266, 'blue_0': -66.9000000000004, 'blue_1': -69.20000000000012}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 386
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 506
(MultiAgentEnvRunner pid=37492) {'red_0': -23.700000000000053, 'red_1': -33.80000000000018, 'blue_0': -42.60000000000035, 'blue_1': -48.400000000000375}
ITERATION 56: reward=-172.54000000000119, metadata={'num_env_steps_sampled_lifetime': 342000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003223919552113536, 'timers': {'connectors': {'batch_individual_items': 9.688940610639885e-05, 'add_states_from_episodes_to_batch': 6.357341901676268e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2214797351994743e-05, 'numpy_to_tensor': 6.592419697141174e-05, 'agent_to_module_mapping': 7.93802753468025e-06, 'add_observations_from_episodes_to_batch': 3.787932344484162e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000858156157790668, 'timers': {'connectors': {'get_actions': 0.0004422468235853272, 'un_batch_to_individual_items': 6.473046203632642e-05, 'tensor_to_numpy': 0.00011655565242467151, 'module_to_agent_unmapping': 6.294651937747496e-06, 'normalize_and_clip_actions': 7.09607118072558e-05, 'listify_data_for_vector_env': 2.3693370627767976e-05, 'remove_single_ts_time_rank_from_batch': 2.2982797158280898e-06}}}, 'sample': 88.9930531999562, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -34.740000000000265, 'blue_0': -51.84000000000039, 'blue_1': -53.16000000000035, 'red_0': -32.800000000000225}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 56.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 342000.0, 'blue_0': 342000.0, 'blue_1': 342000.0, 'red_0': 342000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -34.740000000000265, 'blue_policy': -53.16000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 684000.0, 'blue_policy': 684000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003398291373323188, 'episode_return_mean': -172.54000000000119, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -148.50000000000097, 'episode_duration_sec_mean': 17.680723559996114, 'episode_return_min': -192.30000000000086, 'rlmodule_inference_timer': 0.012603186037153232, 'num_episodes_lifetime': 285.0, 'episode_len_min': 1200, 'time_between_sampling': 260.8107213000767, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.129545865748874, 'throughput_since_last_restore': 15.601719821988908}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 880
(MultiAgentEnvRunner pid=37492) {'red_0': -48.50000000000034, 'red_1': -50.80000000000038, 'blue_0': -35.4000000000003, 'blue_1': -27.30000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 726
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1065
(MultiAgentEnvRunner pid=37492) {'red_0': -45.40000000000034, 'red_1': -53.500000000000426, 'blue_0': -28.300000000000164, 'blue_1': -35.90000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 525
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 861
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 927
(MultiAgentEnvRunner pid=37492) {'red_0': -41.50000000000025, 'red_1': -45.200000000000365, 'blue_0': -52.000000000000426, 'blue_1': -42.800000000000416}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 867
(MultiAgentEnvRunner pid=37492) {'red_0': -54.10000000000042, 'red_1': -36.10000000000017, 'blue_0': -56.10000000000058, 'blue_1': -35.50000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 467
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 695
(MultiAgentEnvRunner pid=37492) {'red_0': -30.300000000000164, 'red_1': -33.30000000000023, 'blue_0': -44.00000000000038, 'blue_1': -41.90000000000031}
ITERATION 57: reward=-167.58000000000135, metadata={'num_env_steps_sampled_lifetime': 348000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032123807990945135, 'timers': {'connectors': {'batch_individual_items': 9.770608560801597e-05, 'add_states_from_episodes_to_batch': 6.275775896031811e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2036698340597666e-05, 'numpy_to_tensor': 6.555379887298747e-05, 'agent_to_module_mapping': 7.860453188228582e-06, 'add_observations_from_episodes_to_batch': 3.8100615189016324e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008480342843061597, 'timers': {'connectors': {'get_actions': 0.00043640157340722063, 'un_batch_to_individual_items': 6.495681421088988e-05, 'tensor_to_numpy': 0.00011545596153460445, 'module_to_agent_unmapping': 6.1627483205304555e-06, 'normalize_and_clip_actions': 6.968700752958096e-05, 'listify_data_for_vector_env': 2.3556371879247824e-05, 'remove_single_ts_time_rank_from_batch': 2.3006366142665503e-06}}}, 'sample': 87.62270920001902, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -43.780000000000314, 'blue_0': -43.160000000000366, 'blue_1': -36.68000000000035, 'red_0': -43.960000000000306}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 57.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 348000.0, 'blue_0': 348000.0, 'blue_1': 348000.0, 'red_0': 348000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -43.780000000000314, 'blue_policy': -36.68000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 696000.0, 'blue_policy': 696000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003278011424960628, 'episode_return_mean': -167.58000000000135, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -149.50000000000108, 'episode_duration_sec_mean': 17.404418700025417, 'episode_return_min': -181.8000000000016, 'rlmodule_inference_timer': 0.012474412104796087, 'num_episodes_lifetime': 290.0, 'episode_len_min': 1200, 'time_between_sampling': 261.2812916999683, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.254392741929983, 'throughput_since_last_restore': 15.627525957740295}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 409
(MultiAgentEnvRunner pid=37492) {'red_0': -45.100000000000364, 'red_1': -50.90000000000043, 'blue_0': -36.30000000000023, 'blue_1': -46.00000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 813
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1197
(MultiAgentEnvRunner pid=37492) {'red_0': -24.100000000000104, 'red_1': -19.50000000000003, 'blue_0': -55.50000000000044, 'blue_1': -44.20000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -37.50000000000025, 'red_1': -45.30000000000036, 'blue_0': -47.6000000000004, 'blue_1': -40.60000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 512
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 582
(MultiAgentEnvRunner pid=37492) {'red_0': -35.40000000000026, 'red_1': -40.600000000000335, 'blue_0': -51.50000000000044, 'blue_1': -49.60000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 180
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 195
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 244
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 244
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 603
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 635
(MultiAgentEnvRunner pid=37492) {'red_0': -35.100000000000215, 'red_1': -37.30000000000025, 'blue_0': -42.80000000000038, 'blue_1': -47.600000000000456}
ITERATION 58: reward=-166.5000000000013, metadata={'num_env_steps_sampled_lifetime': 354000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003236810879586017, 'timers': {'connectors': {'batch_individual_items': 9.687652407191394e-05, 'add_states_from_episodes_to_batch': 6.294492405540929e-06, 'add_time_dim_to_batch_and_zero_pad': 1.215170185815919e-05, 'numpy_to_tensor': 6.65974334411693e-05, 'agent_to_module_mapping': 8.171223696188357e-06, 'add_observations_from_episodes_to_batch': 3.828483520743141e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008605237580021008, 'timers': {'connectors': {'get_actions': 0.00044259623718718126, 'un_batch_to_individual_items': 6.538794259057e-05, 'tensor_to_numpy': 0.00011725795218538279, 'module_to_agent_unmapping': 6.273781020419925e-06, 'normalize_and_clip_actions': 7.005139019348542e-05, 'listify_data_for_vector_env': 2.3307512663828106e-05, 'remove_single_ts_time_rank_from_batch': 2.2969476504933787e-06}}}, 'sample': 88.56831240002066, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -38.720000000000276, 'blue_0': -46.74000000000038, 'blue_1': -45.60000000000039, 'red_0': -35.44000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 58.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 354000.0, 'blue_0': 354000.0, 'blue_1': 354000.0, 'red_0': 354000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -38.720000000000276, 'blue_policy': -45.60000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 708000.0, 'blue_policy': 708000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032704099734862796, 'episode_return_mean': -166.5000000000013, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -143.300000000001, 'episode_duration_sec_mean': 17.59717419997323, 'episode_return_min': -178.3000000000014, 'rlmodule_inference_timer': 0.0124331062772374, 'num_episodes_lifetime': 295.0, 'episode_len_min': 1200, 'time_between_sampling': 260.11992880003527, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.209617221023166, 'throughput_since_last_restore': 15.651911859946074}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 609
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 760
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 878
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1160
(MultiAgentEnvRunner pid=37492) {'red_0': -57.000000000000426, 'red_1': -43.600000000000264, 'blue_0': -27.50000000000016, 'blue_1': -23.500000000000085}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 980
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1148
(MultiAgentEnvRunner pid=37492) {'red_0': -44.100000000000335, 'red_1': -33.90000000000024, 'blue_0': -40.1000000000003, 'blue_1': -54.80000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 920
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1060
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1069
(MultiAgentEnvRunner pid=37492) {'red_0': -29.700000000000223, 'red_1': -26.900000000000254, 'blue_0': -49.70000000000038, 'blue_1': -55.100000000000456}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 750
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 978
(MultiAgentEnvRunner pid=37492) {'red_0': -28.400000000000162, 'red_1': -30.900000000000198, 'blue_0': -54.400000000000475, 'blue_1': -46.20000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 327
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 386
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1089
(MultiAgentEnvRunner pid=37492) {'red_0': -37.200000000000244, 'red_1': -31.400000000000198, 'blue_0': -63.4000000000006, 'blue_1': -50.10000000000046}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.2, 'blue_0': -115.7999999999977, 'blue_1': -1.0999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.8999999999999999, 'red_1': -115.29999999999772, 'blue_0': -1.0999999999999999, 'blue_1': -1.0999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-25 21:39:16,531	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.8999999999999999, 'blue_0': -114.99999999999774, 'blue_1': -0.9999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -118.59999999999754, 'blue_1': -118.79999999999752}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': 0, 'blue_0': -7.29999999999999, 'blue_1': -0.8999999999999999}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 59: reward=-165.58000000000126, metadata={'num_env_steps_sampled_lifetime': 360000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003182910309813561, 'timers': {'connectors': {'batch_individual_items': 9.553516945022859e-05, 'add_states_from_episodes_to_batch': 6.292323919121986e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2090101512716206e-05, 'numpy_to_tensor': 6.55378853586053e-05, 'agent_to_module_mapping': 7.756477437930716e-06, 'add_observations_from_episodes_to_batch': 3.7674565823801515e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008474529064196047, 'timers': {'connectors': {'get_actions': 0.00043550221912586474, 'un_batch_to_individual_items': 6.407270216981572e-05, 'tensor_to_numpy': 0.00011496190021364806, 'module_to_agent_unmapping': 6.398847379173739e-06, 'normalize_and_clip_actions': 6.98207476254783e-05, 'listify_data_for_vector_env': 2.3111952904343868e-05, 'remove_single_ts_time_rank_from_batch': 2.3455240411497335e-06}}}, 'sample': 87.93984790006652, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.34000000000024, 'blue_0': -47.02000000000038, 'blue_1': -45.94000000000038, 'red_0': -39.28000000000027}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 59.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 360000.0, 'blue_0': 360000.0, 'blue_1': 360000.0, 'red_0': 360000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.34000000000024, 'blue_policy': -45.94000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 720000.0, 'blue_policy': 720000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003284109631257456, 'episode_return_mean': -165.58000000000126, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -151.60000000000093, 'episode_duration_sec_mean': 17.455037399986757, 'episode_return_min': -182.1000000000015, 'rlmodule_inference_timer': 0.012416119877721668, 'num_episodes_lifetime': 300.0, 'episode_len_min': 1200, 'time_between_sampling': 260.0792214999674, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.746430310368702, 'throughput_since_last_restore': 15.615833082170845}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 296
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 555
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 627
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 822
(MultiAgentEnvRunner pid=37492) {'red_0': -26.500000000000256, 'red_1': -32.30000000000026, 'blue_0': -54.90000000000043, 'blue_1': -58.600000000000456}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -47.200000000000394, 'red_1': -31.700000000000173, 'blue_0': -55.20000000000049, 'blue_1': -59.40000000000056}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 957
(MultiAgentEnvRunner pid=37492) {'red_0': -35.300000000000296, 'red_1': -20.300000000000168, 'blue_0': -46.900000000000325, 'blue_1': -40.500000000000234}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -23.400000000000063, 'red_1': -27.900000000000126, 'blue_0': -45.000000000000355, 'blue_1': -42.70000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 328
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 820
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 879
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 879
(MultiAgentEnvRunner pid=37492) {'red_0': -28.600000000000183, 'red_1': -33.600000000000264, 'blue_0': -42.300000000000246, 'blue_1': -37.40000000000025}
ITERATION 60: reward=-157.9400000000012, metadata={'num_env_steps_sampled_lifetime': 366000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031832861724177743, 'timers': {'connectors': {'batch_individual_items': 9.470645288123723e-05, 'add_states_from_episodes_to_batch': 6.140170488444235e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2043399059107473e-05, 'numpy_to_tensor': 6.607144203118924e-05, 'agent_to_module_mapping': 7.88726132600353e-06, 'add_observations_from_episodes_to_batch': 3.779517563674936e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008660300310586695, 'timers': {'connectors': {'get_actions': 0.0004485687932066845, 'un_batch_to_individual_items': 6.374438505660468e-05, 'tensor_to_numpy': 0.00011790237999476822, 'module_to_agent_unmapping': 6.466232891246308e-06, 'normalize_and_clip_actions': 7.07524241573118e-05, 'listify_data_for_vector_env': 2.3924151833911848e-05, 'remove_single_ts_time_rank_from_batch': 2.28131554233765e-06}}}, 'sample': 88.59868609998375, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.160000000000196, 'blue_0': -48.86000000000037, 'blue_1': -47.72000000000037, 'red_0': -32.20000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 60.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 366000.0, 'blue_0': 366000.0, 'blue_1': 366000.0, 'red_0': 366000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.160000000000196, 'blue_policy': -47.72000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 732000.0, 'blue_policy': 732000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032627795680409335, 'episode_return_mean': -157.9400000000012, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -139.00000000000088, 'episode_duration_sec_mean': 17.602399699995296, 'episode_return_min': -193.50000000000165, 'rlmodule_inference_timer': 0.012496392022928855, 'num_episodes_lifetime': 305.0, 'episode_len_min': 1200, 'time_between_sampling': 348.5397028999869, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.218184417042195, 'throughput_since_last_restore': 15.63969092533997}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 348.4315044999821, 'restore_env_runners': 8.79995059221983e-06, 'training_step': 348.4312581999693, 'env_runner_sampling_timer': 88.72215680009685, 'learner_update_timer': 259.6591459000483, 'synch_weights': 0.012871699989773333, 'synch_env_connectors': 0.0021518999710679054, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 366000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031832861724177743, 'timers': {'connectors': {'batch_individual_items': 9.470645288123723e-05, 'add_states_from_episodes_to_batch': 6.140170488444235e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2043399059107473e-05, 'numpy_to_tensor': 6.607144203118924e-05, 'agent_to_module_mapping': 7.88726132600353e-06, 'add_observations_from_episodes_to_batch': 3.779517563674936e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008660300310586695, 'timers': {'connectors': {'get_actions': 0.0004485687932066845, 'un_batch_to_individual_items': 6.374438505660468e-05, 'tensor_to_numpy': 0.00011790237999476822, 'module_to_agent_unmapping': 6.466232891246308e-06, 'normalize_and_clip_actions': 7.07524241573118e-05, 'listify_data_for_vector_env': 2.3924151833911848e-05, 'remove_single_ts_time_rank_from_batch': 2.28131554233765e-06}}}, 'sample': 88.59868609998375, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.160000000000196, 'blue_0': -48.86000000000037, 'blue_1': -47.72000000000037, 'red_0': -32.20000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 60.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 366000.0, 'blue_0': 366000.0, 'blue_1': 366000.0, 'red_0': 366000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.160000000000196, 'blue_policy': -47.72000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 732000.0, 'blue_policy': 732000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032627795680409335, 'episode_return_mean': -157.9400000000012, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -139.00000000000088, 'episode_duration_sec_mean': 17.602399699995296, 'episode_return_min': -193.50000000000165, 'rlmodule_inference_timer': 0.012496392022928855, 'num_episodes_lifetime': 305.0, 'episode_len_min': 1200, 'time_between_sampling': 348.5397028999869, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.218184417042195, 'throughput_since_last_restore': 15.63969092533997}}, 'learners': {'red_policy': {'policy_loss': -0.17195208370685577, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.010969201102852821, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 61.0, 'num_module_steps_trained_lifetime': 21979520.0, 'curr_entropy_coeff': 0.04451, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002634, 'vf_explained_var': -0.6351296901702881, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.791311264038086, 'entropy': 1.6641420125961304, 'vf_loss_unclipped': 680.73193359375, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1034.0152591940732, 'throughput_since_last_restore': 939.2155561355524}}, 'blue_policy': {'weights_seq_no': 61.0, 'num_module_steps_trained_lifetime': 21979520.0, 'curr_entropy_coeff': 0.04451, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002634, 'vf_explained_var': 0.7060215473175049, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 1.6985894441604614, 'total_loss': -0.17868058383464813, 'entropy': 1.745396375656128, 'policy_loss': -0.34158027172088623, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.2176196575164795, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.01522612664848566, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1034.0132802867302, 'throughput_since_last_restore': 939.2155597235305}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.700051017105579e-06, 'batch_individual_items': 0.9220381999621168, 'add_time_dim_to_batch_and_zero_pad': 2.119992859661579e-05, 'numpy_to_tensor': 0.13137680001091212, 'add_observations_from_episodes_to_batch': 0.00023809995036572218, 'agent_to_module_mapping': 0.020106000010855496, 'add_one_ts_to_episodes_and_truncate': 0.17570750007871538, 'add_columns_from_episodes_to_train_batch': 0.4762906999094412, 'general_advantage_estimation': 13.00564910005778}}, 'connector_pipeline_timer': 14.731872500036843}, 'num_module_steps_trained_lifetime': 43959040.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 1030290000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48469.32159156268, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48469.320186732286, 'throughput_since_last_restore': 44025.72953342517}, 'num_module_steps_trained_throughput': 2068.0243089763153, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2068.024270994459, 'throughput_since_last_restore': 1878.431124897252}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 366000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 61, 'trial_id': 'default', 'date': '2026-01-25_21-48-53', 'timestamp': 1769374133, 'time_this_iter_s': 348.44646739959717, 'time_total_s': 23381.716447114944, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 23381.716447114944, 'iterations_since_restore': 61, 'perf': {'cpu_util_percent': 14.640241448692153, 'ram_util_percent': 91.96257545271628}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -26.700000000000113, 'red_1': -36.000000000000234, 'blue_0': -46.30000000000038, 'blue_1': -52.40000000000046}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 564
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 758
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 764
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 764
(MultiAgentEnvRunner pid=37492) {'red_0': -41.6000000000003, 'red_1': -42.300000000000274, 'blue_0': -37.7000000000004, 'blue_1': -46.300000000000445}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 388
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1007
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1060
(MultiAgentEnvRunner pid=37492) {'red_0': -65.80000000000042, 'red_1': -40.700000000000216, 'blue_0': -27.800000000000352, 'blue_1': -39.70000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -27.100000000000115, 'red_1': -44.70000000000035, 'blue_0': -55.70000000000051, 'blue_1': -45.60000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 826
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 886
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1103
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1192
(MultiAgentEnvRunner pid=37492) {'red_0': -24.8000000000001, 'red_1': -32.00000000000023, 'blue_0': -64.90000000000055, 'blue_1': -43.300000000000345}
ITERATION 61: reward=-168.2800000000013, metadata={'num_env_steps_sampled_lifetime': 372000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003187841629023824, 'timers': {'connectors': {'batch_individual_items': 9.616047383061839e-05, 'add_states_from_episodes_to_batch': 6.269888949232097e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2189073280120109e-05, 'numpy_to_tensor': 6.576324441367849e-05, 'agent_to_module_mapping': 7.980498704688829e-06, 'add_observations_from_episodes_to_batch': 3.717296138061063e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000848848978053138, 'timers': {'connectors': {'get_actions': 0.000438460688138974, 'un_batch_to_individual_items': 6.364585839453643e-05, 'tensor_to_numpy': 0.00011570559479534056, 'module_to_agent_unmapping': 6.3144521988735755e-06, 'normalize_and_clip_actions': 6.943470764975727e-05, 'listify_data_for_vector_env': 2.3170553478717383e-05, 'remove_single_ts_time_rank_from_batch': 2.2983927526293906e-06}}}, 'sample': 88.18596459995024, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -39.140000000000256, 'blue_0': -46.48000000000044, 'blue_1': -45.46000000000039, 'red_0': -37.2000000000002}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 61.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 372000.0, 'blue_0': 372000.0, 'blue_1': 372000.0, 'red_0': 372000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -39.140000000000256, 'blue_policy': -45.46000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 744000.0, 'blue_policy': 744000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003232069644951966, 'episode_return_mean': -168.2800000000013, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -161.40000000000117, 'episode_duration_sec_mean': 17.5131768199848, 'episode_return_min': -174.00000000000134, 'rlmodule_inference_timer': 0.012451390350301348, 'num_episodes_lifetime': 310.0, 'episode_len_min': 1200, 'time_between_sampling': 259.96495329996105, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.23563427227283, 'throughput_since_last_restore': 15.663081630499928}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 631
(MultiAgentEnvRunner pid=37492) {'red_0': -32.000000000000135, 'red_1': -33.80000000000014, 'blue_0': -32.70000000000025, 'blue_1': -38.80000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 471
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 794
(MultiAgentEnvRunner pid=37492) {'red_0': -25.500000000000092, 'red_1': -55.50000000000052, 'blue_0': -45.4000000000004, 'blue_1': -46.50000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 551
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1020
(MultiAgentEnvRunner pid=37492) {'red_0': -44.60000000000034, 'red_1': -36.70000000000027, 'blue_0': -44.30000000000038, 'blue_1': -56.500000000000504}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 734
(MultiAgentEnvRunner pid=37492) {'red_0': -35.50000000000016, 'red_1': -25.900000000000077, 'blue_0': -29.50000000000018, 'blue_1': -31.50000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 634
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 967
(MultiAgentEnvRunner pid=37492) {'red_0': -22.400000000000187, 'red_1': -17.09999999999999, 'blue_0': -56.100000000000435, 'blue_1': -50.400000000000375}
ITERATION 62: reward=-152.14000000000107, metadata={'num_env_steps_sampled_lifetime': 378000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032206721133412554, 'timers': {'connectors': {'batch_individual_items': 9.561662292115517e-05, 'add_states_from_episodes_to_batch': 6.299329885471726e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2427783819084206e-05, 'numpy_to_tensor': 6.717158942483239e-05, 'agent_to_module_mapping': 7.96316494635282e-06, 'add_observations_from_episodes_to_batch': 3.8392139246338365e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008650720060028725, 'timers': {'connectors': {'get_actions': 0.000448905579453605, 'un_batch_to_individual_items': 6.432993604796753e-05, 'tensor_to_numpy': 0.00011600434267339707, 'module_to_agent_unmapping': 6.304749075174108e-06, 'normalize_and_clip_actions': 7.084683096442416e-05, 'listify_data_for_vector_env': 2.359942066983516e-05, 'remove_single_ts_time_rank_from_batch': 2.2528587274303033e-06}}}, 'sample': 88.478228699998, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.800000000000196, 'blue_0': -41.60000000000033, 'blue_1': -44.74000000000037, 'red_0': -32.000000000000185}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 62.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 378000.0, 'blue_0': 378000.0, 'blue_1': 378000.0, 'red_0': 378000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.800000000000196, 'blue_policy': -44.74000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 756000.0, 'blue_policy': 756000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003307730309575906, 'episode_return_mean': -152.14000000000107, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -122.40000000000066, 'episode_duration_sec_mean': 17.558043859992175, 'episode_return_min': -182.1000000000015, 'rlmodule_inference_timer': 0.012721084950465127, 'num_episodes_lifetime': 315.0, 'episode_len_min': 1200, 'time_between_sampling': 259.83958439994603, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.9620344925062, 'throughput_since_last_restore': 15.667737985424285}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -39.50000000000029, 'red_1': -29.100000000000144, 'blue_0': -61.80000000000058, 'blue_1': -43.40000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 450
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 457
(MultiAgentEnvRunner pid=37492) {'red_0': -22.600000000000044, 'red_1': -45.10000000000035, 'blue_0': -52.20000000000045, 'blue_1': -41.20000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 272
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 900
(MultiAgentEnvRunner pid=37492) {'red_0': -24.400000000000077, 'red_1': -39.600000000000314, 'blue_0': -60.40000000000056, 'blue_1': -44.70000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 563
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1086
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1196
(MultiAgentEnvRunner pid=37492) {'red_0': -32.500000000000185, 'red_1': -40.700000000000315, 'blue_0': -49.600000000000435, 'blue_1': -45.400000000000404}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 620
(MultiAgentEnvRunner pid=37492) {'red_0': -34.200000000000244, 'red_1': -34.80000000000021, 'blue_0': -54.400000000000475, 'blue_1': -46.300000000000374}
ITERATION 63: reward=-168.38000000000127, metadata={'num_env_steps_sampled_lifetime': 384000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032671051323204353, 'timers': {'connectors': {'batch_individual_items': 9.589037438541163e-05, 'add_states_from_episodes_to_batch': 6.362697850032998e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2719870216877949e-05, 'numpy_to_tensor': 6.906115180211295e-05, 'agent_to_module_mapping': 8.065509362632337e-06, 'add_observations_from_episodes_to_batch': 3.891195606882977e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008779735549885166, 'timers': {'connectors': {'get_actions': 0.00045233686583073536, 'un_batch_to_individual_items': 6.54451415728201e-05, 'tensor_to_numpy': 0.00012092633579791972, 'module_to_agent_unmapping': 6.544002852850882e-06, 'normalize_and_clip_actions': 7.187606916131055e-05, 'listify_data_for_vector_env': 2.391809241355758e-05, 'remove_single_ts_time_rank_from_batch': 2.3405309693804947e-06}}}, 'sample': 91.76797619997524, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -37.86000000000027, 'blue_0': -55.6800000000005, 'blue_1': -44.20000000000035, 'red_0': -30.640000000000168}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 63.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 384000.0, 'blue_0': 384000.0, 'blue_1': 384000.0, 'red_0': 384000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -37.86000000000027, 'blue_policy': -44.20000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 768000.0, 'blue_policy': 768000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003306723567859298, 'episode_return_mean': -168.38000000000127, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -161.10000000000113, 'episode_duration_sec_mean': 18.220654179994018, 'episode_return_min': -173.80000000000135, 'rlmodule_inference_timer': 0.012889822758189354, 'num_episodes_lifetime': 320.0, 'episode_len_min': 1200, 'time_between_sampling': 287.4132045999868, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.906677314228762, 'throughput_since_last_restore': 15.671414407273248}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 472
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 716
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 902
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 971
(MultiAgentEnvRunner pid=37492) {'red_0': -48.90000000000035, 'red_1': -67.8000000000003, 'blue_0': -23.200000000000095, 'blue_1': -38.30000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -29.40000000000014, 'red_1': -23.700000000000067, 'blue_0': -46.50000000000038, 'blue_1': -56.50000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 291
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 528
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1003
(MultiAgentEnvRunner pid=37492) {'red_0': -39.10000000000026, 'red_1': -36.30000000000018, 'blue_0': -40.80000000000035, 'blue_1': -66.70000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 727
(MultiAgentEnvRunner pid=37492) {'red_0': -37.500000000000256, 'red_1': -19.299999999999997, 'blue_0': -41.70000000000031, 'blue_1': -40.4000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -18.699999999999996, 'red_1': -41.700000000000315, 'blue_0': -37.700000000000266, 'blue_1': -41.50000000000031}
ITERATION 64: reward=-159.14000000000107, metadata={'num_env_steps_sampled_lifetime': 390000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00037266876565877704, 'timers': {'connectors': {'batch_individual_items': 0.00011168415065651211, 'add_states_from_episodes_to_batch': 7.524528198084952e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4575889398445749e-05, 'numpy_to_tensor': 7.556537788309702e-05, 'agent_to_module_mapping': 9.249667284270702e-06, 'add_observations_from_episodes_to_batch': 4.21445509278871e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000960932653307676, 'timers': {'connectors': {'get_actions': 0.0004945754584587612, 'un_batch_to_individual_items': 6.980031715777238e-05, 'tensor_to_numpy': 0.00012932954219284194, 'module_to_agent_unmapping': 6.662338497944565e-06, 'normalize_and_clip_actions': 8.156555056257994e-05, 'listify_data_for_vector_env': 2.5468188184270383e-05, 'remove_single_ts_time_rank_from_batch': 2.7605983489943805e-06}}}, 'sample': 92.18571500002872, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -37.760000000000176, 'blue_0': -37.98000000000028, 'blue_1': -48.680000000000376, 'red_0': -34.7200000000002}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 64.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 390000.0, 'blue_0': 390000.0, 'blue_1': 390000.0, 'red_0': 390000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -37.760000000000176, 'blue_policy': -48.680000000000376}, 'num_module_steps_sampled_lifetime': {'red_policy': 780000.0, 'blue_policy': 780000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036175618222682794, 'episode_return_mean': -159.14000000000107, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -138.90000000000086, 'episode_duration_sec_mean': 18.29990045998711, 'episode_return_min': -182.90000000000123, 'rlmodule_inference_timer': 0.014428424177551124, 'num_episodes_lifetime': 325.0, 'episode_len_min': 1200, 'time_between_sampling': 285.43823670002166, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.005012829673385, 'throughput_since_last_restore': 15.66071261356385}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 525
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 738
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 881
(MultiAgentEnvRunner pid=37492) {'red_0': -36.60000000000048, 'red_1': -22.500000000000103, 'blue_0': -54.900000000000425, 'blue_1': -61.90000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 761
(MultiAgentEnvRunner pid=37492) {'red_0': -47.50000000000033, 'red_1': -45.1000000000003, 'blue_0': -35.900000000000304, 'blue_1': -32.90000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -39.100000000000286, 'red_1': -26.80000000000011, 'blue_0': -39.20000000000029, 'blue_1': -52.100000000000456}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 389
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 543
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 543
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 970
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1003
(MultiAgentEnvRunner pid=37492) {'red_0': -42.000000000000355, 'red_1': -34.00000000000025, 'blue_0': -31.100000000000172, 'blue_1': -42.700000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1100
(MultiAgentEnvRunner pid=37492) {'red_0': -48.600000000000335, 'red_1': -31.80000000000011, 'blue_0': -37.40000000000046, 'blue_1': -33.50000000000028}
ITERATION 65: reward=-159.12000000000123, metadata={'num_env_steps_sampled_lifetime': 396000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003612057311099195, 'timers': {'connectors': {'batch_individual_items': 0.00010610835939735937, 'add_states_from_episodes_to_batch': 7.108192903690477e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3754952751443134e-05, 'numpy_to_tensor': 7.789067609534754e-05, 'agent_to_module_mapping': 8.794490267564345e-06, 'add_observations_from_episodes_to_batch': 4.14799539002019e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009597067761650288, 'timers': {'connectors': {'get_actions': 0.000500818586950373, 'un_batch_to_individual_items': 7.079025850099915e-05, 'tensor_to_numpy': 0.00012658627879783718, 'module_to_agent_unmapping': 6.8896297943793136e-06, 'normalize_and_clip_actions': 7.922711356027794e-05, 'listify_data_for_vector_env': 2.568435978501443e-05, 'remove_single_ts_time_rank_from_batch': 2.609032692630062e-06}}}, 'sample': 93.24667189992033, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.04000000000018, 'blue_0': -39.70000000000033, 'blue_1': -44.62000000000038, 'red_0': -42.76000000000036}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 65.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 396000.0, 'blue_0': 396000.0, 'blue_1': 396000.0, 'red_0': 396000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.04000000000018, 'blue_policy': -44.62000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 792000.0, 'blue_policy': 792000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003542717597284452, 'episode_return_mean': -159.12000000000123, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -149.8000000000011, 'episode_duration_sec_mean': 18.46521314000711, 'episode_return_min': -175.9000000000015, 'rlmodule_inference_timer': 0.014386038273874139, 'num_episodes_lifetime': 330.0, 'episode_len_min': 1200, 'time_between_sampling': 307.68358870001975, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.612789702981182, 'throughput_since_last_restore': 15.643713387096524}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1010
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1196
(MultiAgentEnvRunner pid=37492) {'red_0': -35.900000000000226, 'red_1': -35.500000000000206, 'blue_0': -34.400000000000276, 'blue_1': -43.60000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 468
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 490
(MultiAgentEnvRunner pid=37492) {'red_0': -62.40000000000052, 'red_1': -35.300000000000196, 'blue_0': -43.30000000000041, 'blue_1': -38.000000000000355}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 741
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 792
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 869
(MultiAgentEnvRunner pid=37492) {'red_0': -43.10000000000026, 'red_1': -55.900000000000425, 'blue_0': -24.000000000000163, 'blue_1': -44.80000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 548
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 674
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 987
(MultiAgentEnvRunner pid=37492) {'red_0': -58.60000000000057, 'red_1': -21.39999999999997, 'blue_0': -55.10000000000045, 'blue_1': -55.70000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 421
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 547
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 654
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 719
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1056
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1130
(MultiAgentEnvRunner pid=37492) {'red_0': -25.300000000000033, 'red_1': -28.500000000000103, 'blue_0': -45.00000000000037, 'blue_1': -49.90000000000038}
ITERATION 66: reward=-167.14000000000124, metadata={'num_env_steps_sampled_lifetime': 402000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003712018468053306, 'timers': {'connectors': {'batch_individual_items': 0.00011536069461239592, 'add_states_from_episodes_to_batch': 6.922894658914111e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4039821594788033e-05, 'numpy_to_tensor': 7.683567595992835e-05, 'agent_to_module_mapping': 8.84035053509598e-06, 'add_observations_from_episodes_to_batch': 4.2447362628388785e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009615695899865706, 'timers': {'connectors': {'get_actions': 0.0004979454896794331, 'un_batch_to_individual_items': 7.335191608381881e-05, 'tensor_to_numpy': 0.000128211879982153, 'module_to_agent_unmapping': 6.990474074512126e-06, 'normalize_and_clip_actions': 7.959630766179486e-05, 'listify_data_for_vector_env': 2.6044654929672242e-05, 'remove_single_ts_time_rank_from_batch': 2.5052789826760976e-06}}}, 'sample': 95.73524010006804, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.320000000000185, 'blue_0': -40.36000000000033, 'blue_1': -46.4000000000004, 'red_0': -45.06000000000032}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 66.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 402000.0, 'blue_0': 402000.0, 'blue_1': 402000.0, 'red_0': 402000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.320000000000185, 'blue_policy': -46.4000000000004}, 'num_module_steps_sampled_lifetime': {'red_policy': 804000.0, 'blue_policy': 804000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036185572205681203, 'episode_return_mean': -167.14000000000124, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -148.7000000000009, 'episode_duration_sec_mean': 19.01338136002887, 'episode_return_min': -190.80000000000146, 'rlmodule_inference_timer': 0.014072709826656172, 'num_episodes_lifetime': 335.0, 'episode_len_min': 1200, 'time_between_sampling': 317.35254779993556, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.655301559608164, 'throughput_since_last_restore': 15.609786411300234}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1128
(MultiAgentEnvRunner pid=37492) {'red_0': -40.20000000000029, 'red_1': -19.900000000000013, 'blue_0': -38.7000000000003, 'blue_1': -47.50000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 412
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 682
(MultiAgentEnvRunner pid=37492) {'red_0': -33.80000000000021, 'red_1': -46.50000000000038, 'blue_0': -39.80000000000032, 'blue_1': -39.80000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 544
(MultiAgentEnvRunner pid=37492) {'red_0': -33.90000000000024, 'red_1': -33.0000000000002, 'blue_0': -47.70000000000039, 'blue_1': -43.60000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 371
(MultiAgentEnvRunner pid=37492) {'red_0': -36.20000000000024, 'red_1': -31.700000000000184, 'blue_0': -54.80000000000052, 'blue_1': -64.60000000000058}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 898
(MultiAgentEnvRunner pid=37492) {'red_0': -54.3000000000005, 'red_1': -16.699999999999967, 'blue_0': -51.00000000000044, 'blue_1': -42.900000000000325}
ITERATION 67: reward=-163.32000000000124, metadata={'num_env_steps_sampled_lifetime': 408000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034876073254839547, 'timers': {'connectors': {'batch_individual_items': 0.00010275224722528909, 'add_states_from_episodes_to_batch': 6.823046014760424e-06, 'add_time_dim_to_batch_and_zero_pad': 1.284244464482218e-05, 'numpy_to_tensor': 7.288188451781703e-05, 'agent_to_module_mapping': 9.152079803444228e-06, 'add_observations_from_episodes_to_batch': 4.111287546787646e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000908999626650944, 'timers': {'connectors': {'get_actions': 0.0004675273356135118, 'un_batch_to_individual_items': 6.807611133408457e-05, 'tensor_to_numpy': 0.00012404266616457637, 'module_to_agent_unmapping': 6.580300487591807e-06, 'normalize_and_clip_actions': 7.543680696527885e-05, 'listify_data_for_vector_env': 2.511298246948929e-05, 'remove_single_ts_time_rank_from_batch': 2.886114497344506e-06}}}, 'sample': 103.4412527999375, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.560000000000144, 'blue_0': -46.4000000000004, 'blue_1': -47.68000000000038, 'red_0': -39.6800000000003}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 67.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 408000.0, 'blue_0': 408000.0, 'blue_1': 408000.0, 'red_0': 408000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.560000000000144, 'blue_policy': -47.68000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 816000.0, 'blue_policy': 816000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003412293144744536, 'episode_return_mean': -163.32000000000124, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -146.300000000001, 'episode_duration_sec_mean': 20.535104439989663, 'episode_return_min': -187.30000000000152, 'rlmodule_inference_timer': 0.013509909356171926, 'num_episodes_lifetime': 340.0, 'episode_len_min': 1200, 'time_between_sampling': 343.66304220003076, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.859408262462772, 'throughput_since_last_restore': 15.580846554645285}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 564
(MultiAgentEnvRunner pid=37492) {'red_0': -37.500000000000256, 'red_1': -38.50000000000027, 'blue_0': -35.10000000000026, 'blue_1': -37.50000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 411
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 867
(MultiAgentEnvRunner pid=37492) {'red_0': -32.70000000000017, 'red_1': -33.30000000000018, 'blue_0': -36.40000000000026, 'blue_1': -44.50000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 512
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1133
(MultiAgentEnvRunner pid=37492) {'red_0': -37.60000000000025, 'red_1': -37.70000000000026, 'blue_0': -33.00000000000019, 'blue_1': -43.60000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 429
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 565
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 622
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 658
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 658
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 674
(MultiAgentEnvRunner pid=37492) {'red_0': -24.300000000000107, 'red_1': -62.30000000000054, 'blue_0': -28.40000000000009, 'blue_1': -47.70000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -19.60000000000001, 'red_1': -25.30000000000009, 'blue_0': -41.200000000000315, 'blue_1': -50.50000000000044}
ITERATION 68: reward=-149.340000000001, metadata={'num_env_steps_sampled_lifetime': 414000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035809303253841573, 'timers': {'connectors': {'batch_individual_items': 0.00010665673616492172, 'add_states_from_episodes_to_batch': 6.612521047801458e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3731595749278136e-05, 'numpy_to_tensor': 7.492533658637365e-05, 'agent_to_module_mapping': 8.672030184185852e-06, 'add_observations_from_episodes_to_batch': 4.2036674407869375e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009437656141137341, 'timers': {'connectors': {'get_actions': 0.0004909566670571202, 'un_batch_to_individual_items': 6.874272699995302e-05, 'tensor_to_numpy': 0.00012496431964976112, 'module_to_agent_unmapping': 7.326981981244432e-06, 'normalize_and_clip_actions': 7.532953780563402e-05, 'listify_data_for_vector_env': 2.5295876773006097e-05, 'remove_single_ts_time_rank_from_batch': 2.4879291898476524e-06}}}, 'sample': 96.87453740008641, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -39.42000000000027, 'blue_0': -34.82000000000022, 'blue_1': -44.76000000000037, 'red_0': -30.340000000000156}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 68.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 414000.0, 'blue_0': 414000.0, 'blue_1': 414000.0, 'red_0': 414000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -39.42000000000027, 'blue_policy': -44.76000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 828000.0, 'blue_policy': 828000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034885191915749156, 'episode_return_mean': -149.340000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -136.60000000000088, 'episode_duration_sec_mean': 19.243817739957013, 'episode_return_min': -162.70000000000118, 'rlmodule_inference_timer': 0.014071542664042208, 'num_episodes_lifetime': 345.0, 'episode_len_min': 1200, 'time_between_sampling': 329.4822613999713, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.742899952342702, 'throughput_since_last_restore': 15.55070415622027}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -38.10000000000027, 'red_1': -43.30000000000033, 'blue_0': -46.00000000000038, 'blue_1': -37.90000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 492
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 523
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 523
(MultiAgentEnvRunner pid=37492) {'red_0': -30.30000000000012, 'red_1': -36.900000000000254, 'blue_0': -36.30000000000024, 'blue_1': -43.6000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 187
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 250
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 282
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 549
(MultiAgentEnvRunner pid=37492) {'red_0': -46.30000000000033, 'red_1': -22.500000000000085, 'blue_0': -27.100000000000087, 'blue_1': -37.900000000000254}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 195
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 551
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 950
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 950
(MultiAgentEnvRunner pid=37492) {'red_0': -24.000000000000036, 'red_1': -26.200000000000102, 'blue_0': -37.100000000000215, 'blue_1': -53.60000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 490
(MultiAgentEnvRunner pid=37492) {'red_0': -41.400000000000304, 'red_1': -48.00000000000039, 'blue_0': -44.90000000000036, 'blue_1': -36.60000000000024}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -113.09999999999785, 'red_1': -113.19999999999784, 'blue_0': -112.1999999999979, 'blue_1': -112.99999999999785}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.30000000000000004, 'red_1': -0.2, 'blue_0': -0.5, 'blue_1': -0.2}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) PICKED UP by blue_1 at STEP 137
2026-01-25 22:46:13,800	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -59.20000000000058, 'red_1': -60.600000000000605, 'blue_0': 0.9999999999999979, 'blue_1': -93.9999999999989}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -113.9999999999978, 'red_1': 0, 'blue_0': -1.4000000000000001, 'blue_1': -1.0999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -115.6999999999977, 'blue_1': -1.3}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 69: reward=-151.600000000001, metadata={'num_env_steps_sampled_lifetime': 420000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00039302976667753676, 'timers': {'connectors': {'batch_individual_items': 0.00012217199329900297, 'add_states_from_episodes_to_batch': 7.226189281454718e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4596370438600256e-05, 'numpy_to_tensor': 7.97668939468792e-05, 'agent_to_module_mapping': 9.885472558109794e-06, 'add_observations_from_episodes_to_batch': 4.4071971547009454e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009939174061319367, 'timers': {'connectors': {'get_actions': 0.0005193886878316361, 'un_batch_to_individual_items': 7.337785597540373e-05, 'tensor_to_numpy': 0.00013311415196222106, 'module_to_agent_unmapping': 6.94338350763596e-06, 'normalize_and_clip_actions': 7.875812418764963e-05, 'listify_data_for_vector_env': 2.853843622898904e-05, 'remove_single_ts_time_rank_from_batch': 2.5692511168071506e-06}}}, 'sample': 99.52949869993608, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.38000000000024, 'blue_0': -38.28000000000026, 'blue_1': -41.92000000000031, 'red_0': -36.02000000000021}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 69.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 420000.0, 'blue_0': 420000.0, 'blue_1': 420000.0, 'red_0': 420000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.38000000000024, 'blue_policy': -41.92000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 840000.0, 'blue_policy': 840000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00037620223129078596, 'episode_return_mean': -151.600000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -133.80000000000075, 'episode_duration_sec_mean': 19.7577419599751, 'episode_return_min': -170.90000000000128, 'rlmodule_inference_timer': 0.014884929433386204, 'num_episodes_lifetime': 350.0, 'episode_len_min': 1200, 'time_between_sampling': 339.71512130007613, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 11.568830387889347, 'throughput_since_last_restore': 15.474614174772299}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 233
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 743
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 759
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 887
(MultiAgentEnvRunner pid=37492) {'red_0': -34.30000000000015, 'red_1': -44.100000000000264, 'blue_0': -36.40000000000051, 'blue_1': -44.00000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 528
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 891
(MultiAgentEnvRunner pid=37492) {'red_0': -31.300000000000104, 'red_1': -43.50000000000028, 'blue_0': -44.400000000000404, 'blue_1': -27.300000000000335}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1012
(MultiAgentEnvRunner pid=37492) {'red_0': -24.400000000000105, 'red_1': -21.300000000000033, 'blue_0': -40.50000000000029, 'blue_1': -35.600000000000236}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 570
(MultiAgentEnvRunner pid=37492) {'red_0': -21.300000000000033, 'red_1': -43.500000000000334, 'blue_0': -53.10000000000046, 'blue_1': -41.400000000000304}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 800
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 836
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 916
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 918
(MultiAgentEnvRunner pid=37492) {'red_0': -44.30000000000026, 'red_1': -27.400000000000077, 'blue_0': -30.50000000000034, 'blue_1': -32.1000000000003}
ITERATION 70: reward=-144.14000000000104, metadata={'num_env_steps_sampled_lifetime': 426000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003642352479296204, 'timers': {'connectors': {'batch_individual_items': 0.00011935817218259876, 'add_states_from_episodes_to_batch': 6.884465719787267e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2913548410506048e-05, 'numpy_to_tensor': 7.255870752419439e-05, 'agent_to_module_mapping': 8.472325710363146e-06, 'add_observations_from_episodes_to_batch': 4.215679882971485e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000910956220195873, 'timers': {'connectors': {'get_actions': 0.00046399651197087075, 'un_batch_to_individual_items': 6.73578875839587e-05, 'tensor_to_numpy': 0.0001331544474275571, 'module_to_agent_unmapping': 6.73379867377944e-06, 'normalize_and_clip_actions': 7.328885729063188e-05, 'listify_data_for_vector_env': 2.4435660182940822e-05, 'remove_single_ts_time_rank_from_batch': 2.393910366675882e-06}}}, 'sample': 98.58779429993592, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.96000000000019, 'blue_0': -40.9800000000004, 'blue_1': -36.08000000000032, 'red_0': -31.12000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 70.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 426000.0, 'blue_0': 426000.0, 'blue_1': 426000.0, 'red_0': 426000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.96000000000019, 'blue_policy': -36.08000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 852000.0, 'blue_policy': 852000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003507803167877336, 'episode_return_mean': -144.14000000000104, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -121.80000000000067, 'episode_duration_sec_mean': 19.580367079982533, 'episode_return_min': -159.30000000000115, 'rlmodule_inference_timer': 0.013689436534209468, 'num_episodes_lifetime': 355.0, 'episode_len_min': 1200, 'time_between_sampling': 419.1117694999557, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.688962972583953, 'throughput_since_last_restore': 15.446233919784804}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 438.26517630007584, 'restore_env_runners': 1.2599979527294636e-05, 'training_step': 438.2649196999846, 'env_runner_sampling_timer': 98.72645439999178, 'learner_update_timer': 339.47917649999727, 'synch_weights': 0.014765299973078072, 'synch_env_connectors': 0.002731099957600236, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 426000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003642352479296204, 'timers': {'connectors': {'batch_individual_items': 0.00011935817218259876, 'add_states_from_episodes_to_batch': 6.884465719787267e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2913548410506048e-05, 'numpy_to_tensor': 7.255870752419439e-05, 'agent_to_module_mapping': 8.472325710363146e-06, 'add_observations_from_episodes_to_batch': 4.215679882971485e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000910956220195873, 'timers': {'connectors': {'get_actions': 0.00046399651197087075, 'un_batch_to_individual_items': 6.73578875839587e-05, 'tensor_to_numpy': 0.0001331544474275571, 'module_to_agent_unmapping': 6.73379867377944e-06, 'normalize_and_clip_actions': 7.328885729063188e-05, 'listify_data_for_vector_env': 2.4435660182940822e-05, 'remove_single_ts_time_rank_from_batch': 2.393910366675882e-06}}}, 'sample': 98.58779429993592, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.96000000000019, 'blue_0': -40.9800000000004, 'blue_1': -36.08000000000032, 'red_0': -31.12000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 70.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 426000.0, 'blue_0': 426000.0, 'blue_1': 426000.0, 'red_0': 426000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.96000000000019, 'blue_policy': -36.08000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 852000.0, 'blue_policy': 852000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003507803167877336, 'episode_return_mean': -144.14000000000104, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -121.80000000000067, 'episode_duration_sec_mean': 19.580367079982533, 'episode_return_min': -159.30000000000115, 'rlmodule_inference_timer': 0.013689436534209468, 'num_episodes_lifetime': 355.0, 'episode_len_min': 1200, 'time_between_sampling': 419.1117694999557, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.688962972583953, 'throughput_since_last_restore': 15.446233919784804}}, 'learners': {'red_policy': {'policy_loss': -0.22030401229858398, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.009191704913973808, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 71.0, 'num_module_steps_trained_lifetime': 25582720.0, 'curr_entropy_coeff': 0.04361, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00025739999999999997, 'vf_explained_var': -0.4022538661956787, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.748489379882812, 'entropy': 1.4328893423080444, 'vf_loss_unclipped': 672.177001953125, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 822.0705464799578, 'throughput_since_last_restore': 927.5978229377923}}, 'blue_policy': {'weights_seq_no': 71.0, 'num_module_steps_trained_lifetime': 25582720.0, 'curr_entropy_coeff': 0.04361, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00025739999999999997, 'vf_explained_var': 0.8698407411575317, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.476239413022995, 'total_loss': 0.12529008090496063, 'entropy': 1.747359037399292, 'policy_loss': -0.062493275851011276, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.24369585514068604, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.013463091105222702, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 822.0696516535886, 'throughput_since_last_restore': 927.5978254569449}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 1.0399962775409222e-05, 'batch_individual_items': 0.8151756000006571, 'add_time_dim_to_batch_and_zero_pad': 2.7099973522126675e-05, 'numpy_to_tensor': 0.14790179999545217, 'add_observations_from_episodes_to_batch': 0.00030069996137171984, 'agent_to_module_mapping': 0.022184400004334748, 'add_one_ts_to_episodes_and_truncate': 0.22820989997126162, 'add_columns_from_episodes_to_train_batch': 0.545253699994646, 'general_advantage_estimation': 15.747316400054842}}, 'connector_pipeline_timer': 17.506893799989484}, 'num_module_steps_trained_lifetime': 51165440.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 1199190000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 38534.49187837274, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 38534.490418960595, 'throughput_since_last_restore': 43481.14818921742}, 'num_module_steps_trained_throughput': 1644.138248123162, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1644.1382267417462, 'throughput_since_last_restore': 1855.19565472794}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 426000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 71, 'trial_id': 'default', 'date': '2026-01-25_22-58-31', 'timestamp': 1769378311, 'time_this_iter_s': 438.2811315059662, 'time_total_s': 27558.91903758049, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 27558.91903758049, 'iterations_since_restore': 71, 'perf': {'cpu_util_percent': 19.540894568690096, 'ram_util_percent': 90.59904153354633}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 693
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 939
(MultiAgentEnvRunner pid=37492) {'red_0': -28.600000000000207, 'red_1': -23.300000000000207, 'blue_0': -60.900000000000496, 'blue_1': -53.00000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -37.300000000000246, 'red_1': -35.600000000000236, 'blue_0': -38.00000000000027, 'blue_1': -41.50000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1030
(MultiAgentEnvRunner pid=37492) {'red_0': -33.30000000000019, 'red_1': -38.100000000000264, 'blue_0': -32.900000000000226, 'blue_1': -42.70000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 441
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1147
(MultiAgentEnvRunner pid=37492) {'red_0': -39.40000000000022, 'red_1': -43.00000000000026, 'blue_0': -37.00000000000032, 'blue_1': -24.20000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 319
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 526
(MultiAgentEnvRunner pid=37492) {'red_0': -49.40000000000035, 'red_1': -25.000000000000103, 'blue_0': -41.70000000000039, 'blue_1': -26.100000000000065}
ITERATION 71: reward=-150.2000000000011, metadata={'num_env_steps_sampled_lifetime': 432000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003705053084403412, 'timers': {'connectors': {'batch_individual_items': 0.00010747844912881184, 'add_states_from_episodes_to_batch': 7.08548835827435e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4538739721423157e-05, 'numpy_to_tensor': 7.750852734760706e-05, 'agent_to_module_mapping': 9.675170799121707e-06, 'add_observations_from_episodes_to_batch': 4.4963530681915285e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009716575875465836, 'timers': {'connectors': {'get_actions': 0.0004983708944107915, 'un_batch_to_individual_items': 7.267070858581639e-05, 'tensor_to_numpy': 0.00012875923871402998, 'module_to_agent_unmapping': 6.8522631299702525e-06, 'normalize_and_clip_actions': 7.961864754284811e-05, 'listify_data_for_vector_env': 2.6163213631264368e-05, 'remove_single_ts_time_rank_from_batch': 2.5743801021136463e-06}}}, 'sample': 96.89881759998389, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.00000000000021, 'blue_0': -42.10000000000034, 'blue_1': -37.500000000000284, 'red_0': -37.60000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 71.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 432000.0, 'blue_0': 432000.0, 'blue_1': 432000.0, 'red_0': 432000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.00000000000021, 'blue_policy': -37.500000000000284}, 'num_module_steps_sampled_lifetime': {'red_policy': 864000.0, 'blue_policy': 864000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00038247949159373736, 'episode_return_mean': -150.2000000000011, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -142.2000000000009, 'episode_duration_sec_mean': 19.248864340013824, 'episode_return_min': -165.80000000000132, 'rlmodule_inference_timer': 0.014549976702559182, 'num_episodes_lifetime': 360.0, 'episode_len_min': 1200, 'time_between_sampling': 339.85080610006116, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.443065135733514, 'throughput_since_last_restore': 15.431346367731487}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -29.700000000000145, 'red_1': -16.699999999999967, 'blue_0': -48.70000000000041, 'blue_1': -46.10000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 591
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1092
(MultiAgentEnvRunner pid=37492) {'red_0': -38.600000000000364, 'red_1': -20.60000000000023, 'blue_0': -59.00000000000046, 'blue_1': -57.10000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 373
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 467
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 514
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 545
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1017
(MultiAgentEnvRunner pid=37492) {'red_0': -43.30000000000041, 'red_1': -16.800000000000008, 'blue_0': -46.60000000000034, 'blue_1': -48.000000000000384}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1186
(MultiAgentEnvRunner pid=37492) {'red_0': -42.30000000000025, 'red_1': -23.19999999999999, 'blue_0': -38.10000000000034, 'blue_1': -26.40000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 512
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 984
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1058
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1059
(MultiAgentEnvRunner pid=37492) {'red_0': -23.100000000000012, 'red_1': -29.100000000000094, 'blue_0': -37.60000000000025, 'blue_1': -46.200000000000344}
ITERATION 72: reward=-147.44000000000102, metadata={'num_env_steps_sampled_lifetime': 438000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034793825522814987, 'timers': {'connectors': {'batch_individual_items': 9.925085174291558e-05, 'add_states_from_episodes_to_batch': 6.6901365130757615e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3169762235226555e-05, 'numpy_to_tensor': 7.458070026066768e-05, 'agent_to_module_mapping': 8.659844726385582e-06, 'add_observations_from_episodes_to_batch': 4.240149320943737e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009094558107703998, 'timers': {'connectors': {'get_actions': 0.00046995707516330496, 'un_batch_to_individual_items': 6.836973155193177e-05, 'tensor_to_numpy': 0.00012230732198727798, 'module_to_agent_unmapping': 6.579738498637484e-06, 'normalize_and_clip_actions': 7.38629542074922e-05, 'listify_data_for_vector_env': 2.525552004117943e-05, 'remove_single_ts_time_rank_from_batch': 2.780143249212397e-06}}}, 'sample': 93.9742544000037, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.280000000000058, 'blue_0': -46.000000000000355, 'blue_1': -44.76000000000037, 'red_0': -35.40000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 72.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 438000.0, 'blue_0': 438000.0, 'blue_1': 438000.0, 'red_0': 438000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.280000000000058, 'blue_policy': -44.76000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 876000.0, 'blue_policy': 876000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003464334493632202, 'episode_return_mean': -147.44000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -130.0000000000009, 'episode_duration_sec_mean': 18.666874839994126, 'episode_return_min': -175.3000000000015, 'rlmodule_inference_timer': 0.01375343682139273, 'num_episodes_lifetime': 365.0, 'episode_len_min': 1200, 'time_between_sampling': 318.39883239998017, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.176202556146652, 'throughput_since_last_restore': 15.42779165278544}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 396
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 671
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 889
(MultiAgentEnvRunner pid=37492) {'red_0': -64.50000000000051, 'red_1': -47.40000000000039, 'blue_0': -37.10000000000026, 'blue_1': -35.20000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1047
(MultiAgentEnvRunner pid=37492) {'red_0': -36.400000000000276, 'red_1': -21.600000000000037, 'blue_0': -40.800000000000296, 'blue_1': -45.100000000000364}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 438
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1104
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1117
(MultiAgentEnvRunner pid=37492) {'red_0': -34.80000000000019, 'red_1': -41.1000000000002, 'blue_0': -16.400000000000073, 'blue_1': -31.300000000000246}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 769
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 809
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 934
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1091
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1140
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1140
(MultiAgentEnvRunner pid=37492) {'red_0': -45.40000000000034, 'red_1': -36.90000000000027, 'blue_0': -36.700000000000294, 'blue_1': -22.200000000000117}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 518
(MultiAgentEnvRunner pid=37492) {'red_0': -34.50000000000022, 'red_1': -21.00000000000003, 'blue_0': -38.50000000000028, 'blue_1': -40.400000000000304}
ITERATION 73: reward=-145.46000000000095, metadata={'num_env_steps_sampled_lifetime': 444000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003445346965412657, 'timers': {'connectors': {'batch_individual_items': 0.00010667641451297336, 'add_states_from_episodes_to_batch': 6.561592561279066e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2947877519488687e-05, 'numpy_to_tensor': 7.084679042124094e-05, 'agent_to_module_mapping': 8.267276535433077e-06, 'add_observations_from_episodes_to_batch': 3.9530632607213734e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009227802419446363, 'timers': {'connectors': {'get_actions': 0.0004770107592508738, 'un_batch_to_individual_items': 6.849765544007823e-05, 'tensor_to_numpy': 0.00012313439138274254, 'module_to_agent_unmapping': 6.72847179143069e-06, 'normalize_and_clip_actions': 7.604006547019537e-05, 'listify_data_for_vector_env': 2.5555815771379672e-05, 'remove_single_ts_time_rank_from_batch': 2.4910914616336086e-06}}}, 'sample': 94.48770840000361, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.60000000000018, 'blue_0': -33.90000000000024, 'blue_1': -34.840000000000245, 'red_0': -43.12000000000031}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 73.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 444000.0, 'blue_0': 444000.0, 'blue_1': 444000.0, 'red_0': 444000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.60000000000018, 'blue_policy': -34.840000000000245}, 'num_module_steps_sampled_lifetime': {'red_policy': 888000.0, 'blue_policy': 888000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035378011875356876, 'episode_return_mean': -145.46000000000095, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -123.6000000000007, 'episode_duration_sec_mean': 18.735446040006355, 'episode_return_min': -184.20000000000135, 'rlmodule_inference_timer': 0.013884661134848299, 'num_episodes_lifetime': 370.0, 'episode_len_min': 1200, 'time_between_sampling': 301.3756416999968, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.954012325913993, 'throughput_since_last_restore': 15.421187472590775}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1067
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1173
(MultiAgentEnvRunner pid=37492) {'red_0': -34.60000000000014, 'red_1': -28.300000000000047, 'blue_0': -35.0000000000003, 'blue_1': -31.700000000000415}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 599
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1151
(MultiAgentEnvRunner pid=37492) {'red_0': -25.90000000000013, 'red_1': -31.000000000000227, 'blue_0': -54.50000000000045, 'blue_1': -56.50000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 629
(MultiAgentEnvRunner pid=37492) {'red_0': -17.699999999999974, 'red_1': -26.500000000000107, 'blue_0': -51.600000000000456, 'blue_1': -46.300000000000374}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 761
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 762
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 773
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 960
(MultiAgentEnvRunner pid=37492) {'red_0': -45.80000000000022, 'red_1': -35.80000000000014, 'blue_0': -35.60000000000051, 'blue_1': -21.60000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 523
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 899
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1027
(MultiAgentEnvRunner pid=37492) {'red_0': -50.10000000000044, 'red_1': -35.30000000000023, 'blue_0': -54.70000000000051, 'blue_1': -52.00000000000049}
ITERATION 74: reward=-154.1000000000012, metadata={'num_env_steps_sampled_lifetime': 450000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003532787662804963, 'timers': {'connectors': {'batch_individual_items': 9.987271810587263e-05, 'add_states_from_episodes_to_batch': 7.1193405207190685e-06, 'add_time_dim_to_batch_and_zero_pad': 1.370719665833166e-05, 'numpy_to_tensor': 7.370709857629711e-05, 'agent_to_module_mapping': 9.04412704146954e-06, 'add_observations_from_episodes_to_batch': 4.285403215647984e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009482114041975648, 'timers': {'connectors': {'get_actions': 0.00048696467782548145, 'un_batch_to_individual_items': 7.054821853131375e-05, 'tensor_to_numpy': 0.00012868102959939947, 'module_to_agent_unmapping': 6.920483102048019e-06, 'normalize_and_clip_actions': 7.803380796625433e-05, 'listify_data_for_vector_env': 2.7734955801816995e-05, 'remove_single_ts_time_rank_from_batch': 2.4911025465858163e-06}}}, 'sample': 91.94996120000724, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -31.380000000000148, 'blue_0': -46.28000000000044, 'blue_1': -41.62000000000039, 'red_0': -34.82000000000018}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 74.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 450000.0, 'blue_0': 450000.0, 'blue_1': 450000.0, 'red_0': 450000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -31.380000000000148, 'blue_policy': -41.62000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 900000.0, 'blue_policy': 900000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003591340043407258, 'episode_return_mean': -154.1000000000012, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -129.6000000000009, 'episode_duration_sec_mean': 18.261128980037757, 'episode_return_min': -192.10000000000167, 'rlmodule_inference_timer': 0.014663970517141875, 'num_episodes_lifetime': 375.0, 'episode_len_min': 1200, 'time_between_sampling': 306.74601780006196, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.701022616438548, 'throughput_since_last_restore': 15.411120365942669}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 265
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 983
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1140
(MultiAgentEnvRunner pid=37492) {'red_0': -33.10000000000025, 'red_1': -49.900000000000375, 'blue_0': -49.10000000000035, 'blue_1': -46.00000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 821
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 978
(MultiAgentEnvRunner pid=37492) {'red_0': -15.499999999999961, 'red_1': -35.9000000000003, 'blue_0': -55.50000000000049, 'blue_1': -51.900000000000446}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 372
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 382
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 382
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1080
(MultiAgentEnvRunner pid=37492) {'red_0': -20.89999999999999, 'red_1': -12.999999999999963, 'blue_0': -55.900000000000475, 'blue_1': -42.90000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 397
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 518
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 723
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 748
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 944
(MultiAgentEnvRunner pid=37492) {'red_0': -30.500000000000192, 'red_1': -36.30000000000011, 'blue_0': -37.90000000000039, 'blue_1': -38.8000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 291
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 946
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1168
(MultiAgentEnvRunner pid=37492) {'red_0': -24.300000000000004, 'red_1': -36.90000000000024, 'blue_0': -47.10000000000045, 'blue_1': -25.400000000000304}
ITERATION 75: reward=-149.3600000000011, metadata={'num_env_steps_sampled_lifetime': 456000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003389165810309316, 'timers': {'connectors': {'batch_individual_items': 9.981329643513558e-05, 'add_states_from_episodes_to_batch': 6.6593070774793e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3496720546856496e-05, 'numpy_to_tensor': 7.030195591590195e-05, 'agent_to_module_mapping': 8.414636091317278e-06, 'add_observations_from_episodes_to_batch': 3.985155513566687e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000907073671567477, 'timers': {'connectors': {'get_actions': 0.00047023313158460126, 'un_batch_to_individual_items': 6.993177796414112e-05, 'tensor_to_numpy': 0.00012066188298837186, 'module_to_agent_unmapping': 7.200331845621893e-06, 'normalize_and_clip_actions': 7.381646605260987e-05, 'listify_data_for_vector_env': 2.4534168505018118e-05, 'remove_single_ts_time_rank_from_batch': 2.5386004355970004e-06}}}, 'sample': 95.96975140005816, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -34.40000000000019, 'blue_0': -49.10000000000043, 'blue_1': -41.00000000000036, 'red_0': -24.86000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 75.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 456000.0, 'blue_0': 456000.0, 'blue_1': 456000.0, 'red_0': 456000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -34.40000000000019, 'blue_policy': -41.00000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 912000.0, 'blue_policy': 912000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034330041476879667, 'episode_return_mean': -149.3600000000011, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -132.70000000000078, 'episode_duration_sec_mean': 19.044371979986316, 'episode_return_min': -178.10000000000133, 'rlmodule_inference_timer': 0.013722387999787021, 'num_episodes_lifetime': 380.0, 'episode_len_min': 1200, 'time_between_sampling': 316.18544040003326, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.712175817891316, 'throughput_since_last_restore': 15.40149184683478}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 987
(MultiAgentEnvRunner pid=37492) {'red_0': -27.200000000000102, 'red_1': -34.600000000000215, 'blue_0': -46.80000000000039, 'blue_1': -39.30000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 970
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1132
(MultiAgentEnvRunner pid=37492) {'red_0': -28.600000000000044, 'red_1': -52.500000000000334, 'blue_0': -26.100000000000378, 'blue_1': -34.800000000000495}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -28.400000000000126, 'red_1': -19.200000000000003, 'blue_0': -43.30000000000034, 'blue_1': -41.100000000000314}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 351
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 497
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 694
(MultiAgentEnvRunner pid=37492) {'red_0': -45.90000000000038, 'red_1': -32.70000000000022, 'blue_0': -38.50000000000022, 'blue_1': -24.80000000000007}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 457
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 670
(MultiAgentEnvRunner pid=37492) {'red_0': -29.800000000000207, 'red_1': -30.00000000000017, 'blue_0': -54.800000000000416, 'blue_1': -48.500000000000476}
ITERATION 76: reward=-145.38000000000105, metadata={'num_env_steps_sampled_lifetime': 462000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033823743160510834, 'timers': {'connectors': {'batch_individual_items': 0.00010513474580637895, 'add_states_from_episodes_to_batch': 6.320845664763994e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2462685564750978e-05, 'numpy_to_tensor': 6.890281817755087e-05, 'agent_to_module_mapping': 8.583675214604022e-06, 'add_observations_from_episodes_to_batch': 3.947179031784484e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008816124827015516, 'timers': {'connectors': {'get_actions': 0.0004511387472635045, 'un_batch_to_individual_items': 6.609415215308071e-05, 'tensor_to_numpy': 0.00012040616806065731, 'module_to_agent_unmapping': 6.809002782598641e-06, 'normalize_and_clip_actions': 7.187985527719745e-05, 'listify_data_for_vector_env': 2.4221273073657833e-05, 'remove_single_ts_time_rank_from_batch': 2.3263657811158755e-06}}}, 'sample': 92.94899790000636, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.80000000000019, 'blue_0': -41.90000000000035, 'blue_1': -37.70000000000034, 'red_0': -31.98000000000017}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 76.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 462000.0, 'blue_0': 462000.0, 'blue_1': 462000.0, 'red_0': 462000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.80000000000019, 'blue_policy': -37.70000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 924000.0, 'blue_policy': 924000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035233489675898987, 'episode_return_mean': -145.38000000000105, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -132.00000000000077, 'episode_duration_sec_mean': 18.461676819995045, 'episode_return_min': -163.10000000000127, 'rlmodule_inference_timer': 0.013084614342559519, 'num_episodes_lifetime': 385.0, 'episode_len_min': 1200, 'time_between_sampling': 311.8644597999519, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.726145424371396, 'throughput_since_last_restore': 15.392323124556805}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 907
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 977
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 980
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1108
(MultiAgentEnvRunner pid=37492) {'red_0': -42.20000000000024, 'red_1': -40.80000000000025, 'blue_0': -63.10000000000066, 'blue_1': -19.400000000000276}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 693
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 769
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 785
(MultiAgentEnvRunner pid=37492) {'red_0': -23.400000000000095, 'red_1': -41.500000000000306, 'blue_0': -38.70000000000028, 'blue_1': -60.80000000000063}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 135
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 207
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 213
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 600
(MultiAgentEnvRunner pid=37492) {'red_0': -9.199999999999958, 'red_1': -9.999999999999984, 'blue_0': -66.40000000000047, 'blue_1': -52.6000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 670
(MultiAgentEnvRunner pid=37492) {'red_0': -26.30000000000018, 'red_1': -30.30000000000024, 'blue_0': -59.100000000000485, 'blue_1': -56.50000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 476
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 477
(MultiAgentEnvRunner pid=37492) {'red_0': -22.80000000000005, 'red_1': -28.100000000000133, 'blue_0': -58.600000000000534, 'blue_1': -45.20000000000035}
ITERATION 77: reward=-159.0000000000012, metadata={'num_env_steps_sampled_lifetime': 468000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032841289502055803, 'timers': {'connectors': {'batch_individual_items': 9.701950870537061e-05, 'add_states_from_episodes_to_batch': 6.397252354462654e-06, 'add_time_dim_to_batch_and_zero_pad': 1.293928107755061e-05, 'numpy_to_tensor': 6.66217782586886e-05, 'agent_to_module_mapping': 8.52106184820637e-06, 'add_observations_from_episodes_to_batch': 3.8629363953515606e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008648304545303203, 'timers': {'connectors': {'get_actions': 0.00044733230242227005, 'un_batch_to_individual_items': 6.495040084864343e-05, 'tensor_to_numpy': 0.00011649376940212874, 'module_to_agent_unmapping': 6.410871444246986e-06, 'normalize_and_clip_actions': 7.246928925706847e-05, 'listify_data_for_vector_env': 2.3391342651768528e-05, 'remove_single_ts_time_rank_from_batch': 2.3082166428112515e-06}}}, 'sample': 95.33398370002396, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.140000000000178, 'blue_0': -57.18000000000048, 'blue_1': -46.90000000000044, 'red_0': -24.780000000000108}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 77.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 468000.0, 'blue_0': 468000.0, 'blue_1': 468000.0, 'red_0': 468000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.140000000000178, 'blue_policy': -46.90000000000044}, 'num_module_steps_sampled_lifetime': {'red_policy': 936000.0, 'blue_policy': 936000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032626878773553387, 'episode_return_mean': -159.0000000000012, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -138.20000000000093, 'episode_duration_sec_mean': 18.94244802002795, 'episode_return_min': -172.20000000000135, 'rlmodule_inference_timer': 0.012920033400343693, 'num_episodes_lifetime': 390.0, 'episode_len_min': 1200, 'time_between_sampling': 314.49454179999884, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.539969096251374, 'throughput_since_last_restore': 15.380762456597965}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 604
(MultiAgentEnvRunner pid=37492) {'red_0': -62.100000000000534, 'red_1': -20.800000000000043, 'blue_0': -38.10000000000031, 'blue_1': -31.100000000000236}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 327
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 371
(MultiAgentEnvRunner pid=37492) {'red_0': -34.50000000000023, 'red_1': -19.500000000000004, 'blue_0': -43.70000000000033, 'blue_1': -46.00000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 414
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1162
(MultiAgentEnvRunner pid=37492) {'red_0': -23.19999999999999, 'red_1': -42.80000000000029, 'blue_0': -33.80000000000041, 'blue_1': -42.200000000000394}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 755
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 889
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 966
(MultiAgentEnvRunner pid=37492) {'red_0': -5.899999999999977, 'red_1': -14.299999999999946, 'blue_0': -56.000000000000426, 'blue_1': -59.000000000000476}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 142
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 176
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1167
(MultiAgentEnvRunner pid=37492) {'red_0': -24.600000000000286, 'red_1': -27.20000000000019, 'blue_0': -52.900000000000404, 'blue_1': -51.20000000000037}
ITERATION 78: reward=-145.78000000000102, metadata={'num_env_steps_sampled_lifetime': 474000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003544572734938936, 'timers': {'connectors': {'batch_individual_items': 0.00010046499180670407, 'add_states_from_episodes_to_batch': 7.194799068822884e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3709693027417404e-05, 'numpy_to_tensor': 7.401738803071634e-05, 'agent_to_module_mapping': 8.825190992091413e-06, 'add_observations_from_episodes_to_batch': 4.338494291886779e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009317119717224979, 'timers': {'connectors': {'get_actions': 0.00047887941403002536, 'un_batch_to_individual_items': 6.848718512652958e-05, 'tensor_to_numpy': 0.00012692593381244684, 'module_to_agent_unmapping': 6.7086283280772815e-06, 'normalize_and_clip_actions': 7.688856600664829e-05, 'listify_data_for_vector_env': 2.6074254608508218e-05, 'remove_single_ts_time_rank_from_batch': 2.506756575825411e-06}}}, 'sample': 95.658262400073, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -24.920000000000094, 'blue_0': -44.900000000000375, 'blue_1': -45.900000000000375, 'red_0': -30.0600000000002}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 78.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 474000.0, 'blue_0': 474000.0, 'blue_1': 474000.0, 'red_0': 474000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -24.920000000000094, 'blue_policy': -45.900000000000375}, 'num_module_steps_sampled_lifetime': {'red_policy': 948000.0, 'blue_policy': 948000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003591636633487461, 'episode_return_mean': -145.78000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -135.20000000000084, 'episode_duration_sec_mean': 18.96849570001941, 'episode_return_min': -155.90000000000126, 'rlmodule_inference_timer': 0.014145778552692653, 'num_episodes_lifetime': 395.0, 'episode_len_min': 1200, 'time_between_sampling': 317.31603200000245, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.57779710409285, 'throughput_since_last_restore': 15.370044005461484}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 427
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1081
(MultiAgentEnvRunner pid=37492) {'red_0': -14.799999999999956, 'red_1': -31.100000000000193, 'blue_0': -49.70000000000042, 'blue_1': -56.5000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 383
(MultiAgentEnvRunner pid=37492) {'red_0': -34.20000000000015, 'red_1': -18.50000000000001, 'blue_0': -38.500000000000256, 'blue_1': -24.800000000000065}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 791
(MultiAgentEnvRunner pid=37492) {'red_0': -27.000000000000043, 'red_1': -25.100000000000016, 'blue_0': -28.700000000000262, 'blue_1': -46.10000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 338
(MultiAgentEnvRunner pid=37492) {'red_0': -17.299999999999976, 'red_1': -34.10000000000021, 'blue_0': -43.10000000000033, 'blue_1': -50.30000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 423
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 472
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 472
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 914
(MultiAgentEnvRunner pid=37492) {'red_0': -25.40000000000003, 'red_1': -23.500000000000036, 'blue_0': -13.099999999999945, 'blue_1': -35.10000000000028}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -114.59999999999776, 'red_1': -113.09999999999785, 'blue_0': -112.49999999999788, 'blue_1': -113.19999999999784}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -2.400000000000001, 'red_1': 0, 'blue_0': -1.5000000000000002, 'blue_1': -113.29999999999784}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-25 23:56:24,771	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -0.2, 'red_1': 0, 'blue_0': -0.2, 'blue_1': -0.4}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': -0.5, 'blue_0': -0.30000000000000004, 'blue_1': -0.8999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.1, 'blue_1': -0.2}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 79: reward=-127.3800000000007, metadata={'num_env_steps_sampled_lifetime': 480000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034209350097696517, 'timers': {'connectors': {'batch_individual_items': 0.00010318473185107592, 'add_states_from_episodes_to_batch': 6.62510682433839e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3112380614765133e-05, 'numpy_to_tensor': 7.26218610579059e-05, 'agent_to_module_mapping': 8.432989877271727e-06, 'add_observations_from_episodes_to_batch': 4.016243472266735e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009176514396772592, 'timers': {'connectors': {'get_actions': 0.000478354477290509, 'un_batch_to_individual_items': 6.851566239030963e-05, 'tensor_to_numpy': 0.0001211187976839354, 'module_to_agent_unmapping': 6.809347424051733e-06, 'normalize_and_clip_actions': 7.518737981492221e-05, 'listify_data_for_vector_env': 2.4439913705186088e-05, 'remove_single_ts_time_rank_from_batch': 2.588778146411418e-06}}}, 'sample': 93.83584269997664, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.460000000000093, 'blue_0': -34.62000000000024, 'blue_1': -42.56000000000034, 'red_0': -23.74000000000003}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 79.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 480000.0, 'blue_0': 480000.0, 'blue_1': 480000.0, 'red_0': 480000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.460000000000093, 'blue_policy': -42.56000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 960000.0, 'blue_policy': 960000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003504421573982814, 'episode_return_mean': -127.3800000000007, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -97.10000000000028, 'episode_duration_sec_mean': 18.64667690000497, 'episode_return_min': -152.10000000000105, 'rlmodule_inference_timer': 0.013819042664374304, 'num_episodes_lifetime': 400.0, 'episode_len_min': 1200, 'time_between_sampling': 315.9350826999871, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 11.989487602882669, 'throughput_since_last_restore': 15.31606097774976}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 336
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 837
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1054
(MultiAgentEnvRunner pid=37492) {'red_0': -46.40000000000034, 'red_1': -38.70000000000021, 'blue_0': -43.5000000000004, 'blue_1': -27.50000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 696
(MultiAgentEnvRunner pid=37492) {'red_0': -27.300000000000136, 'red_1': -28.200000000000117, 'blue_0': -23.800000000000068, 'blue_1': -33.50000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 460
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1047
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1052
(MultiAgentEnvRunner pid=37492) {'red_0': -30.900000000000105, 'red_1': -31.500000000000092, 'blue_0': -35.50000000000031, 'blue_1': -19.70000000000006}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 251
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 714
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 888
(MultiAgentEnvRunner pid=37492) {'red_0': -18.099999999999977, 'red_1': -32.30000000000035, 'blue_0': -47.60000000000036, 'blue_1': -49.400000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -17.099999999999977, 'red_1': -37.70000000000025, 'blue_0': -50.10000000000042, 'blue_1': -39.30000000000028}
ITERATION 80: reward=-135.62000000000089, metadata={'num_env_steps_sampled_lifetime': 486000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003604225119339241, 'timers': {'connectors': {'batch_individual_items': 0.00011741330345581233, 'add_states_from_episodes_to_batch': 6.843276648336481e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3059477596232698e-05, 'numpy_to_tensor': 7.208108697268427e-05, 'agent_to_module_mapping': 8.504392333595792e-06, 'add_observations_from_episodes_to_batch': 4.059415803735845e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009219507662999235, 'timers': {'connectors': {'get_actions': 0.0004765022306012743, 'un_batch_to_individual_items': 6.975127543862811e-05, 'tensor_to_numpy': 0.0001251212812933721, 'module_to_agent_unmapping': 6.743161418984301e-06, 'normalize_and_clip_actions': 7.540725572400173e-05, 'listify_data_for_vector_env': 2.5062333265716786e-05, 'remove_single_ts_time_rank_from_batch': 2.4533897199557223e-06}}}, 'sample': 93.52967349998653, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.6800000000002, 'blue_0': -40.100000000000314, 'blue_1': -33.880000000000244, 'red_0': -27.960000000000104}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 80.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 486000.0, 'blue_0': 486000.0, 'blue_1': 486000.0, 'red_0': 486000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.6800000000002, 'blue_policy': -33.880000000000244}, 'num_module_steps_sampled_lifetime': {'red_policy': 972000.0, 'blue_policy': 972000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003842405550394834, 'episode_return_mean': -135.62000000000089, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -112.80000000000061, 'episode_duration_sec_mean': 18.47295357999392, 'episode_return_min': -156.10000000000116, 'rlmodule_inference_timer': 0.013887482922424279, 'num_episodes_lifetime': 405.0, 'episode_len_min': 1200, 'time_between_sampling': 406.60998079995625, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.235999379522452, 'throughput_since_last_restore': 15.301726502273866}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 421.42563520011026, 'restore_env_runners': 9.599956683814526e-06, 'training_step': 421.42540409998037, 'env_runner_sampling_timer': 93.72938099992462, 'learner_update_timer': 327.62757860007696, 'synch_weights': 0.016138599952682853, 'synch_env_connectors': 0.004672299954108894, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 486000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003604225119339241, 'timers': {'connectors': {'batch_individual_items': 0.00011741330345581233, 'add_states_from_episodes_to_batch': 6.843276648336481e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3059477596232698e-05, 'numpy_to_tensor': 7.208108697268427e-05, 'agent_to_module_mapping': 8.504392333595792e-06, 'add_observations_from_episodes_to_batch': 4.059415803735845e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009219507662999235, 'timers': {'connectors': {'get_actions': 0.0004765022306012743, 'un_batch_to_individual_items': 6.975127543862811e-05, 'tensor_to_numpy': 0.0001251212812933721, 'module_to_agent_unmapping': 6.743161418984301e-06, 'normalize_and_clip_actions': 7.540725572400173e-05, 'listify_data_for_vector_env': 2.5062333265716786e-05, 'remove_single_ts_time_rank_from_batch': 2.4533897199557223e-06}}}, 'sample': 93.52967349998653, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.6800000000002, 'blue_0': -40.100000000000314, 'blue_1': -33.880000000000244, 'red_0': -27.960000000000104}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 80.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 486000.0, 'blue_0': 486000.0, 'blue_1': 486000.0, 'red_0': 486000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.6800000000002, 'blue_policy': -33.880000000000244}, 'num_module_steps_sampled_lifetime': {'red_policy': 972000.0, 'blue_policy': 972000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003842405550394834, 'episode_return_mean': -135.62000000000089, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -112.80000000000061, 'episode_duration_sec_mean': 18.47295357999392, 'episode_return_min': -156.10000000000116, 'rlmodule_inference_timer': 0.013887482922424279, 'num_episodes_lifetime': 405.0, 'episode_len_min': 1200, 'time_between_sampling': 406.60998079995625, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.235999379522452, 'throughput_since_last_restore': 15.301726502273866}}, 'learners': {'red_policy': {'policy_loss': -0.22694149613380432, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.009107028134167194, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 81.0, 'num_module_steps_trained_lifetime': 29185920.0, 'curr_entropy_coeff': 0.042710000000000005, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002514, 'vf_explained_var': -0.44512176513671875, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.75169563293457, 'entropy': 1.2262495756149292, 'vf_loss_unclipped': 697.8790893554688, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 854.9230753957074, 'throughput_since_last_restore': 918.9196720787046}}, 'blue_policy': {'weights_seq_no': 81.0, 'num_module_steps_trained_lifetime': 29185920.0, 'curr_entropy_coeff': 0.042710000000000005, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002514, 'vf_explained_var': 0.8318580985069275, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.6530802249908447, 'total_loss': 0.12422740459442139, 'entropy': 1.7400732040405273, 'policy_loss': -0.24215209484100342, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.42109307646751404, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.013011740520596504, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 854.9218287080814, 'throughput_since_last_restore': 918.9196742196881}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 9.499955922365189e-06, 'batch_individual_items': 1.211781800026074, 'add_time_dim_to_batch_and_zero_pad': 3.839994315057993e-05, 'numpy_to_tensor': 0.1583992000669241, 'add_observations_from_episodes_to_batch': 0.0003400000277906656, 'agent_to_module_mapping': 0.029360000044107437, 'add_one_ts_to_episodes_and_truncate': 0.2250633001094684, 'add_columns_from_episodes_to_train_batch': 0.5850823000073433, 'general_advantage_estimation': 15.07674060005229}}, 'connector_pipeline_timer': 17.28731569997035}, 'num_module_steps_trained_lifetime': 58371840.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 1368090000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 40074.42553028191, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 40074.42416106975, 'throughput_since_last_restore': 43074.35983062707}, 'num_module_steps_trained_throughput': 1709.8420938882161, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1709.842077660045, 'throughput_since_last_restore': 1837.839351668216}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 486000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 81, 'trial_id': 'default', 'date': '2026-01-26_00-08-12', 'timestamp': 1769382492, 'time_this_iter_s': 421.4415235519409, 'time_total_s': 31740.12948322296, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 31740.12948322296, 'iterations_since_restore': 81, 'perf': {'cpu_util_percent': 16.970216306156406, 'ram_util_percent': 91.655074875208}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 878
(MultiAgentEnvRunner pid=37492) {'red_0': -42.300000000000395, 'red_1': -22.20000000000015, 'blue_0': -51.20000000000038, 'blue_1': -52.8000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 576
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 616
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1043
(MultiAgentEnvRunner pid=37492) {'red_0': -18.600000000000254, 'red_1': -22.500000000000156, 'blue_0': -58.600000000000435, 'blue_1': -50.60000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 729
(MultiAgentEnvRunner pid=37492) {'red_0': -29.800000000000146, 'red_1': -18.100000000000005, 'blue_0': -30.800000000000214, 'blue_1': -36.200000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 509
(MultiAgentEnvRunner pid=37492) {'red_0': -26.30000000000012, 'red_1': -23.000000000000075, 'blue_0': -40.90000000000036, 'blue_1': -32.70000000000015}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 506
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 527
(MultiAgentEnvRunner pid=37492) {'red_0': -39.20000000000028, 'red_1': -18.599999999999987, 'blue_0': -40.3000000000003, 'blue_1': -40.00000000000029}
ITERATION 81: reward=-138.94000000000094, metadata={'num_env_steps_sampled_lifetime': 492000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000354976165775877, 'timers': {'connectors': {'batch_individual_items': 0.00011676660748067456, 'add_states_from_episodes_to_batch': 6.617098901654183e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3279780973273872e-05, 'numpy_to_tensor': 7.030536454711005e-05, 'agent_to_module_mapping': 8.728013282169018e-06, 'add_observations_from_episodes_to_batch': 3.9275929089380584e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008920298425964601, 'timers': {'connectors': {'get_actions': 0.00046571152780599395, 'un_batch_to_individual_items': 6.460025814115874e-05, 'tensor_to_numpy': 0.00011928380490292494, 'module_to_agent_unmapping': 6.464943638831588e-06, 'normalize_and_clip_actions': 7.25413675114819e-05, 'listify_data_for_vector_env': 2.441274750419704e-05, 'remove_single_ts_time_rank_from_batch': 2.3806534078170573e-06}}}, 'sample': 93.63166549999733, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -20.880000000000074, 'blue_0': -44.36000000000033, 'blue_1': -42.4600000000003, 'red_0': -31.240000000000236}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 81.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 492000.0, 'blue_0': 492000.0, 'blue_1': 492000.0, 'red_0': 492000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -20.880000000000074, 'blue_policy': -42.4600000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 984000.0, 'blue_policy': 984000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034165630640482287, 'episode_return_mean': -138.94000000000094, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -114.90000000000067, 'episode_duration_sec_mean': 18.57368102001492, 'episode_return_min': -168.50000000000134, 'rlmodule_inference_timer': 0.013308416943781706, 'num_episodes_lifetime': 410.0, 'episode_len_min': 1200, 'time_between_sampling': 328.05424880003557, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.748210755244262, 'throughput_since_last_restore': 15.29472507883913}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 280
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 358
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1041
(MultiAgentEnvRunner pid=37492) {'red_0': -1.4000000000000024, 'red_1': -34.00000000000016, 'blue_0': -55.60000000000045, 'blue_1': -48.60000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 601
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1037
(MultiAgentEnvRunner pid=37492) {'red_0': -37.000000000000185, 'red_1': -51.400000000000354, 'blue_0': -35.200000000000436, 'blue_1': -33.1000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 715
(MultiAgentEnvRunner pid=37492) {'red_0': -27.200000000000113, 'red_1': -23.500000000000046, 'blue_0': -36.70000000000041, 'blue_1': -50.7000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.399999999999983, 'red_1': -16.999999999999975, 'blue_0': -42.600000000000335, 'blue_1': -46.80000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 354
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 399
(MultiAgentEnvRunner pid=37492) {'red_0': -19.099999999999977, 'red_1': -16.299999999999947, 'blue_0': -53.600000000000406, 'blue_1': -47.80000000000035}
ITERATION 82: reward=-137.40000000000094, metadata={'num_env_steps_sampled_lifetime': 498000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003345061433811513, 'timers': {'connectors': {'batch_individual_items': 0.00010160196429441508, 'add_states_from_episodes_to_batch': 6.295636795179562e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2245474314061067e-05, 'numpy_to_tensor': 7.220293085466548e-05, 'agent_to_module_mapping': 8.242736655862444e-06, 'add_observations_from_episodes_to_batch': 3.787138270259385e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008628902443189396, 'timers': {'connectors': {'get_actions': 0.0004418713477626322, 'un_batch_to_individual_items': 6.610197380266911e-05, 'tensor_to_numpy': 0.00011902418416559691, 'module_to_agent_unmapping': 6.55636696095267e-06, 'normalize_and_clip_actions': 7.135445350818979e-05, 'listify_data_for_vector_env': 2.3200507638319684e-05, 'remove_single_ts_time_rank_from_batch': 2.3101950267575684e-06}}}, 'sample': 94.69577379990369, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.440000000000094, 'blue_0': -44.74000000000041, 'blue_1': -45.400000000000404, 'red_0': -18.820000000000054}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 82.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 498000.0, 'blue_0': 498000.0, 'blue_1': 498000.0, 'red_0': 498000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.440000000000094, 'blue_policy': -45.400000000000404}, 'num_module_steps_sampled_lifetime': {'red_policy': 996000.0, 'blue_policy': 996000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003438757510176194, 'episode_return_mean': -137.40000000000094, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -115.80000000000067, 'episode_duration_sec_mean': 18.810577799985186, 'episode_return_min': -156.7000000000013, 'rlmodule_inference_timer': 0.012990027604842764, 'num_episodes_lifetime': 415.0, 'episode_len_min': 1200, 'time_between_sampling': 313.0754613999743, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.03314263751978, 'throughput_since_last_restore': 15.291518260542475}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 525
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 540
(MultiAgentEnvRunner pid=37492) {'red_0': -28.900000000000162, 'red_1': -25.000000000000103, 'blue_0': -23.800000000000033, 'blue_1': -34.300000000000274}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 927
(MultiAgentEnvRunner pid=37492) {'red_0': -18.300000000000008, 'red_1': -21.20000000000001, 'blue_0': -42.90000000000039, 'blue_1': -33.800000000000416}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -35.400000000000226, 'red_1': -17.899999999999984, 'blue_0': -45.100000000000364, 'blue_1': -36.70000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -14.599999999999964, 'red_1': -16.99999999999997, 'blue_0': -36.900000000000254, 'blue_1': -50.200000000000436}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 737
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 814
(MultiAgentEnvRunner pid=37492) {'red_0': -37.900000000000254, 'red_1': -15.299999999999955, 'blue_0': -45.90000000000036, 'blue_1': -46.1000000000004}
ITERATION 83: reward=-125.44000000000077, metadata={'num_env_steps_sampled_lifetime': 504000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003397905600399479, 'timers': {'connectors': {'batch_individual_items': 0.00010900608166432836, 'add_states_from_episodes_to_batch': 6.601916022897723e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2259545401756595e-05, 'numpy_to_tensor': 6.783849086324896e-05, 'agent_to_module_mapping': 8.156596136449022e-06, 'add_observations_from_episodes_to_batch': 3.924188675096545e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008704609245036649, 'timers': {'connectors': {'get_actions': 0.00044873479830739675, 'un_batch_to_individual_items': 6.424228424662156e-05, 'tensor_to_numpy': 0.00011863716332748088, 'module_to_agent_unmapping': 6.387157388261429e-06, 'normalize_and_clip_actions': 7.154810810883944e-05, 'listify_data_for_vector_env': 2.4751126270175723e-05, 'remove_single_ts_time_rank_from_batch': 2.4141615473845556e-06}}}, 'sample': 97.329791199998, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -19.28000000000001, 'blue_0': -38.92000000000028, 'blue_1': -40.220000000000354, 'red_0': -27.020000000000124}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 83.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 504000.0, 'blue_0': 504000.0, 'blue_1': 504000.0, 'red_0': 504000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -19.28000000000001, 'blue_policy': -40.220000000000354}, 'num_module_steps_sampled_lifetime': {'red_policy': 1008000.0, 'blue_policy': 1008000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033434402777259405, 'episode_return_mean': -125.44000000000077, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -112.00000000000057, 'episode_duration_sec_mean': 19.320569660002366, 'episode_return_min': -145.20000000000095, 'rlmodule_inference_timer': 0.012804879268844545, 'num_episodes_lifetime': 420.0, 'episode_len_min': 1200, 'time_between_sampling': 304.42936810001265, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.994942127297083, 'throughput_since_last_restore': 15.287917100142293}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1130
(MultiAgentEnvRunner pid=37492) {'red_0': -55.600000000000435, 'red_1': -37.700000000000195, 'blue_0': -40.30000000000037, 'blue_1': -23.000000000000263}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 282
(MultiAgentEnvRunner pid=37492) {'red_0': -64.90000000000053, 'red_1': -14.299999999999965, 'blue_0': -43.10000000000034, 'blue_1': -47.00000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 665
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1038
(MultiAgentEnvRunner pid=37492) {'red_0': -62.80000000000061, 'red_1': -30.800000000000157, 'blue_0': -52.70000000000047, 'blue_1': -33.50000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 559
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1135
(MultiAgentEnvRunner pid=37492) {'red_0': -27.600000000000122, 'red_1': -36.00000000000023, 'blue_0': -50.90000000000044, 'blue_1': -45.000000000000384}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 448
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 572
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 572
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 805
(MultiAgentEnvRunner pid=37492) {'red_0': -49.6000000000004, 'red_1': -20.900000000000038, 'blue_0': -40.400000000000375, 'blue_1': -33.400000000000176}
ITERATION 84: reward=-161.90000000000123, metadata={'num_env_steps_sampled_lifetime': 510000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003353911690355129, 'timers': {'connectors': {'batch_individual_items': 0.00010027298141547264, 'add_states_from_episodes_to_batch': 6.706323643918485e-06, 'add_time_dim_to_batch_and_zero_pad': 1.248419481770434e-05, 'numpy_to_tensor': 6.890156848496e-05, 'agent_to_module_mapping': 8.292992332122552e-06, 'add_observations_from_episodes_to_batch': 3.97347374475906e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008882883748988954, 'timers': {'connectors': {'get_actions': 0.0004581498695525488, 'un_batch_to_individual_items': 6.703172216968477e-05, 'tensor_to_numpy': 0.00011933356757580561, 'module_to_agent_unmapping': 6.692065228531557e-06, 'normalize_and_clip_actions': 7.259723722378276e-05, 'listify_data_for_vector_env': 2.4567418112727333e-05, 'remove_single_ts_time_rank_from_batch': 2.340985537716033e-06}}}, 'sample': 92.61801850004122, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.94000000000012, 'blue_0': -45.4800000000004, 'blue_1': -36.380000000000294, 'red_0': -52.10000000000042}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 84.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 510000.0, 'blue_0': 510000.0, 'blue_1': 510000.0, 'red_0': 510000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.94000000000012, 'blue_policy': -36.380000000000294}, 'num_module_steps_sampled_lifetime': {'red_policy': 1020000.0, 'blue_policy': 1020000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003399114047994948, 'episode_return_mean': -161.90000000000123, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -144.30000000000098, 'episode_duration_sec_mean': 18.399928500014358, 'episode_return_min': -179.8000000000015, 'rlmodule_inference_timer': 0.01307516518585985, 'num_episodes_lifetime': 425.0, 'episode_len_min': 1200, 'time_between_sampling': 302.8026988999918, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.613411567361707, 'throughput_since_last_restore': 15.291666471163222}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 283
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 563
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1051
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1126
(MultiAgentEnvRunner pid=37492) {'red_0': -32.900000000000205, 'red_1': -34.10000000000039, 'blue_0': -47.00000000000043, 'blue_1': -36.20000000000021}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 651
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 658
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 667
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 807
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1119
(MultiAgentEnvRunner pid=37492) {'red_0': -26.200000000000152, 'red_1': -55.10000000000053, 'blue_0': -23.200000000000042, 'blue_1': -48.70000000000046}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 728
(MultiAgentEnvRunner pid=37492) {'red_0': -25.400000000000023, 'red_1': -13.899999999999984, 'blue_0': -31.800000000000317, 'blue_1': -30.10000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 431
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 621
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 622
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 815
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1179
(MultiAgentEnvRunner pid=37492) {'red_0': -37.20000000000031, 'red_1': -61.90000000000059, 'blue_0': -50.50000000000034, 'blue_1': -46.50000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 206
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 419
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 419
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 442
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 522
(MultiAgentEnvRunner pid=37492) {'red_0': -35.80000000000026, 'red_1': -40.00000000000026, 'blue_0': -25.00000000000007, 'blue_1': -23.400000000000045}
ITERATION 85: reward=-144.98000000000104, metadata={'num_env_steps_sampled_lifetime': 516000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003618655948574576, 'timers': {'connectors': {'batch_individual_items': 0.00010635650864320943, 'add_states_from_episodes_to_batch': 7.1304641994110795e-06, 'add_time_dim_to_batch_and_zero_pad': 1.374495381782899e-05, 'numpy_to_tensor': 7.640092712816317e-05, 'agent_to_module_mapping': 8.996233476971604e-06, 'add_observations_from_episodes_to_batch': 4.237337974478496e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009425956556888424, 'timers': {'connectors': {'get_actions': 0.0004786411031612068, 'un_batch_to_individual_items': 7.228423721754102e-05, 'tensor_to_numpy': 0.00012996868444548924, 'module_to_agent_unmapping': 7.155064627798871e-06, 'normalize_and_clip_actions': 7.756818735343149e-05, 'listify_data_for_vector_env': 2.6106444093049485e-05, 'remove_single_ts_time_rank_from_batch': 2.5507902879790564e-06}}}, 'sample': 129.3612540999893, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -41.00000000000035, 'blue_0': -35.50000000000024, 'blue_1': -36.98000000000026, 'red_0': -31.500000000000192}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 85.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 516000.0, 'blue_0': 516000.0, 'blue_1': 516000.0, 'red_0': 516000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -41.00000000000035, 'blue_policy': -36.98000000000026}, 'num_module_steps_sampled_lifetime': {'red_policy': 1032000.0, 'blue_policy': 1032000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036333792408407736, 'episode_return_mean': -144.98000000000104, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.20000000000056, 'episode_duration_sec_mean': 25.685608160006815, 'episode_return_min': -196.10000000000164, 'rlmodule_inference_timer': 0.014151408652108202, 'num_episodes_lifetime': 430.0, 'episode_len_min': 1200, 'time_between_sampling': 291.6871763999807, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.814950932523132, 'throughput_since_last_restore': 15.272682200084446}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -22.40000000000005, 'red_1': -19.300000000000004, 'blue_0': -48.3000000000004, 'blue_1': -48.30000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 258
(MultiAgentEnvRunner pid=37492) CAPTURED by blue_0 at STEP 843
(MultiAgentEnvRunner pid=37492) {'red_0': -30.70000000000003, 'red_1': -45.70000000000026, 'blue_0': 7.100000000000053, 'blue_1': -26.100000000000207}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -12.499999999999973, 'red_1': -8.699999999999985, 'blue_0': -39.80000000000029, 'blue_1': -38.10000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 283
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 285
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 285
(MultiAgentEnvRunner pid=37492) {'red_0': -16.599999999999955, 'red_1': -17.899999999999988, 'blue_0': -40.000000000000306, 'blue_1': -43.000000000000334}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 651
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 670
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 888
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1163
(MultiAgentEnvRunner pid=37492) {'red_0': -18.599999999999987, 'red_1': -52.40000000000035, 'blue_0': -12.900000000000169, 'blue_1': -31.100000000000293}
ITERATION 86: reward=-113.06000000000066, metadata={'num_env_steps_sampled_lifetime': 522000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0004927990685822935, 'timers': {'connectors': {'batch_individual_items': 0.00014777428354303515, 'add_states_from_episodes_to_batch': 1.046517584033011e-05, 'add_time_dim_to_batch_and_zero_pad': 1.8114591661425534e-05, 'numpy_to_tensor': 9.837399253650157e-05, 'agent_to_module_mapping': 1.1510815735091178e-05, 'add_observations_from_episodes_to_batch': 5.301672129320361e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0012264702702271186, 'timers': {'connectors': {'get_actions': 0.0006076462467034421, 'un_batch_to_individual_items': 9.502997162291656e-05, 'tensor_to_numpy': 0.0001678509434758383, 'module_to_agent_unmapping': 9.432726514713118e-06, 'normalize_and_clip_actions': 0.00010511800360798395, 'listify_data_for_vector_env': 3.573025160569825e-05, 'remove_single_ts_time_rank_from_batch': 3.753558482516401e-06}}}, 'sample': 128.60098029999062, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.800000000000114, 'blue_0': -26.78000000000022, 'blue_1': -37.3200000000003, 'red_0': -20.159999999999997}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 86.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 522000.0, 'blue_0': 522000.0, 'blue_1': 522000.0, 'red_0': 522000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.800000000000114, 'blue_policy': -37.3200000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 1044000.0, 'blue_policy': 1044000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0004889032855118882, 'episode_return_mean': -113.06000000000066, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -95.40000000000045, 'episode_duration_sec_mean': 25.346128159994258, 'episode_return_min': -138.30000000000086, 'rlmodule_inference_timer': 0.020060674462098468, 'num_episodes_lifetime': 435.0, 'episode_len_min': 1200, 'time_between_sampling': 304.94977309997194, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.389553134922588, 'throughput_since_last_restore': 15.261914915565859}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 307
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 436
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 864
(MultiAgentEnvRunner pid=37492) {'red_0': -50.9000000000005, 'red_1': -13.69999999999997, 'blue_0': -47.500000000000384, 'blue_1': -41.80000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 741
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 948
(MultiAgentEnvRunner pid=37492) {'red_0': -25.300000000000047, 'red_1': -42.60000000000024, 'blue_0': -36.60000000000032, 'blue_1': -27.500000000000266}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 497
(MultiAgentEnvRunner pid=37492) {'red_0': -42.10000000000032, 'red_1': -18.999999999999993, 'blue_0': -39.50000000000029, 'blue_1': -45.60000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 535
(MultiAgentEnvRunner pid=37492) {'red_0': -25.00000000000005, 'red_1': -37.10000000000018, 'blue_0': -31.300000000000203, 'blue_1': -40.200000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 843
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 913
(MultiAgentEnvRunner pid=37492) {'red_0': -53.400000000000325, 'red_1': -19.19999999999999, 'blue_0': -32.10000000000043, 'blue_1': -24.300000000000146}
ITERATION 87: reward=-138.9400000000009, metadata={'num_env_steps_sampled_lifetime': 528000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003486939938114552, 'timers': {'connectors': {'batch_individual_items': 9.736179836883378e-05, 'add_states_from_episodes_to_batch': 6.669820364040332e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3082347958964057e-05, 'numpy_to_tensor': 7.873952604085688e-05, 'agent_to_module_mapping': 8.660052067649298e-06, 'add_observations_from_episodes_to_batch': 3.979989448322015e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009106249453123542, 'timers': {'connectors': {'get_actions': 0.00047033890172799934, 'un_batch_to_individual_items': 6.925421653794762e-05, 'tensor_to_numpy': 0.0001251905540514701, 'module_to_agent_unmapping': 6.5885802377412895e-06, 'normalize_and_clip_actions': 7.392729667180707e-05, 'listify_data_for_vector_env': 2.4379217041972148e-05, 'remove_single_ts_time_rank_from_batch': 2.4489307808217755e-06}}}, 'sample': 93.54220850009006, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.32000000000007, 'blue_0': -37.400000000000325, 'blue_1': -35.88000000000029, 'red_0': -39.340000000000245}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 87.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 528000.0, 'blue_0': 528000.0, 'blue_1': 528000.0, 'red_0': 528000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.32000000000007, 'blue_policy': -35.88000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 1056000.0, 'blue_policy': 1056000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035350699630740025, 'episode_return_mean': -138.9400000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -129.00000000000088, 'episode_duration_sec_mean': 18.580991719989107, 'episode_return_min': -153.90000000000114, 'rlmodule_inference_timer': 0.01342948619956939, 'num_episodes_lifetime': 440.0, 'episode_len_min': 1200, 'time_between_sampling': 288.36896590003744, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.413126712875448, 'throughput_since_last_restore': 15.263615412872419}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 514
(MultiAgentEnvRunner pid=37492) {'red_0': -21.70000000000004, 'red_1': -18.499999999999996, 'blue_0': -39.800000000000324, 'blue_1': -44.40000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 453
(MultiAgentEnvRunner pid=37492) {'red_0': -68.80000000000025, 'red_1': -32.800000000000175, 'blue_0': -38.30000000000027, 'blue_1': -27.10000000000008}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 286
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 621
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 845
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 845
(MultiAgentEnvRunner pid=37492) {'red_0': -17.800000000000015, 'red_1': -49.50000000000037, 'blue_0': -33.70000000000027, 'blue_1': -38.40000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -30.10000000000015, 'red_1': -22.500000000000053, 'blue_0': -41.100000000000314, 'blue_1': -39.90000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -17.099999999999973, 'red_1': -32.20000000000019, 'blue_0': -50.200000000000436, 'blue_1': -56.60000000000051}
ITERATION 88: reward=-144.10000000000088, metadata={'num_env_steps_sampled_lifetime': 534000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033758964331199223, 'timers': {'connectors': {'batch_individual_items': 9.831697251235034e-05, 'add_states_from_episodes_to_batch': 7.007195520163361e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3818671695386961e-05, 'numpy_to_tensor': 7.017890715330644e-05, 'agent_to_module_mapping': 8.345383917977738e-06, 'add_observations_from_episodes_to_batch': 4.061467879963378e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008983099790814492, 'timers': {'connectors': {'get_actions': 0.00046136686837429917, 'un_batch_to_individual_items': 6.799634394026024e-05, 'tensor_to_numpy': 0.00012156664444115136, 'module_to_agent_unmapping': 6.629312657096897e-06, 'normalize_and_clip_actions': 7.371668011715759e-05, 'listify_data_for_vector_env': 2.4635955134711205e-05, 'remove_single_ts_time_rank_from_batch': 2.4814652407889053e-06}}}, 'sample': 95.83636980003212, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -31.100000000000158, 'blue_0': -40.620000000000324, 'blue_1': -41.28000000000031, 'red_0': -31.10000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 88.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 534000.0, 'blue_0': 534000.0, 'blue_1': 534000.0, 'red_0': 534000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -31.100000000000158, 'blue_policy': -41.28000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 1068000.0, 'blue_policy': 1068000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000345378066125995, 'episode_return_mean': -144.10000000000088, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -124.40000000000072, 'episode_duration_sec_mean': 19.006813639961184, 'episode_return_min': -167.00000000000077, 'rlmodule_inference_timer': 0.013685760717154124, 'num_episodes_lifetime': 445.0, 'episode_len_min': 1200, 'time_between_sampling': 295.7318432999309, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.760927301618404, 'throughput_since_last_restore': 15.257775641869406}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 845
(MultiAgentEnvRunner pid=37492) {'red_0': -35.10000000000016, 'red_1': -36.80000000000018, 'blue_0': -61.40000000000064, 'blue_1': -33.40000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 448
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 653
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 995
(MultiAgentEnvRunner pid=37492) {'red_0': -48.50000000000038, 'red_1': -24.400000000000034, 'blue_0': -25.100000000000055, 'blue_1': -30.300000000000086}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 492
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 524
(MultiAgentEnvRunner pid=37492) {'red_0': -25.900000000000095, 'red_1': -9.599999999999982, 'blue_0': -40.6000000000003, 'blue_1': -37.20000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 521
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 918
(MultiAgentEnvRunner pid=37492) {'red_0': -38.2000000000002, 'red_1': -30.20000000000013, 'blue_0': -43.1000000000004, 'blue_1': -33.80000000000021}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 714
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 748
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 982
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1168
(MultiAgentEnvRunner pid=37492) {'red_0': -43.20000000000028, 'red_1': -60.80000000000056, 'blue_0': -39.60000000000034, 'blue_1': -32.500000000000334}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.6, 'blue_0': -0.4, 'blue_1': -117.4999999999976}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -117.29999999999761, 'red_1': -117.69999999999759, 'blue_0': -117.09999999999762, 'blue_1': -117.19999999999762}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 01:05:49,533	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': -0.4, 'blue_0': -113.19999999999784, 'blue_1': -113.39999999999783}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.30000000000000004, 'blue_1': -0.6}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -7.899999999999988, 'red_1': -1.7000000000000004, 'blue_0': -2.400000000000001, 'blue_1': -3.900000000000002}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 89: reward=-145.94000000000102, metadata={'num_env_steps_sampled_lifetime': 540000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003715217570675331, 'timers': {'connectors': {'batch_individual_items': 0.00010914441668570197, 'add_states_from_episodes_to_batch': 7.2551616233725385e-06, 'add_time_dim_to_batch_and_zero_pad': 1.409009202703808e-05, 'numpy_to_tensor': 8.073891926679181e-05, 'agent_to_module_mapping': 1.0869893081627196e-05, 'add_observations_from_episodes_to_batch': 4.2414104430553165e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009949446199456714, 'timers': {'connectors': {'get_actions': 0.0005119480903086818, 'un_batch_to_individual_items': 7.385289504715954e-05, 'tensor_to_numpy': 0.00013835772929995813, 'module_to_agent_unmapping': 7.2893350667672126e-06, 'normalize_and_clip_actions': 8.089268007701732e-05, 'listify_data_for_vector_env': 2.688391432162684e-05, 'remove_single_ts_time_rank_from_batch': 2.5802867296923644e-06}}}, 'sample': 99.12022819998674, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.36000000000018, 'blue_0': -41.96000000000034, 'blue_1': -33.44000000000025, 'red_0': -38.18000000000022}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 89.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 540000.0, 'blue_0': 540000.0, 'blue_1': 540000.0, 'red_0': 540000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.36000000000018, 'blue_policy': -33.44000000000025}, 'num_module_steps_sampled_lifetime': {'red_policy': 1080000.0, 'blue_policy': 1080000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003849547493889199, 'episode_return_mean': -145.94000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -113.30000000000064, 'episode_duration_sec_mean': 19.687238819990306, 'episode_return_min': -176.10000000000153, 'rlmodule_inference_timer': 0.014464818721955566, 'num_episodes_lifetime': 450.0, 'episode_len_min': 1200, 'time_between_sampling': 310.65063599997666, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.531432346345817, 'throughput_since_last_restore': 15.220980143333701}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1126
(MultiAgentEnvRunner pid=37492) {'red_0': -36.600000000000165, 'red_1': -28.300000000000058, 'blue_0': -39.30000000000049, 'blue_1': -31.00000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 425
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1039
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1169
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1169
(MultiAgentEnvRunner pid=37492) {'red_0': -26.00000000000003, 'red_1': -19.399999999999967, 'blue_0': -40.70000000000035, 'blue_1': -39.70000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 181
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 803
(MultiAgentEnvRunner pid=37492) {'red_0': -25.40000000000016, 'red_1': -36.80000000000041, 'blue_0': -44.500000000000284, 'blue_1': -58.50000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -10.09999999999998, 'red_1': -23.700000000000067, 'blue_0': -32.20000000000019, 'blue_1': -42.400000000000325}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 460
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 914
(MultiAgentEnvRunner pid=37492) {'red_0': -20.800000000000043, 'red_1': -45.400000000000325, 'blue_0': -36.90000000000035, 'blue_1': -25.50000000000006}
ITERATION 90: reward=-132.64000000000084, metadata={'num_env_steps_sampled_lifetime': 546000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034467316446637654, 'timers': {'connectors': {'batch_individual_items': 0.00010386169547695997, 'add_states_from_episodes_to_batch': 8.001246469723388e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3160008869507623e-05, 'numpy_to_tensor': 7.07518775766396e-05, 'agent_to_module_mapping': 8.486559839915332e-06, 'add_observations_from_episodes_to_batch': 4.0496242957744506e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009133559131930023, 'timers': {'connectors': {'get_actions': 0.00047383499835075977, 'un_batch_to_individual_items': 6.807100505783228e-05, 'tensor_to_numpy': 0.00012394901067239717, 'module_to_agent_unmapping': 7.038502691722487e-06, 'normalize_and_clip_actions': 7.489370188673838e-05, 'listify_data_for_vector_env': 2.454904904045345e-05, 'remove_single_ts_time_rank_from_batch': 2.4358838924144882e-06}}}, 'sample': 96.37183990003541, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.720000000000162, 'blue_0': -38.720000000000326, 'blue_1': -39.42000000000028, 'red_0': -23.780000000000076}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 90.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 546000.0, 'blue_0': 546000.0, 'blue_1': 546000.0, 'red_0': 546000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.720000000000162, 'blue_policy': -39.42000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 1092000.0, 'blue_policy': 1092000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036440505747706964, 'episode_return_mean': -132.64000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.40000000000055, 'episode_duration_sec_mean': 19.140312919998543, 'episode_return_min': -165.20000000000132, 'rlmodule_inference_timer': 0.013588763431799149, 'num_episodes_lifetime': 455.0, 'episode_len_min': 1200, 'time_between_sampling': 379.67708950000815, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.690042642753198, 'throughput_since_last_restore': 15.225980222375576}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 382.36684579995926, 'restore_env_runners': 1.1200085282325745e-05, 'training_step': 382.36656570003834, 'env_runner_sampling_timer': 96.50233970000409, 'learner_update_timer': 285.8036301000975, 'synch_weights': 0.016723899985663593, 'synch_env_connectors': 0.0033331000013276935, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 546000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034467316446637654, 'timers': {'connectors': {'batch_individual_items': 0.00010386169547695997, 'add_states_from_episodes_to_batch': 8.001246469723388e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3160008869507623e-05, 'numpy_to_tensor': 7.07518775766396e-05, 'agent_to_module_mapping': 8.486559839915332e-06, 'add_observations_from_episodes_to_batch': 4.0496242957744506e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009133559131930023, 'timers': {'connectors': {'get_actions': 0.00047383499835075977, 'un_batch_to_individual_items': 6.807100505783228e-05, 'tensor_to_numpy': 0.00012394901067239717, 'module_to_agent_unmapping': 7.038502691722487e-06, 'normalize_and_clip_actions': 7.489370188673838e-05, 'listify_data_for_vector_env': 2.454904904045345e-05, 'remove_single_ts_time_rank_from_batch': 2.4358838924144882e-06}}}, 'sample': 96.37183990003541, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.720000000000162, 'blue_0': -38.720000000000326, 'blue_1': -39.42000000000028, 'red_0': -23.780000000000076}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 90.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 546000.0, 'blue_0': 546000.0, 'blue_1': 546000.0, 'red_0': 546000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.720000000000162, 'blue_policy': -39.42000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 1092000.0, 'blue_policy': 1092000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036440505747706964, 'episode_return_mean': -132.64000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.40000000000055, 'episode_duration_sec_mean': 19.140312919998543, 'episode_return_min': -165.20000000000132, 'rlmodule_inference_timer': 0.013588763431799149, 'num_episodes_lifetime': 455.0, 'episode_len_min': 1200, 'time_between_sampling': 379.67708950000815, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.690042642753198, 'throughput_since_last_restore': 15.225980222375576}}, 'learners': {'red_policy': {'policy_loss': -0.22796353697776794, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.008254677057266235, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 91.0, 'num_module_steps_trained_lifetime': 32789120.0, 'curr_entropy_coeff': 0.04181, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002454, 'vf_explained_var': -0.36735594272613525, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.747138977050781, 'entropy': 1.267439603805542, 'vf_loss_unclipped': 730.155517578125, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 942.2449663877362, 'throughput_since_last_restore': 914.3708570484584}}, 'blue_policy': {'weights_seq_no': 91.0, 'num_module_steps_trained_lifetime': 32789120.0, 'curr_entropy_coeff': 0.04181, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002454, 'vf_explained_var': 0.8492579460144043, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.8387398719787598, 'total_loss': 0.06095855310559273, 'entropy': 1.7372987270355225, 'policy_loss': -0.2258046567440033, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.33904051780700684, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.013508155941963196, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 942.2434899660226, 'throughput_since_last_restore': 914.3708589684969}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 7.00005330145359e-06, 'batch_individual_items': 0.9031269999686629, 'add_time_dim_to_batch_and_zero_pad': 2.5399960577487946e-05, 'numpy_to_tensor': 0.16407780000008643, 'add_observations_from_episodes_to_batch': 0.0002715999726206064, 'agent_to_module_mapping': 0.02168340003117919, 'add_one_ts_to_episodes_and_truncate': 0.17789549997542053, 'add_columns_from_episodes_to_train_batch': 0.49561649991665035, 'general_advantage_estimation': 13.730129799921997}}, 'connector_pipeline_timer': 15.493271799990907}, 'num_module_steps_trained_lifetime': 65578240.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 1536990000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 44167.62374494083, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 44167.62193160543, 'throughput_since_last_restore': 42861.13410283531}, 'num_module_steps_trained_throughput': 1884.4852014301393, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1884.4851836887779, 'throughput_since_last_restore': 1828.741720736729}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 546000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 91, 'trial_id': 'default', 'date': '2026-01-26_01-16-31', 'timestamp': 1769386591, 'time_this_iter_s': 382.38122749328613, 'time_total_s': 35838.3872961998, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 35838.3872961998, 'iterations_since_restore': 91, 'perf': {'cpu_util_percent': 16.956959706959708, 'ram_util_percent': 85.11941391941392}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.799999999999985, 'red_1': -7.099999999999991, 'blue_0': -45.60000000000037, 'blue_1': -35.600000000000236}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.599999999999989, 'red_1': -7.599999999999989, 'blue_0': -41.700000000000315, 'blue_1': -35.40000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 578
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 627
(MultiAgentEnvRunner pid=37492) {'red_0': -43.900000000000304, 'red_1': -12.199999999999985, 'blue_0': -57.10000000000057, 'blue_1': -31.100000000000147}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 515
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 720
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1051
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1183
(MultiAgentEnvRunner pid=37492) {'red_0': -33.10000000000017, 'red_1': -57.50000000000041, 'blue_0': -24.100000000000357, 'blue_1': -22.30000000000013}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 663
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 689
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1136
(MultiAgentEnvRunner pid=37492) {'red_0': -16.599999999999977, 'red_1': -52.500000000000384, 'blue_0': -28.800000000000338, 'blue_1': -44.70000000000045}
ITERATION 91: reward=-122.66000000000085, metadata={'num_env_steps_sampled_lifetime': 552000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003261451944672848, 'timers': {'connectors': {'batch_individual_items': 9.701020664943307e-05, 'add_states_from_episodes_to_batch': 6.39362149813792e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2369812360479276e-05, 'numpy_to_tensor': 6.742882043080091e-05, 'agent_to_module_mapping': 8.195699964373039e-06, 'add_observations_from_episodes_to_batch': 3.8686854784084763e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008798417166625447, 'timers': {'connectors': {'get_actions': 0.0004536735822701186, 'un_batch_to_individual_items': 6.667091155047744e-05, 'tensor_to_numpy': 0.00011861742100168507, 'module_to_agent_unmapping': 6.502995511996433e-06, 'normalize_and_clip_actions': 7.314919907503596e-05, 'listify_data_for_vector_env': 2.4104716317116396e-05, 'remove_single_ts_time_rank_from_batch': 2.385808616961571e-06}}}, 'sample': 90.49702709994745, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.380000000000155, 'blue_0': -39.46000000000039, 'blue_1': -33.820000000000235, 'red_0': -22.00000000000009}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 91.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 552000.0, 'blue_0': 552000.0, 'blue_1': 552000.0, 'red_0': 552000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.380000000000155, 'blue_policy': -33.820000000000235}, 'num_module_steps_sampled_lifetime': {'red_policy': 1104000.0, 'blue_policy': 1104000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003420491726083943, 'episode_return_mean': -122.66000000000085, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -92.30000000000052, 'episode_duration_sec_mean': 17.951865939982234, 'episode_return_min': -144.300000000001, 'rlmodule_inference_timer': 0.013054439400841932, 'num_episodes_lifetime': 460.0, 'episode_len_min': 1200, 'time_between_sampling': 286.13400930003263, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.196392539399184, 'throughput_since_last_restore': 15.244966346094449}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 802
(MultiAgentEnvRunner pid=37492) {'red_0': -12.599999999999989, 'red_1': -13.799999999999985, 'blue_0': -40.600000000000364, 'blue_1': -37.50000000000046}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 377
(MultiAgentEnvRunner pid=37492) {'red_0': -28.800000000000143, 'red_1': -36.20000000000024, 'blue_0': -65.6000000000005, 'blue_1': -39.9000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 232
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 430
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 497
(MultiAgentEnvRunner pid=37492) {'red_0': -31.500000000000163, 'red_1': -33.9000000000002, 'blue_0': -40.2000000000003, 'blue_1': -49.00000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1159
(MultiAgentEnvRunner pid=37492) {'red_0': -3.800000000000002, 'red_1': -12.599999999999975, 'blue_0': -33.30000000000023, 'blue_1': -41.200000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -16.29999999999996, 'red_1': -7.39999999999999, 'blue_0': -35.90000000000024, 'blue_1': -49.500000000000426}
ITERATION 92: reward=-125.92000000000083, metadata={'num_env_steps_sampled_lifetime': 558000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003123530585163187, 'timers': {'connectors': {'batch_individual_items': 9.286297676345376e-05, 'add_states_from_episodes_to_batch': 6.203704472134662e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1753838654530907e-05, 'numpy_to_tensor': 6.396220913922531e-05, 'agent_to_module_mapping': 7.915961021086065e-06, 'add_observations_from_episodes_to_batch': 3.689303977527754e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008455736895134062, 'timers': {'connectors': {'get_actions': 0.000438529772304008, 'un_batch_to_individual_items': 6.306748636854988e-05, 'tensor_to_numpy': 0.00011524561108860576, 'module_to_agent_unmapping': 6.240070293894672e-06, 'normalize_and_clip_actions': 6.979661324302123e-05, 'listify_data_for_vector_env': 2.26023477834123e-05, 'remove_single_ts_time_rank_from_batch': 2.2635784717059924e-06}}}, 'sample': 89.29717190004885, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -20.78000000000008, 'blue_0': -43.12000000000033, 'blue_1': -43.42000000000037, 'red_0': -18.60000000000005}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 92.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 558000.0, 'blue_0': 558000.0, 'blue_1': 558000.0, 'red_0': 558000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -20.78000000000008, 'blue_policy': -43.42000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 1116000.0, 'blue_policy': 1116000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032722370954653375, 'episode_return_mean': -125.92000000000083, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -90.90000000000052, 'episode_duration_sec_mean': 17.740355520020238, 'episode_return_min': -170.50000000000117, 'rlmodule_inference_timer': 0.012399239842683743, 'num_episodes_lifetime': 465.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3175992000615, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.317347828093702, 'throughput_since_last_restore': 15.264607663649375}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 469
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1001
(MultiAgentEnvRunner pid=37492) {'red_0': -33.10000000000013, 'red_1': -25.600000000000055, 'blue_0': -41.10000000000041, 'blue_1': -42.50000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 400
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1160
(MultiAgentEnvRunner pid=37492) {'red_0': -25.80000000000011, 'red_1': -58.100000000000485, 'blue_0': -44.10000000000034, 'blue_1': -31.500000000000135}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 951
(MultiAgentEnvRunner pid=37492) {'red_0': -3.0999999999999917, 'red_1': -25.90000000000031, 'blue_0': -43.300000000000274, 'blue_1': -52.700000000000394}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 618
(MultiAgentEnvRunner pid=37492) {'red_0': -56.00000000000045, 'red_1': -49.900000000000354, 'blue_0': -38.800000000000345, 'blue_1': -24.90000000000007}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 652
(MultiAgentEnvRunner pid=37492) {'red_0': -27.800000000000125, 'red_1': -46.300000000000374, 'blue_0': -51.70000000000046, 'blue_1': -43.00000000000036}
ITERATION 93: reward=-153.04000000000116, metadata={'num_env_steps_sampled_lifetime': 564000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003376465376592935, 'timers': {'connectors': {'batch_individual_items': 9.760738765176826e-05, 'add_states_from_episodes_to_batch': 6.721023778628803e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2940928212651392e-05, 'numpy_to_tensor': 7.063809422615731e-05, 'agent_to_module_mapping': 8.478274266720772e-06, 'add_observations_from_episodes_to_batch': 4.028257347274742e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009082219767441615, 'timers': {'connectors': {'get_actions': 0.00046645378491168505, 'un_batch_to_individual_items': 6.825210689067907e-05, 'tensor_to_numpy': 0.0001239556857053985, 'module_to_agent_unmapping': 6.875084784831763e-06, 'normalize_and_clip_actions': 7.506048719844221e-05, 'listify_data_for_vector_env': 2.5522767898102744e-05, 'remove_single_ts_time_rank_from_batch': 2.466766876473215e-06}}}, 'sample': 87.46554780006409, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -41.160000000000316, 'blue_0': -43.80000000000037, 'blue_1': -38.9200000000003, 'red_0': -29.16000000000016}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 93.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 564000.0, 'blue_0': 564000.0, 'blue_1': 564000.0, 'red_0': 564000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -41.160000000000316, 'blue_policy': -38.9200000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 1128000.0, 'blue_policy': 1128000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003521984438977045, 'episode_return_mean': -153.04000000000116, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -125.00000000000097, 'episode_duration_sec_mean': 17.34826104005333, 'episode_return_min': -169.60000000000122, 'rlmodule_inference_timer': 0.013481842661689414, 'num_episodes_lifetime': 470.0, 'episode_len_min': 1200, 'time_between_sampling': 257.17677620006725, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.383540460602106, 'throughput_since_last_restore': 15.284426595833404}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 560
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 565
(MultiAgentEnvRunner pid=37492) {'red_0': -17.899999999999988, 'red_1': -20.000000000000046, 'blue_0': -44.70000000000038, 'blue_1': -42.10000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 802
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 850
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 850
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1038
(MultiAgentEnvRunner pid=37492) {'red_0': -21.099999999999984, 'red_1': -30.40000000000008, 'blue_0': -33.90000000000045, 'blue_1': -24.80000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 333
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 338
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 889
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1117
(MultiAgentEnvRunner pid=37492) {'red_0': -38.90000000000013, 'red_1': -33.30000000000024, 'blue_0': -28.5000000000002, 'blue_1': -31.300000000000253}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 167
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 175
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 188
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 251
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 462
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 827
(MultiAgentEnvRunner pid=37492) {'red_0': -36.100000000000264, 'red_1': -59.000000000000576, 'blue_0': -40.80000000000027, 'blue_1': -18.899999999999956}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.699999999999982, 'red_1': -13.199999999999969, 'blue_0': -43.00000000000033, 'blue_1': -40.7000000000003}
ITERATION 94: reward=-125.66000000000079, metadata={'num_env_steps_sampled_lifetime': 570000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031254724522015, 'timers': {'connectors': {'batch_individual_items': 9.289778233853673e-05, 'add_states_from_episodes_to_batch': 6.1390754046550575e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1896776164391259e-05, 'numpy_to_tensor': 6.45909512625387e-05, 'agent_to_module_mapping': 7.872203632892575e-06, 'add_observations_from_episodes_to_batch': 3.7126790037759635e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008402469463096096, 'timers': {'connectors': {'get_actions': 0.0004309797717552172, 'un_batch_to_individual_items': 6.417043051621074e-05, 'tensor_to_numpy': 0.00011529620995527401, 'module_to_agent_unmapping': 6.1577429528226235e-06, 'normalize_and_clip_actions': 6.957992606083313e-05, 'listify_data_for_vector_env': 2.3142562714896984e-05, 'remove_single_ts_time_rank_from_batch': 2.2483716268710943e-06}}}, 'sample': 87.08891020005103, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -31.180000000000177, 'blue_0': -38.18000000000033, 'blue_1': -31.56000000000022, 'red_0': -24.740000000000066}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 94.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 570000.0, 'blue_0': 570000.0, 'blue_1': 570000.0, 'red_0': 570000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -31.180000000000177, 'blue_policy': -31.56000000000022}, 'num_module_steps_sampled_lifetime': {'red_policy': 1140000.0, 'blue_policy': 1140000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003261974230892872, 'episode_return_mean': -125.66000000000079, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -106.60000000000058, 'episode_duration_sec_mean': 17.30113082001917, 'episode_return_min': -154.80000000000106, 'rlmodule_inference_timer': 0.01236785045744354, 'num_episodes_lifetime': 475.0, 'episode_len_min': 1200, 'time_between_sampling': 257.6914459000109, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.450064024283616, 'throughput_since_last_restore': 15.304418612165158}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 851
(MultiAgentEnvRunner pid=37492) {'red_0': -34.2000000000002, 'red_1': -23.90000000000007, 'blue_0': -32.10000000000026, 'blue_1': -21.100000000000133}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 737
(MultiAgentEnvRunner pid=37492) {'red_0': -20.4, 'red_1': -15.899999999999977, 'blue_0': -43.20000000000053, 'blue_1': -40.50000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1004
(MultiAgentEnvRunner pid=37492) {'red_0': -18.40000000000001, 'red_1': -34.000000000000135, 'blue_0': -31.20000000000038, 'blue_1': -43.000000000000405}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.399999999999997, 'red_1': -7.699999999999989, 'blue_0': -48.80000000000041, 'blue_1': -44.20000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 416
(MultiAgentEnvRunner pid=37492) {'red_0': -11.899999999999991, 'red_1': -10.59999999999999, 'blue_0': -24.60000000000006, 'blue_1': -49.30000000000049}
ITERATION 95: reward=-112.08000000000075, metadata={'num_env_steps_sampled_lifetime': 576000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000319907950626096, 'timers': {'connectors': {'batch_individual_items': 9.587745988184875e-05, 'add_states_from_episodes_to_batch': 6.3450414305892105e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2084826040183237e-05, 'numpy_to_tensor': 6.57496377000001e-05, 'agent_to_module_mapping': 7.943396781430184e-06, 'add_observations_from_episodes_to_batch': 3.791551403358895e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008471482222861744, 'timers': {'connectors': {'get_actions': 0.0004337656838025921, 'un_batch_to_individual_items': 6.425582503018767e-05, 'tensor_to_numpy': 0.00011634864274325775, 'module_to_agent_unmapping': 6.711467906051631e-06, 'normalize_and_clip_actions': 6.986782443130845e-05, 'listify_data_for_vector_env': 2.3505542895113304e-05, 'remove_single_ts_time_rank_from_batch': 2.3103477512989906e-06}}}, 'sample': 87.13177900004666, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.420000000000034, 'blue_0': -35.980000000000324, 'blue_1': -39.62000000000035, 'red_0': -18.060000000000038}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 95.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 576000.0, 'blue_0': 576000.0, 'blue_1': 576000.0, 'red_0': 576000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.420000000000034, 'blue_policy': -39.62000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 1152000.0, 'blue_policy': 1152000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003387637954767554, 'episode_return_mean': -112.08000000000075, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.40000000000052, 'episode_duration_sec_mean': 17.303787020035088, 'episode_return_min': -126.60000000000093, 'rlmodule_inference_timer': 0.01239186873332306, 'num_episodes_lifetime': 480.0, 'episode_len_min': 1200, 'time_between_sampling': 256.7511214999249, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.37164233752843, 'throughput_since_last_restore': 15.323412342084906}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -30.60000000000017, 'red_1': -10.499999999999979, 'blue_0': -50.90000000000043, 'blue_1': -41.900000000000325}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.899999999999984, 'red_1': -1.9000000000000006, 'blue_0': -50.600000000000435, 'blue_1': -45.00000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 558
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 566
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 629
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 950
(MultiAgentEnvRunner pid=37492) {'red_0': -54.90000000000039, 'red_1': -25.100000000000065, 'blue_0': -37.30000000000037, 'blue_1': -21.700000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 768
(MultiAgentEnvRunner pid=37492) {'red_0': -56.80000000000073, 'red_1': -12.499999999999972, 'blue_0': -56.20000000000044, 'blue_1': -38.60000000000021}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1005
(MultiAgentEnvRunner pid=37492) {'red_0': -25.40000000000002, 'red_1': -25.500000000000025, 'blue_0': -39.300000000000345, 'blue_1': -28.200000000000337}
ITERATION 96: reward=-132.36000000000095, metadata={'num_env_steps_sampled_lifetime': 582000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031712183363150716, 'timers': {'connectors': {'batch_individual_items': 9.321425314618036e-05, 'add_states_from_episodes_to_batch': 6.286635018773789e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2136593195601559e-05, 'numpy_to_tensor': 6.529900513779366e-05, 'agent_to_module_mapping': 8.045479232417938e-06, 'add_observations_from_episodes_to_batch': 3.7720717890567404e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008442287975057629, 'timers': {'connectors': {'get_actions': 0.00043444143360510975, 'un_batch_to_individual_items': 6.290510549640402e-05, 'tensor_to_numpy': 0.00011459280857645743, 'module_to_agent_unmapping': 6.2270069980473915e-06, 'normalize_and_clip_actions': 7.054319739341102e-05, 'listify_data_for_vector_env': 2.338159113820825e-05, 'remove_single_ts_time_rank_from_batch': 2.3021171834354763e-06}}}, 'sample': 88.34168959991075, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -15.100000000000009, 'blue_0': -46.860000000000404, 'blue_1': -35.080000000000254, 'red_0': -35.320000000000256}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 96.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 582000.0, 'blue_0': 582000.0, 'blue_1': 582000.0, 'red_0': 582000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -15.100000000000009, 'blue_policy': -35.080000000000254}, 'num_module_steps_sampled_lifetime': {'red_policy': 1164000.0, 'blue_policy': 1164000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000334435343796159, 'episode_return_mean': -132.36000000000095, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -106.40000000000077, 'episode_duration_sec_mean': 17.530155139998534, 'episode_return_min': -164.10000000000133, 'rlmodule_inference_timer': 0.012436439080859221, 'num_episodes_lifetime': 485.0, 'episode_len_min': 1200, 'time_between_sampling': 258.2602581999963, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.339010500275908, 'throughput_since_last_restore': 15.341797105789581}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 646
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1083
(MultiAgentEnvRunner pid=37492) {'red_0': -20.800000000000043, 'red_1': -26.000000000000092, 'blue_0': -26.40000000000012, 'blue_1': -42.9000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 337
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1126
(MultiAgentEnvRunner pid=37492) {'red_0': -23.599999999999987, 'red_1': -26.90000000000004, 'blue_0': -24.600000000000286, 'blue_1': -38.900000000000354}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1013
(MultiAgentEnvRunner pid=37492) {'red_0': -38.10000000000026, 'red_1': -19.1, 'blue_0': -43.10000000000036, 'blue_1': -57.20000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 975
(MultiAgentEnvRunner pid=37492) {'red_0': -38.80000000000022, 'red_1': -33.5000000000002, 'blue_0': -31.70000000000025, 'blue_1': -33.700000000000415}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 489
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 590
(MultiAgentEnvRunner pid=37492) {'red_0': -27.20000000000013, 'red_1': -18.3, 'blue_0': -35.90000000000022, 'blue_1': -37.600000000000264}
ITERATION 97: reward=-128.8600000000008, metadata={'num_env_steps_sampled_lifetime': 588000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032027709079247586, 'timers': {'connectors': {'batch_individual_items': 9.729423107696458e-05, 'add_states_from_episodes_to_batch': 6.308781257925498e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2040069324050547e-05, 'numpy_to_tensor': 6.573823044336403e-05, 'agent_to_module_mapping': 8.009912683083065e-06, 'add_observations_from_episodes_to_batch': 3.747385248910301e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000842332528437553, 'timers': {'connectors': {'get_actions': 0.00043423913103293483, 'un_batch_to_individual_items': 6.227611005073106e-05, 'tensor_to_numpy': 0.00011490427156223239, 'module_to_agent_unmapping': 6.196825474222368e-06, 'normalize_and_clip_actions': 6.972391933300535e-05, 'listify_data_for_vector_env': 2.362738300157254e-05, 'remove_single_ts_time_rank_from_batch': 2.2840882043404487e-06}}}, 'sample': 87.56219009996857, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -24.76000000000007, 'blue_0': -32.340000000000245, 'blue_1': -42.060000000000386, 'red_0': -29.700000000000124}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 97.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 588000.0, 'blue_0': 588000.0, 'blue_1': 588000.0, 'red_0': 588000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -24.76000000000007, 'blue_policy': -42.060000000000386}, 'num_module_steps_sampled_lifetime': {'red_policy': 1176000.0, 'blue_policy': 1176000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003264536660988599, 'episode_return_mean': -128.8600000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -114.00000000000065, 'episode_duration_sec_mean': 17.393785720015877, 'episode_return_min': -157.50000000000114, 'rlmodule_inference_timer': 0.012379050567996625, 'num_episodes_lifetime': 490.0, 'episode_len_min': 1200, 'time_between_sampling': 257.6996087000007, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.380201205277523, 'throughput_since_last_restore': 15.360178722242303}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1035
(MultiAgentEnvRunner pid=37492) {'red_0': -50.90000000000037, 'red_1': -16.899999999999988, 'blue_0': -38.200000000000344, 'blue_1': -27.900000000000333}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 249
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 871
(MultiAgentEnvRunner pid=37492) {'red_0': -21.0, 'red_1': -26.400000000000038, 'blue_0': -39.10000000000047, 'blue_1': -43.10000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 319
(MultiAgentEnvRunner pid=37492) {'red_0': -17.29999999999999, 'red_1': -18.100000000000005, 'blue_0': -53.80000000000043, 'blue_1': -40.700000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 243
(MultiAgentEnvRunner pid=37492) {'red_0': -13.899999999999967, 'red_1': -32.200000000000195, 'blue_0': -45.70000000000037, 'blue_1': -44.50000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -4.100000000000001, 'red_1': -4.799999999999999, 'blue_0': -51.00000000000044, 'blue_1': -34.800000000000225}
ITERATION 98: reward=-124.88000000000083, metadata={'num_env_steps_sampled_lifetime': 594000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003116117821114707, 'timers': {'connectors': {'batch_individual_items': 9.211960640988561e-05, 'add_states_from_episodes_to_batch': 6.137508272491957e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1824919652626575e-05, 'numpy_to_tensor': 6.488430250906566e-05, 'agent_to_module_mapping': 7.660754202996356e-06, 'add_observations_from_episodes_to_batch': 3.69769787864591e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008296755951784026, 'timers': {'connectors': {'get_actions': 0.00042858006947910776, 'un_batch_to_individual_items': 6.187979572891343e-05, 'tensor_to_numpy': 0.00011326634417087295, 'module_to_agent_unmapping': 6.07634752057475e-06, 'normalize_and_clip_actions': 6.831775938299261e-05, 'listify_data_for_vector_env': 2.2771912682834703e-05, 'remove_single_ts_time_rank_from_batch': 2.246945664417166e-06}}}, 'sample': 87.67266109993216, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -19.680000000000042, 'blue_0': -45.560000000000414, 'blue_1': -38.20000000000033, 'red_0': -21.440000000000065}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 98.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 594000.0, 'blue_0': 594000.0, 'blue_1': 594000.0, 'red_0': 594000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -19.680000000000042, 'blue_policy': -38.20000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 1188000.0, 'blue_policy': 1188000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003261896135994169, 'episode_return_mean': -124.88000000000083, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -94.70000000000067, 'episode_duration_sec_mean': 17.418869879981504, 'episode_return_min': -136.3000000000009, 'rlmodule_inference_timer': 0.012303739507472006, 'num_episodes_lifetime': 495.0, 'episode_len_min': 1200, 'time_between_sampling': 257.6637043999508, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.395800078065122, 'throughput_since_last_restore': 15.378355055462043}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -35.80000000000023, 'red_1': -8.799999999999985, 'blue_0': -55.0000000000005, 'blue_1': -48.8000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 358
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 784
(MultiAgentEnvRunner pid=37492) {'red_0': -31.50000000000011, 'red_1': -23.200000000000067, 'blue_0': -31.900000000000276, 'blue_1': -35.6000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -3.4000000000000017, 'red_1': -2.700000000000001, 'blue_0': -60.800000000000566, 'blue_1': -42.80000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 183
(MultiAgentEnvRunner pid=37492) {'red_0': -56.60000000000052, 'red_1': -22.000000000000053, 'blue_0': -41.90000000000032, 'blue_1': -38.90000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 668
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1029
(MultiAgentEnvRunner pid=37492) {'red_0': -26.900000000000155, 'red_1': -33.50000000000024, 'blue_0': -37.60000000000031, 'blue_1': -30.600000000000154}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -7.19999999999999, 'red_1': -6.999999999999992, 'blue_0': -3.2000000000000015, 'blue_1': -3.700000000000002}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.99999999999746, 'red_1': -119.99999999999746, 'blue_0': -119.99999999999746, 'blue_1': -119.99999999999746}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 02:06:05,673	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.5, 'blue_1': -118.49999999999754}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -118.99999999999751, 'red_1': -118.69999999999753, 'blue_0': -118.89999999999752, 'blue_1': -118.99999999999751}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -2.1000000000000005, 'blue_0': -1.2, 'blue_1': -116.09999999999768}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 99: reward=-133.66000000000093, metadata={'num_env_steps_sampled_lifetime': 600000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031797445913637714, 'timers': {'connectors': {'batch_individual_items': 9.773640074505246e-05, 'add_states_from_episodes_to_batch': 6.1850352200013505e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1709295001315135e-05, 'numpy_to_tensor': 6.444099559909262e-05, 'agent_to_module_mapping': 7.85467190717133e-06, 'add_observations_from_episodes_to_batch': 3.754149661378088e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008395869520920528, 'timers': {'connectors': {'get_actions': 0.00043111700964380936, 'un_batch_to_individual_items': 6.307977781351411e-05, 'tensor_to_numpy': 0.0001150060580230143, 'module_to_agent_unmapping': 6.2577495670534556e-06, 'normalize_and_clip_actions': 6.997833420556989e-05, 'listify_data_for_vector_env': 2.318905411516278e-05, 'remove_single_ts_time_rank_from_batch': 2.253479738445822e-06}}}, 'sample': 88.06603869993705, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.04000000000007, 'blue_0': -45.440000000000396, 'blue_1': -39.340000000000295, 'red_0': -30.840000000000202}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 99.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 600000.0, 'blue_0': 600000.0, 'blue_1': 600000.0, 'red_0': 600000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.04000000000007, 'blue_policy': -39.340000000000295}, 'num_module_steps_sampled_lifetime': {'red_policy': 1200000.0, 'blue_policy': 1200000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032855378253297964, 'episode_return_mean': -133.66000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.7000000000009, 'episode_duration_sec_mean': 17.48677302002907, 'episode_return_min': -159.40000000000117, 'rlmodule_inference_timer': 0.01231348890382867, 'num_episodes_lifetime': 500.0, 'episode_len_min': 1200, 'time_between_sampling': 257.24050020007417, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.846208588738602, 'throughput_since_last_restore': 15.361355925425892}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 847
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 957
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1041
(MultiAgentEnvRunner pid=37492) {'red_0': -68.20000000000022, 'red_1': -59.10000000000042, 'blue_0': -21.900000000000336, 'blue_1': -26.40000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 630
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 660
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 828
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1129
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1163
(MultiAgentEnvRunner pid=37492) {'red_0': -22.800000000000132, 'red_1': -3.0999999999999903, 'blue_0': -50.20000000000039, 'blue_1': -76.10000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 415
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 426
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 426
(MultiAgentEnvRunner pid=37492) {'red_0': -35.400000000000226, 'red_1': -4.799999999999999, 'blue_0': -38.30000000000028, 'blue_1': -38.700000000000244}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 220
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 406
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 968
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 976
(MultiAgentEnvRunner pid=37492) {'red_0': -24.60000000000008, 'red_1': -40.20000000000036, 'blue_0': -33.30000000000018, 'blue_1': -53.400000000000475}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 996
(MultiAgentEnvRunner pid=37492) {'red_0': -13.499999999999986, 'red_1': -23.199999999999992, 'blue_0': -46.30000000000044, 'blue_1': -37.50000000000046}
ITERATION 100: reward=-143.40000000000094, metadata={'num_env_steps_sampled_lifetime': 606000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032684827339096836, 'timers': {'connectors': {'batch_individual_items': 9.624287811666261e-05, 'add_states_from_episodes_to_batch': 6.597353179526043e-06, 'add_time_dim_to_batch_and_zero_pad': 1.253304077060429e-05, 'numpy_to_tensor': 6.781925026238809e-05, 'agent_to_module_mapping': 8.107568943945793e-06, 'add_observations_from_episodes_to_batch': 3.849816534734872e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008669162253227967, 'timers': {'connectors': {'get_actions': 0.0004472417754426867, 'un_batch_to_individual_items': 6.40878725480533e-05, 'tensor_to_numpy': 0.00011867769039089218, 'module_to_agent_unmapping': 6.31954235955895e-06, 'normalize_and_clip_actions': 7.272159378495145e-05, 'listify_data_for_vector_env': 2.3832248325633888e-05, 'remove_single_ts_time_rank_from_batch': 2.2077771013250664e-06}}}, 'sample': 88.86947159992997, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.080000000000155, 'blue_0': -38.00000000000033, 'blue_1': -46.42000000000036, 'red_0': -32.90000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 100.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 606000.0, 'blue_0': 606000.0, 'blue_1': 606000.0, 'red_0': 606000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.080000000000155, 'blue_policy': -46.42000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 1212000.0, 'blue_policy': 1212000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003462109792582723, 'episode_return_mean': -143.40000000000094, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -117.20000000000076, 'episode_duration_sec_mean': 17.633414199994878, 'episode_return_min': -175.6000000000014, 'rlmodule_inference_timer': 0.012744336706503337, 'num_episodes_lifetime': 505.0, 'episode_len_min': 1200, 'time_between_sampling': 345.2686606000643, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.302846998102712, 'throughput_since_last_restore': 15.3784393220212}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 346.7261975000147, 'restore_env_runners': 7.900060154497623e-06, 'training_step': 346.72597969998606, 'env_runner_sampling_timer': 88.98850149998907, 'learner_update_timer': 257.6788758999901, 'synch_weights': 0.012227999977767467, 'synch_env_connectors': 0.002149500069208443, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 606000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032684827339096836, 'timers': {'connectors': {'batch_individual_items': 9.624287811666261e-05, 'add_states_from_episodes_to_batch': 6.597353179526043e-06, 'add_time_dim_to_batch_and_zero_pad': 1.253304077060429e-05, 'numpy_to_tensor': 6.781925026238809e-05, 'agent_to_module_mapping': 8.107568943945793e-06, 'add_observations_from_episodes_to_batch': 3.849816534734872e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008669162253227967, 'timers': {'connectors': {'get_actions': 0.0004472417754426867, 'un_batch_to_individual_items': 6.40878725480533e-05, 'tensor_to_numpy': 0.00011867769039089218, 'module_to_agent_unmapping': 6.31954235955895e-06, 'normalize_and_clip_actions': 7.272159378495145e-05, 'listify_data_for_vector_env': 2.3832248325633888e-05, 'remove_single_ts_time_rank_from_batch': 2.2077771013250664e-06}}}, 'sample': 88.86947159992997, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.080000000000155, 'blue_0': -38.00000000000033, 'blue_1': -46.42000000000036, 'red_0': -32.90000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 100.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 606000.0, 'blue_0': 606000.0, 'blue_1': 606000.0, 'red_0': 606000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.080000000000155, 'blue_policy': -46.42000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 1212000.0, 'blue_policy': 1212000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003462109792582723, 'episode_return_mean': -143.40000000000094, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -117.20000000000076, 'episode_duration_sec_mean': 17.633414199994878, 'episode_return_min': -175.6000000000014, 'rlmodule_inference_timer': 0.012744336706503337, 'num_episodes_lifetime': 505.0, 'episode_len_min': 1200, 'time_between_sampling': 345.2686606000643, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.302846998102712, 'throughput_since_last_restore': 15.3784393220212}}, 'learners': {'red_policy': {'policy_loss': 0.014859853312373161, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.008233955129981041, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 101.0, 'num_module_steps_trained_lifetime': 36392320.0, 'curr_entropy_coeff': 0.04091, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00023939999999999996, 'vf_explained_var': -0.26456642150878906, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.994667053222656, 'entropy': 1.1787691116333008, 'vf_loss_unclipped': 674.6543579101562, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1039.0987535224463, 'throughput_since_last_restore': 923.5265327374497}}, 'blue_policy': {'weights_seq_no': 101.0, 'num_module_steps_trained_lifetime': 36392320.0, 'curr_entropy_coeff': 0.04091, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00023939999999999996, 'vf_explained_var': 0.5718743801116943, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 3.6687183380126953, 'total_loss': 0.8860129714012146, 'entropy': 1.7265076637268066, 'policy_loss': -0.1550210416316986, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.090075969696045, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014317650347948074, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1039.097040383054, 'throughput_since_last_restore': 923.5265349849914}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 7.100054062902927e-06, 'batch_individual_items': 0.8161734999157488, 'add_time_dim_to_batch_and_zero_pad': 2.0999927073717117e-05, 'numpy_to_tensor': 0.12549689994193614, 'add_observations_from_episodes_to_batch': 0.0002815000480040908, 'agent_to_module_mapping': 0.021710099885240197, 'add_one_ts_to_episodes_and_truncate': 0.17784730007406324, 'add_columns_from_episodes_to_train_batch': 0.5154154000338167, 'general_advantage_estimation': 13.150776900001802}}, 'connector_pipeline_timer': 14.808115699910559}, 'num_module_steps_trained_lifetime': 72784640.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 1705890000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48707.631923748835, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48707.62952181621, 'throughput_since_last_restore': 43290.30643782835}, 'num_module_steps_trained_throughput': 2078.1921701567917, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2078.1921258080574, 'throughput_since_last_restore': 1847.0530735276125}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 606000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 101, 'trial_id': 'default', 'date': '2026-01-26_02-15-37', 'timestamp': 1769390137, 'time_this_iter_s': 346.7460412979126, 'time_total_s': 39384.11293935776, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 39384.11293935776, 'iterations_since_restore': 101, 'perf': {'cpu_util_percent': 14.432995951417004, 'ram_util_percent': 91.42246963562754}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 546
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 772
(MultiAgentEnvRunner pid=37492) {'red_0': -49.00000000000042, 'red_1': -52.50000000000039, 'blue_0': -22.300000000000022, 'blue_1': -40.90000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 475
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 840
(MultiAgentEnvRunner pid=37492) {'red_0': -51.300000000000445, 'red_1': -21.600000000000037, 'blue_0': -42.80000000000034, 'blue_1': -42.60000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.299999999999983, 'red_1': -6.999999999999991, 'blue_0': -44.10000000000035, 'blue_1': -47.800000000000395}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 373
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 432
(MultiAgentEnvRunner pid=37492) {'red_0': -11.199999999999983, 'red_1': -44.400000000000276, 'blue_0': -29.000000000000124, 'blue_1': -37.80000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 625
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1028
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1082
(MultiAgentEnvRunner pid=37492) {'red_0': -36.00000000000015, 'red_1': -36.00000000000013, 'blue_0': -43.2000000000004, 'blue_1': -22.30000000000026}
ITERATION 101: reward=-138.22000000000094, metadata={'num_env_steps_sampled_lifetime': 612000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003117488954306689, 'timers': {'connectors': {'batch_individual_items': 9.495265104863573e-05, 'add_states_from_episodes_to_batch': 6.148877667423752e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1617211454722995e-05, 'numpy_to_tensor': 6.361558550933937e-05, 'agent_to_module_mapping': 7.710043409255493e-06, 'add_observations_from_episodes_to_batch': 3.679968516839281e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008316560694692361, 'timers': {'connectors': {'get_actions': 0.00043040939581430216, 'un_batch_to_individual_items': 6.244115521900743e-05, 'tensor_to_numpy': 0.00011265125983727527, 'module_to_agent_unmapping': 6.119349708955785e-06, 'normalize_and_clip_actions': 6.822061935203416e-05, 'listify_data_for_vector_env': 2.2906721338171913e-05, 'remove_single_ts_time_rank_from_batch': 2.2481438381664476e-06}}}, 'sample': 88.95883710007183, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.30000000000016, 'blue_0': -36.28000000000024, 'blue_1': -38.28000000000032, 'red_0': -31.360000000000195}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 101.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 612000.0, 'blue_0': 612000.0, 'blue_1': 612000.0, 'red_0': 612000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.30000000000016, 'blue_policy': -38.28000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 1224000.0, 'blue_policy': 1224000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032868680132654626, 'episode_return_mean': -138.22000000000094, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.20000000000073, 'episode_duration_sec_mean': 17.673456680006346, 'episode_return_min': -164.70000000000118, 'rlmodule_inference_timer': 0.012235546833585046, 'num_episodes_lifetime': 510.0, 'episode_len_min': 1200, 'time_between_sampling': 257.9860841999762, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.309243632008574, 'throughput_since_last_restore': 15.395274187098806}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -18.29999999999999, 'red_1': -46.90000000000038, 'blue_0': -48.90000000000041, 'blue_1': -47.100000000000385}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -13.699999999999969, 'red_1': -4.899999999999999, 'blue_0': -47.00000000000039, 'blue_1': -45.70000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.699999999999982, 'red_1': -7.799999999999988, 'blue_0': -36.300000000000246, 'blue_1': -63.50000000000061}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -12.099999999999975, 'red_1': -11.699999999999976, 'blue_0': -47.900000000000404, 'blue_1': -42.300000000000324}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 494
(MultiAgentEnvRunner pid=37492) {'red_0': -31.800000000000114, 'red_1': -18.29999999999999, 'blue_0': -32.60000000000017, 'blue_1': -36.10000000000023}
ITERATION 102: reward=-124.5200000000008, metadata={'num_env_steps_sampled_lifetime': 618000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031410544104805156, 'timers': {'connectors': {'batch_individual_items': 9.277439928868902e-05, 'add_states_from_episodes_to_batch': 6.4649860632268675e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2112166733867785e-05, 'numpy_to_tensor': 6.453860210646527e-05, 'agent_to_module_mapping': 7.836601558577625e-06, 'add_observations_from_episodes_to_batch': 3.7432138287243964e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008524248133508601, 'timers': {'connectors': {'get_actions': 0.00044084815386229316, 'un_batch_to_individual_items': 6.393046414224494e-05, 'tensor_to_numpy': 0.00011597429354363361, 'module_to_agent_unmapping': 6.346888144718507e-06, 'normalize_and_clip_actions': 6.97941694162476e-05, 'listify_data_for_vector_env': 2.3300438552419132e-05, 'remove_single_ts_time_rank_from_batch': 2.294768185793383e-06}}}, 'sample': 88.1119562999811, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -17.920000000000066, 'blue_0': -42.54000000000032, 'blue_1': -46.94000000000038, 'red_0': -17.120000000000005}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 102.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 618000.0, 'blue_0': 618000.0, 'blue_1': 618000.0, 'red_0': 618000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -17.920000000000066, 'blue_policy': -46.94000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 1236000.0, 'blue_policy': 1236000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033355389025885025, 'episode_return_mean': -124.5200000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.30000000000075, 'episode_duration_sec_mean': 17.48900251998566, 'episode_return_min': -161.20000000000118, 'rlmodule_inference_timer': 0.012532204987866436, 'num_episodes_lifetime': 515.0, 'episode_len_min': 1200, 'time_between_sampling': 257.5919767000014, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.378547496588645, 'throughput_since_last_restore': 15.41234970705774}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 815
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1156
(MultiAgentEnvRunner pid=37492) {'red_0': -36.70000000000017, 'red_1': -25.10000000000009, 'blue_0': -39.20000000000035, 'blue_1': -21.100000000000193}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -19.900000000000013, 'red_1': -42.400000000000325, 'blue_0': -49.70000000000043, 'blue_1': -41.60000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 801
(MultiAgentEnvRunner pid=37492) {'red_0': -15.099999999999955, 'red_1': -20.10000000000002, 'blue_0': -41.1000000000003, 'blue_1': -47.4000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 551
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 568
(MultiAgentEnvRunner pid=37492) {'red_0': -20.800000000000026, 'red_1': -11.499999999999968, 'blue_0': -40.500000000000306, 'blue_1': -47.70000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 820
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1001
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1096
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1096
(MultiAgentEnvRunner pid=37492) {'red_0': -25.10000000000002, 'red_1': -35.80000000000018, 'blue_0': -22.40000000000032, 'blue_1': -44.20000000000041}
ITERATION 103: reward=-129.48000000000084, metadata={'num_env_steps_sampled_lifetime': 624000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000321332676613523, 'timers': {'connectors': {'batch_individual_items': 9.544711348676523e-05, 'add_states_from_episodes_to_batch': 6.301132442654425e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2148370117619786e-05, 'numpy_to_tensor': 6.611137108695026e-05, 'agent_to_module_mapping': 7.971347522893934e-06, 'add_observations_from_episodes_to_batch': 3.832605507450699e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008526009253454473, 'timers': {'connectors': {'get_actions': 0.00044006230978633966, 'un_batch_to_individual_items': 6.420376328182275e-05, 'tensor_to_numpy': 0.00011555765058661616, 'module_to_agent_unmapping': 6.394377346914543e-06, 'normalize_and_clip_actions': 7.051540620409034e-05, 'listify_data_for_vector_env': 2.3412969038144888e-05, 'remove_single_ts_time_rank_from_batch': 2.351852747933682e-06}}}, 'sample': 87.9223124999553, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.98000000000012, 'blue_0': -38.58000000000034, 'blue_1': -40.40000000000034, 'red_0': -23.52000000000004}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 103.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 624000.0, 'blue_0': 624000.0, 'blue_1': 624000.0, 'red_0': 624000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.98000000000012, 'blue_policy': -40.40000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 1248000.0, 'blue_policy': 1248000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003357005773528032, 'episode_return_mean': -129.48000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -120.50000000000068, 'episode_duration_sec_mean': 17.468191400007345, 'episode_return_min': -153.6000000000011, 'rlmodule_inference_timer': 0.012487293777635373, 'num_episodes_lifetime': 520.0, 'episode_len_min': 1200, 'time_between_sampling': 257.13858789997175, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.317427539366843, 'throughput_since_last_restore': 15.428668824480614}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 318
(MultiAgentEnvRunner pid=37492) {'red_0': -5.699999999999996, 'red_1': -8.099999999999987, 'blue_0': -38.50000000000028, 'blue_1': -44.400000000000354}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 514
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 544
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 588
(MultiAgentEnvRunner pid=37492) {'red_0': -23.400000000000084, 'red_1': -41.00000000000022, 'blue_0': -35.80000000000024, 'blue_1': -30.300000000000153}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 354
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 496
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 970
(MultiAgentEnvRunner pid=37492) {'red_0': -33.80000000000008, 'red_1': -70.00000000000013, 'blue_0': -31.300000000000317, 'blue_1': -9.399999999999917}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 251
(MultiAgentEnvRunner pid=37492) {'red_0': -26.100000000000104, 'red_1': -3.900000000000002, 'blue_0': -37.500000000000256, 'blue_1': -46.70000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.999999999999984, 'red_1': -15.199999999999964, 'blue_0': -39.20000000000028, 'blue_1': -36.40000000000025}
ITERATION 104: reward=-117.1400000000006, metadata={'num_env_steps_sampled_lifetime': 630000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003193044916114914, 'timers': {'connectors': {'batch_individual_items': 9.496933110732043e-05, 'add_states_from_episodes_to_batch': 6.308366700843326e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2080879583464972e-05, 'numpy_to_tensor': 6.551745839696916e-05, 'agent_to_module_mapping': 7.958858687598044e-06, 'add_observations_from_episodes_to_batch': 3.818963524481534e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008568915624010793, 'timers': {'connectors': {'get_actions': 0.0004413899213889424, 'un_batch_to_individual_items': 6.47532363595284e-05, 'tensor_to_numpy': 0.00011582918823447861, 'module_to_agent_unmapping': 6.346804715925984e-06, 'normalize_and_clip_actions': 7.114692364119212e-05, 'listify_data_for_vector_env': 2.318545326511541e-05, 'remove_single_ts_time_rank_from_batch': 2.35021551296892e-06}}}, 'sample': 87.14209360000677, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.64000000000006, 'blue_0': -36.46000000000028, 'blue_1': -33.44000000000021, 'red_0': -19.60000000000005}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 104.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 630000.0, 'blue_0': 630000.0, 'blue_1': 630000.0, 'red_0': 630000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.64000000000006, 'blue_policy': -33.44000000000021}, 'num_module_steps_sampled_lifetime': {'red_policy': 1260000.0, 'blue_policy': 1260000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032931165185756596, 'episode_return_mean': -117.1400000000006, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.70000000000061, 'episode_duration_sec_mean': 17.308258159994146, 'episode_return_min': -144.50000000000045, 'rlmodule_inference_timer': 0.012307370032024872, 'num_episodes_lifetime': 525.0, 'episode_len_min': 1200, 'time_between_sampling': 258.55088510003407, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.376947946601394, 'throughput_since_last_restore': 15.445160242056573}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 828
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 841
(MultiAgentEnvRunner pid=37492) {'red_0': -24.30000000000007, 'red_1': -35.000000000000256, 'blue_0': -52.500000000000455, 'blue_1': -51.40000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.799999999999988, 'red_1': -10.699999999999978, 'blue_0': -43.900000000000354, 'blue_1': -50.10000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 522
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 900
(MultiAgentEnvRunner pid=37492) {'red_0': -24.100000000000026, 'red_1': -3.9000000000000044, 'blue_0': -44.500000000000306, 'blue_1': -56.90000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.099999999999987, 'red_1': -10.399999999999979, 'blue_0': -47.5000000000004, 'blue_1': -41.10000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 388
(MultiAgentEnvRunner pid=37492) {'red_0': -10.699999999999996, 'red_1': -34.40000000000014, 'blue_0': -24.50000000000006, 'blue_1': -62.90000000000066}
ITERATION 105: reward=-128.94000000000088, metadata={'num_env_steps_sampled_lifetime': 636000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003198188906342746, 'timers': {'connectors': {'batch_individual_items': 9.802062493661792e-05, 'add_states_from_episodes_to_batch': 6.21749734682627e-06, 'add_time_dim_to_batch_and_zero_pad': 1.192221328940763e-05, 'numpy_to_tensor': 6.516085131215252e-05, 'agent_to_module_mapping': 7.877015314105692e-06, 'add_observations_from_episodes_to_batch': 3.7607555417063274e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008420953604540051, 'timers': {'connectors': {'get_actions': 0.00043716387590406853, 'un_batch_to_individual_items': 6.257002411213826e-05, 'tensor_to_numpy': 0.00011433912009191207, 'module_to_agent_unmapping': 6.117933059251033e-06, 'normalize_and_clip_actions': 6.889979488386858e-05, 'listify_data_for_vector_env': 2.2945643695599178e-05, 'remove_single_ts_time_rank_from_batch': 2.2736085521224537e-06}}}, 'sample': 87.74026190000586, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.88000000000007, 'blue_0': -42.58000000000031, 'blue_1': -52.48000000000046, 'red_0': -15.00000000000001}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 105.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 636000.0, 'blue_0': 636000.0, 'blue_1': 636000.0, 'red_0': 636000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.88000000000007, 'blue_policy': -52.48000000000046}, 'num_module_steps_sampled_lifetime': {'red_policy': 1272000.0, 'blue_policy': 1272000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033519460018667865, 'episode_return_mean': -128.94000000000088, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -107.10000000000068, 'episode_duration_sec_mean': 17.413810059987007, 'episode_return_min': -163.2000000000012, 'rlmodule_inference_timer': 0.012307217588885373, 'num_episodes_lifetime': 530.0, 'episode_len_min': 1200, 'time_between_sampling': 258.14427420007996, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.37659984091746, 'throughput_since_last_restore': 15.461372198615555}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 324
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1014
(MultiAgentEnvRunner pid=37492) {'red_0': -24.40000000000007, 'red_1': -15.899999999999993, 'blue_0': -29.70000000000012, 'blue_1': -60.40000000000064}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -19.200000000000006, 'red_1': -18.29999999999999, 'blue_0': -56.60000000000051, 'blue_1': -41.90000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 420
(MultiAgentEnvRunner pid=37492) {'red_0': -30.10000000000013, 'red_1': -17.299999999999958, 'blue_0': -46.200000000000394, 'blue_1': -41.90000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 630
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1031
(MultiAgentEnvRunner pid=37492) {'red_0': -16.90000000000001, 'red_1': -9.199999999999973, 'blue_0': -59.10000000000051, 'blue_1': -50.300000000000395}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 722
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 820
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 820
(MultiAgentEnvRunner pid=37492) {'red_0': -33.70000000000022, 'red_1': -68.30000000000035, 'blue_0': -34.2000000000003, 'blue_1': -54.30000000000053}
ITERATION 106: reward=-145.58000000000095, metadata={'num_env_steps_sampled_lifetime': 642000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003186983792291692, 'timers': {'connectors': {'batch_individual_items': 9.51487976920883e-05, 'add_states_from_episodes_to_batch': 6.396531165851565e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2100640124419552e-05, 'numpy_to_tensor': 6.535141930741356e-05, 'agent_to_module_mapping': 8.057419164176216e-06, 'add_observations_from_episodes_to_batch': 3.7408053802060556e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008575099185812064, 'timers': {'connectors': {'get_actions': 0.0004406961641106203, 'un_batch_to_individual_items': 6.46972307223066e-05, 'tensor_to_numpy': 0.00011801979518753683, 'module_to_agent_unmapping': 6.243364540089958e-06, 'normalize_and_clip_actions': 7.07180217310224e-05, 'listify_data_for_vector_env': 2.3333863131573174e-05, 'remove_single_ts_time_rank_from_batch': 2.3169183608945925e-06}}}, 'sample': 88.63660520000849, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.800000000000058, 'blue_0': -45.160000000000366, 'blue_1': -49.76000000000043, 'red_0': -24.86000000000009}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 106.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 642000.0, 'blue_0': 642000.0, 'blue_1': 642000.0, 'red_0': 642000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.800000000000058, 'blue_policy': -49.76000000000043}, 'num_module_steps_sampled_lifetime': {'red_policy': 1284000.0, 'blue_policy': 1284000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003339328400461084, 'episode_return_mean': -145.58000000000095, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -130.40000000000083, 'episode_duration_sec_mean': 17.606828880007377, 'episode_return_min': -190.5000000000014, 'rlmodule_inference_timer': 0.012308179054824423, 'num_episodes_lifetime': 535.0, 'episode_len_min': 1200, 'time_between_sampling': 257.55629540001974, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.322999369659335, 'throughput_since_last_restore': 15.47691564271062}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 442
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 744
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 913
(MultiAgentEnvRunner pid=37492) {'red_0': -46.500000000000306, 'red_1': -51.9000000000004, 'blue_0': -33.40000000000044, 'blue_1': -45.10000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 461
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 555
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 668
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 810
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 907
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 931
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1178
(MultiAgentEnvRunner pid=37492) {'red_0': -55.10000000000038, 'red_1': -57.70000000000045, 'blue_0': -46.600000000000506, 'blue_1': -26.50000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 372
(MultiAgentEnvRunner pid=37492) {'red_0': -20.300000000000036, 'red_1': -19.400000000000023, 'blue_0': -26.70000000000009, 'blue_1': -41.200000000000294}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -39.60000000000029, 'red_1': -2.700000000000001, 'blue_0': -70.70000000000023, 'blue_1': -46.400000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 773
(MultiAgentEnvRunner pid=37492) {'red_0': -49.900000000000404, 'red_1': -5.4999999999999964, 'blue_0': -46.20000000000038, 'blue_1': -34.200000000000244}
ITERATION 107: reward=-153.12000000000117, metadata={'num_env_steps_sampled_lifetime': 648000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031982083620672936, 'timers': {'connectors': {'batch_individual_items': 9.510697832416867e-05, 'add_states_from_episodes_to_batch': 6.349911642975005e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2075007440289828e-05, 'numpy_to_tensor': 6.634535052973241e-05, 'agent_to_module_mapping': 7.902383965053411e-06, 'add_observations_from_episodes_to_batch': 3.771499477425584e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008535877730447461, 'timers': {'connectors': {'get_actions': 0.0004399982393075185, 'un_batch_to_individual_items': 6.322203669334163e-05, 'tensor_to_numpy': 0.00011633371427217832, 'module_to_agent_unmapping': 6.280030876996181e-06, 'normalize_and_clip_actions': 7.101025866507204e-05, 'listify_data_for_vector_env': 2.3534517481635673e-05, 'remove_single_ts_time_rank_from_batch': 2.2962355911980873e-06}}}, 'sample': 87.37847209989559, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.440000000000175, 'blue_0': -44.720000000000326, 'blue_1': -38.68000000000037, 'red_0': -42.280000000000285}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 107.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 648000.0, 'blue_0': 648000.0, 'blue_1': 648000.0, 'red_0': 648000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.440000000000175, 'blue_policy': -38.68000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 1296000.0, 'blue_policy': 1296000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003370120433529309, 'episode_return_mean': -153.12000000000117, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -107.60000000000045, 'episode_duration_sec_mean': 17.335952579975128, 'episode_return_min': -185.90000000000185, 'rlmodule_inference_timer': 0.012452362879863238, 'num_episodes_lifetime': 540.0, 'episode_len_min': 1200, 'time_between_sampling': 257.72642419999465, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.36819025177794, 'throughput_since_last_restore': 15.4925355417674}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.599999999999996, 'red_1': -3.900000000000002, 'blue_0': -35.600000000000236, 'blue_1': -44.10000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1100
(MultiAgentEnvRunner pid=37492) {'red_0': -34.30000000000022, 'red_1': -19.50000000000004, 'blue_0': -59.200000000000564, 'blue_1': -45.30000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 362
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 476
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1024
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1160
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1189
(MultiAgentEnvRunner pid=37492) {'red_0': -24.800000000000086, 'red_1': -34.70000000000017, 'blue_0': -44.60000000000043, 'blue_1': -22.50000000000006}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 509
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 532
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 642
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 856
(MultiAgentEnvRunner pid=37492) {'red_0': -47.60000000000035, 'red_1': -54.90000000000034, 'blue_0': -28.800000000000217, 'blue_1': -31.40000000000015}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 656
(MultiAgentEnvRunner pid=37492) {'red_0': -12.699999999999989, 'red_1': -18.999999999999996, 'blue_0': -33.70000000000023, 'blue_1': -48.00000000000047}
ITERATION 108: reward=-130.04000000000084, metadata={'num_env_steps_sampled_lifetime': 654000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032504672652511346, 'timers': {'connectors': {'batch_individual_items': 9.575943261817535e-05, 'add_states_from_episodes_to_batch': 6.480567524391045e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2826375278038822e-05, 'numpy_to_tensor': 6.765382151624259e-05, 'agent_to_module_mapping': 8.083315598919201e-06, 'add_observations_from_episodes_to_batch': 3.814970049307389e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008671743008638028, 'timers': {'connectors': {'get_actions': 0.0004472506267564475, 'un_batch_to_individual_items': 6.415895798622342e-05, 'tensor_to_numpy': 0.00011705283119273255, 'module_to_agent_unmapping': 6.3708036238209005e-06, 'normalize_and_clip_actions': 7.200296630890292e-05, 'listify_data_for_vector_env': 2.381053338956674e-05, 'remove_single_ts_time_rank_from_batch': 2.352573436701761e-06}}}, 'sample': 89.08319160004612, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.40000000000011, 'blue_0': -40.38000000000034, 'blue_1': -38.260000000000275, 'red_0': -25.000000000000128}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 108.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 654000.0, 'blue_0': 654000.0, 'blue_1': 654000.0, 'red_0': 654000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.40000000000011, 'blue_policy': -38.260000000000275}, 'num_module_steps_sampled_lifetime': {'red_policy': 1308000.0, 'blue_policy': 1308000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003427325647094806, 'episode_return_mean': -130.04000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -89.20000000000059, 'episode_duration_sec_mean': 17.69384039998986, 'episode_return_min': -162.70000000000104, 'rlmodule_inference_timer': 0.01273412647667872, 'num_episodes_lifetime': 545.0, 'episode_len_min': 1200, 'time_between_sampling': 258.0855945000658, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.293311128634876, 'throughput_since_last_restore': 15.507349227415968}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 542
(MultiAgentEnvRunner pid=37492) {'red_0': -30.90000000000019, 'red_1': -12.799999999999972, 'blue_0': -51.100000000000435, 'blue_1': -44.900000000000354}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 504
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 662
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 728
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 728
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1101
(MultiAgentEnvRunner pid=37492) {'red_0': -27.80000000000021, 'red_1': -18.20000000000021, 'blue_0': -43.50000000000024, 'blue_1': -48.200000000000365}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.999999999999988, 'red_1': -47.000000000000384, 'blue_0': -46.80000000000039, 'blue_1': -48.3000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 653
(MultiAgentEnvRunner pid=37492) {'red_0': -29.600000000000083, 'red_1': -17.099999999999987, 'blue_0': -31.30000000000023, 'blue_1': -23.90000000000006}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 566
(MultiAgentEnvRunner pid=37492) {'red_0': -12.299999999999974, 'red_1': -19.100000000000033, 'blue_0': -44.00000000000035, 'blue_1': -52.30000000000045}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -6.699999999999992, 'blue_0': -61.4000000000006, 'blue_1': -5.999999999999995}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) PICKED UP by blue_1 at STEP 157
(MultiAgentEnvRunner pid=41856) {'red_0': -5.1, 'red_1': -15.699999999999978, 'blue_0': -0.8999999999999954, 'blue_1': -92.49999999999899}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 03:05:13,066	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -119.89999999999746, 'red_1': 0, 'blue_0': -0.2, 'blue_1': 0}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.6, 'red_1': -115.09999999999773, 'blue_0': -113.39999999999783, 'blue_1': -114.19999999999779}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -52.00000000000047, 'blue_0': -0.1, 'blue_1': 0}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 109: reward=-131.42000000000087, metadata={'num_env_steps_sampled_lifetime': 660000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000312602317184727, 'timers': {'connectors': {'batch_individual_items': 9.348391557542147e-05, 'add_states_from_episodes_to_batch': 6.110343986564466e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2115326922237887e-05, 'numpy_to_tensor': 6.508120997738721e-05, 'agent_to_module_mapping': 7.700617104438043e-06, 'add_observations_from_episodes_to_batch': 3.6876958368038165e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008455619907712894, 'timers': {'connectors': {'get_actions': 0.00043733436952407974, 'un_batch_to_individual_items': 6.29158324600157e-05, 'tensor_to_numpy': 0.00011532443322460587, 'module_to_agent_unmapping': 6.206789956973687e-06, 'normalize_and_clip_actions': 6.969941798443775e-05, 'listify_data_for_vector_env': 2.2978607738942398e-05, 'remove_single_ts_time_rank_from_batch': 2.2481965141429497e-06}}}, 'sample': 87.5704359000083, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.84000000000012, 'blue_0': -43.34000000000033, 'blue_1': -43.52000000000032, 'red_0': -21.720000000000088}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 109.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 660000.0, 'blue_0': 660000.0, 'blue_1': 660000.0, 'red_0': 660000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.84000000000012, 'blue_policy': -43.52000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 1320000.0, 'blue_policy': 1320000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003196961339089322, 'episode_return_mean': -131.42000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.90000000000038, 'episode_duration_sec_mean': 17.38863788000308, 'episode_return_min': -150.10000000000116, 'rlmodule_inference_timer': 0.012267164893333439, 'num_episodes_lifetime': 550.0, 'episode_len_min': 1200, 'time_between_sampling': 257.874231499969, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.673218310912235, 'throughput_since_last_restore': 15.488460874922646}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 833
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1155
(MultiAgentEnvRunner pid=37492) {'red_0': -28.000000000000142, 'red_1': -37.000000000000234, 'blue_0': -33.50000000000025, 'blue_1': -36.500000000000426}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 315
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 386
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 440
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 504
(MultiAgentEnvRunner pid=37492) {'red_0': -33.60000000000014, 'red_1': -41.50000000000034, 'blue_0': -37.30000000000022, 'blue_1': -38.60000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 999
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1126
(MultiAgentEnvRunner pid=37492) {'red_0': -17.49999999999999, 'red_1': -32.00000000000011, 'blue_0': -20.700000000000266, 'blue_1': -36.90000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 968
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1019
(MultiAgentEnvRunner pid=37492) {'red_0': -59.600000000000485, 'red_1': -62.300000000000516, 'blue_0': -49.900000000000496, 'blue_1': -22.50000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.29999999999999, 'red_1': -17.499999999999982, 'blue_0': -46.10000000000038, 'blue_1': -35.80000000000024}
ITERATION 110: reward=-138.82000000000102, metadata={'num_env_steps_sampled_lifetime': 666000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000317716886832709, 'timers': {'connectors': {'batch_individual_items': 9.543525368513809e-05, 'add_states_from_episodes_to_batch': 6.244625980111456e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1808141654154628e-05, 'numpy_to_tensor': 6.5402953220039e-05, 'agent_to_module_mapping': 7.806150296992289e-06, 'add_observations_from_episodes_to_batch': 3.764715749041969e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008453007145446809, 'timers': {'connectors': {'get_actions': 0.0004354152597331787, 'un_batch_to_individual_items': 6.386871440966324e-05, 'tensor_to_numpy': 0.0001150578699536869, 'module_to_agent_unmapping': 6.178331601204176e-06, 'normalize_and_clip_actions': 6.97088014839324e-05, 'listify_data_for_vector_env': 2.3254588817449495e-05, 'remove_single_ts_time_rank_from_batch': 2.2718356954303334e-06}}}, 'sample': 87.34453250002116, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -38.06000000000024, 'blue_0': -37.50000000000032, 'blue_1': -34.0600000000003, 'red_0': -29.20000000000015}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 110.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 666000.0, 'blue_0': 666000.0, 'blue_1': 666000.0, 'red_0': 666000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -38.06000000000024, 'blue_policy': -34.0600000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 1332000.0, 'blue_policy': 1332000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033707336677586165, 'episode_return_mean': -138.82000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -106.70000000000059, 'episode_duration_sec_mean': 17.328654799982907, 'episode_return_min': -194.30000000000177, 'rlmodule_inference_timer': 0.012330920693518585, 'num_episodes_lifetime': 555.0, 'episode_len_min': 1200, 'time_between_sampling': 351.24398629995994, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.42679092628985, 'throughput_since_last_restore': 15.503995735581773}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 344.2623604000546, 'restore_env_runners': 7.70005863159895e-06, 'training_step': 344.2621381999925, 'env_runner_sampling_timer': 87.46121810004115, 'learner_update_timer': 256.7461406000657, 'synch_weights': 0.012567800004035234, 'synch_env_connectors': 0.0021373999770730734, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 666000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000317716886832709, 'timers': {'connectors': {'batch_individual_items': 9.543525368513809e-05, 'add_states_from_episodes_to_batch': 6.244625980111456e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1808141654154628e-05, 'numpy_to_tensor': 6.5402953220039e-05, 'agent_to_module_mapping': 7.806150296992289e-06, 'add_observations_from_episodes_to_batch': 3.764715749041969e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008453007145446809, 'timers': {'connectors': {'get_actions': 0.0004354152597331787, 'un_batch_to_individual_items': 6.386871440966324e-05, 'tensor_to_numpy': 0.0001150578699536869, 'module_to_agent_unmapping': 6.178331601204176e-06, 'normalize_and_clip_actions': 6.97088014839324e-05, 'listify_data_for_vector_env': 2.3254588817449495e-05, 'remove_single_ts_time_rank_from_batch': 2.2718356954303334e-06}}}, 'sample': 87.34453250002116, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -38.06000000000024, 'blue_0': -37.50000000000032, 'blue_1': -34.0600000000003, 'red_0': -29.20000000000015}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 110.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 666000.0, 'blue_0': 666000.0, 'blue_1': 666000.0, 'red_0': 666000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -38.06000000000024, 'blue_policy': -34.0600000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 1332000.0, 'blue_policy': 1332000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033707336677586165, 'episode_return_mean': -138.82000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -106.70000000000059, 'episode_duration_sec_mean': 17.328654799982907, 'episode_return_min': -194.30000000000177, 'rlmodule_inference_timer': 0.012330920693518585, 'num_episodes_lifetime': 555.0, 'episode_len_min': 1200, 'time_between_sampling': 351.24398629995994, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.42679092628985, 'throughput_since_last_restore': 15.503995735581773}}, 'learners': {'red_policy': {'policy_loss': -0.1811920553445816, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.006679505575448275, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 111.0, 'num_module_steps_trained_lifetime': 39995520.0, 'curr_entropy_coeff': 0.040010000000000004, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00023339999999999998, 'vf_explained_var': -0.8166325092315674, 'curr_kl_coeff': 3.4171876907348633, 'total_loss': 9.800271034240723, 'entropy': 1.0314784049987793, 'vf_loss_unclipped': 676.6475830078125, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1046.5408578462796, 'throughput_since_last_restore': 931.0666146936185}}, 'blue_policy': {'weights_seq_no': 111.0, 'num_module_steps_trained_lifetime': 39995520.0, 'curr_entropy_coeff': 0.040010000000000004, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00023339999999999998, 'vf_explained_var': 0.3662729859352112, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 6.729964733123779, 'total_loss': 1.3684216737747192, 'entropy': 1.7240264415740967, 'policy_loss': -0.3584100306034088, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.7738993167877197, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014529048465192318, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1046.5394240436244, 'throughput_since_last_restore': 931.0666167917161}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.700051017105579e-06, 'batch_individual_items': 0.7393274999922141, 'add_time_dim_to_batch_and_zero_pad': 2.0099920220673084e-05, 'numpy_to_tensor': 0.13870130002032965, 'add_observations_from_episodes_to_batch': 0.000282999943010509, 'agent_to_module_mapping': 0.020084300078451633, 'add_one_ts_to_episodes_and_truncate': 0.1656112999189645, 'add_columns_from_episodes_to_train_batch': 0.45878650003578514, 'general_advantage_estimation': 13.069726299960166}}, 'connector_pipeline_timer': 14.592960199923255}, 'num_module_steps_trained_lifetime': 79991040.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 1874790000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 49056.50378528039, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 49056.50110659333, 'throughput_since_last_restore': 43643.74775812309}, 'num_module_steps_trained_throughput': 2093.07735075928, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2093.0773033412383, 'throughput_since_last_restore': 1862.1332366785568}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 666000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 111, 'trial_id': 'default', 'date': '2026-01-26_03-14-48', 'timestamp': 1769393688, 'time_this_iter_s': 344.2766981124878, 'time_total_s': 42934.65669441223, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 42934.65669441223, 'iterations_since_restore': 111, 'perf': {'cpu_util_percent': 14.777800407331975, 'ram_util_percent': 90.92769857433808}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1090
(MultiAgentEnvRunner pid=37492) {'red_0': -13.099999999999987, 'red_1': -17.599999999999998, 'blue_0': -32.80000000000027, 'blue_1': -27.800000000000338}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.099999999999994, 'red_1': -5.1999999999999975, 'blue_0': -45.20000000000036, 'blue_1': -46.90000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1030
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1085
(MultiAgentEnvRunner pid=37492) {'red_0': -51.80000000000038, 'red_1': -40.40000000000025, 'blue_0': -26.500000000000313, 'blue_1': -50.5000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 366
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 550
(MultiAgentEnvRunner pid=37492) {'red_0': -30.8000000000001, 'red_1': -26.000000000000064, 'blue_0': -51.80000000000052, 'blue_1': -26.700000000000063}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 950
(MultiAgentEnvRunner pid=37492) {'red_0': -4.6, 'red_1': -13.199999999999962, 'blue_0': -40.7000000000003, 'blue_1': -42.50000000000032}
ITERATION 111: reward=-120.04000000000083, metadata={'num_env_steps_sampled_lifetime': 672000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031286406499396897, 'timers': {'connectors': {'batch_individual_items': 9.31253660331336e-05, 'add_states_from_episodes_to_batch': 6.140133757127516e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1846517813587559e-05, 'numpy_to_tensor': 6.499869680564196e-05, 'agent_to_module_mapping': 7.686188813511567e-06, 'add_observations_from_episodes_to_batch': 3.6989734028564806e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008379987942073164, 'timers': {'connectors': {'get_actions': 0.00043001789503807565, 'un_batch_to_individual_items': 6.318273514277305e-05, 'tensor_to_numpy': 0.00011403382491056315, 'module_to_agent_unmapping': 6.412342449236735e-06, 'normalize_and_clip_actions': 7.000506789570843e-05, 'listify_data_for_vector_env': 2.3272162814619873e-05, 'remove_single_ts_time_rank_from_batch': 2.285213038605097e-06}}}, 'sample': 88.04523100005463, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -20.480000000000054, 'blue_0': -39.400000000000354, 'blue_1': -38.88000000000032, 'red_0': -21.280000000000094}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 111.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 672000.0, 'blue_0': 672000.0, 'blue_1': 672000.0, 'red_0': 672000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -20.480000000000054, 'blue_policy': -38.88000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 1344000.0, 'blue_policy': 1344000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032786338114743416, 'episode_return_mean': -120.04000000000083, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -91.3000000000006, 'episode_duration_sec_mean': 17.47724819998257, 'episode_return_min': -169.20000000000144, 'rlmodule_inference_timer': 0.012182190947992395, 'num_episodes_lifetime': 560.0, 'episode_len_min': 1200, 'time_between_sampling': 257.0445808999939, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.302124089873768, 'throughput_since_last_restore': 15.518394478736317}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 439
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1017
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1136
(MultiAgentEnvRunner pid=37492) {'red_0': -18.40000000000001, 'red_1': -49.100000000000364, 'blue_0': -24.300000000000075, 'blue_1': -46.50000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 650
(MultiAgentEnvRunner pid=37492) {'red_0': -2.0000000000000018, 'red_1': -20.300000000000022, 'blue_0': -58.000000000000526, 'blue_1': -57.10000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1089
(MultiAgentEnvRunner pid=37492) {'red_0': -24.70000000000008, 'red_1': -44.400000000000276, 'blue_0': -24.10000000000028, 'blue_1': -53.100000000000534}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.699999999999982, 'red_1': -7.499999999999989, 'blue_0': -52.00000000000046, 'blue_1': -53.300000000000466}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 995
(MultiAgentEnvRunner pid=37492) {'red_0': -39.40000000000031, 'red_1': -25.600000000000097, 'blue_0': -44.30000000000034, 'blue_1': -47.7000000000004}
ITERATION 112: reward=-140.30000000000103, metadata={'num_env_steps_sampled_lifetime': 678000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003177789039503659, 'timers': {'connectors': {'batch_individual_items': 9.525869205676515e-05, 'add_states_from_episodes_to_batch': 6.405723096108309e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2021376579646795e-05, 'numpy_to_tensor': 6.51403850028464e-05, 'agent_to_module_mapping': 7.883737117514735e-06, 'add_observations_from_episodes_to_batch': 3.7509982652545376e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008474026981197733, 'timers': {'connectors': {'get_actions': 0.00043748179399017914, 'un_batch_to_individual_items': 6.302439924502725e-05, 'tensor_to_numpy': 0.00011563693808562044, 'module_to_agent_unmapping': 6.1836553229502745e-06, 'normalize_and_clip_actions': 6.964416217475741e-05, 'listify_data_for_vector_env': 2.3141321087757078e-05, 'remove_single_ts_time_rank_from_batch': 2.288481739235228e-06}}}, 'sample': 87.18658430001233, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.380000000000148, 'blue_0': -40.54000000000034, 'blue_1': -51.54000000000046, 'red_0': -18.840000000000078}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 112.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 678000.0, 'blue_0': 678000.0, 'blue_1': 678000.0, 'red_0': 678000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.380000000000148, 'blue_policy': -51.54000000000046}, 'num_module_steps_sampled_lifetime': {'red_policy': 1356000.0, 'blue_policy': 1356000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033232355809676876, 'episode_return_mean': -140.30000000000103, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -122.5000000000009, 'episode_duration_sec_mean': 17.317774039995857, 'episode_return_min': -157.00000000000114, 'rlmodule_inference_timer': 0.012338055721488753, 'num_episodes_lifetime': 565.0, 'episode_len_min': 1200, 'time_between_sampling': 258.64370370004326, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.373360717870373, 'throughput_since_last_restore': 15.533070482766567}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1042
(MultiAgentEnvRunner pid=37492) {'red_0': -57.60000000000047, 'red_1': -14.699999999999982, 'blue_0': -51.60000000000051, 'blue_1': -30.700000000000372}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 511
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 594
(MultiAgentEnvRunner pid=37492) {'red_0': -1.9000000000000024, 'red_1': -35.20000000000029, 'blue_0': -65.10000000000049, 'blue_1': -55.40000000000046}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 823
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 949
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1182
(MultiAgentEnvRunner pid=37492) {'red_0': -29.500000000000327, 'red_1': -10.999999999999961, 'blue_0': -53.60000000000042, 'blue_1': -45.50000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 490
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1160
(MultiAgentEnvRunner pid=37492) {'red_0': -29.000000000000124, 'red_1': -20.50000000000002, 'blue_0': -44.30000000000037, 'blue_1': -57.30000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -33.0000000000002, 'red_1': -26.200000000000106, 'blue_0': -44.80000000000036, 'blue_1': -38.400000000000276}
ITERATION 113: reward=-149.06000000000114, metadata={'num_env_steps_sampled_lifetime': 684000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035136370693481206, 'timers': {'connectors': {'batch_individual_items': 9.886547537260952e-05, 'add_states_from_episodes_to_batch': 7.219942353322623e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3648864165769364e-05, 'numpy_to_tensor': 7.401650692367845e-05, 'agent_to_module_mapping': 8.850333973006893e-06, 'add_observations_from_episodes_to_batch': 4.205848657279554e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009390896920842445, 'timers': {'connectors': {'get_actions': 0.0004811357486415839, 'un_batch_to_individual_items': 7.128031527705492e-05, 'tensor_to_numpy': 0.00012827673502630878, 'module_to_agent_unmapping': 6.9409005444371784e-06, 'normalize_and_clip_actions': 7.824207582647674e-05, 'listify_data_for_vector_env': 2.5885366645628382e-05, 'remove_single_ts_time_rank_from_batch': 2.584349374898606e-06}}}, 'sample': 88.4220384999644, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.520000000000074, 'blue_0': -51.88000000000043, 'blue_1': -45.46000000000039, 'red_0': -30.200000000000227}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 113.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 684000.0, 'blue_0': 684000.0, 'blue_1': 684000.0, 'red_0': 684000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.520000000000074, 'blue_policy': -45.46000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 1368000.0, 'blue_policy': 1368000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036360140449744413, 'episode_return_mean': -149.06000000000114, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -139.60000000000102, 'episode_duration_sec_mean': 17.56486482003238, 'episode_return_min': -157.60000000000124, 'rlmodule_inference_timer': 0.013965530930642109, 'num_episodes_lifetime': 570.0, 'episode_len_min': 1200, 'time_between_sampling': 258.1770011000335, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.3049585535347, 'throughput_since_last_restore': 15.547033105510742}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 167
(MultiAgentEnvRunner pid=37492) {'red_0': -46.40000000000038, 'red_1': -50.10000000000041, 'blue_0': -45.90000000000037, 'blue_1': -49.00000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -3.5000000000000018, 'red_1': -10.09999999999998, 'blue_0': -41.50000000000032, 'blue_1': -46.20000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -23.10000000000006, 'red_1': -20.900000000000027, 'blue_0': -44.20000000000035, 'blue_1': -39.800000000000296}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 873
(MultiAgentEnvRunner pid=37492) {'red_0': -20.40000000000002, 'red_1': -38.00000000000029, 'blue_0': -33.70000000000021, 'blue_1': -50.80000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -48.10000000000039, 'red_1': -35.50000000000023, 'blue_0': -44.90000000000036, 'blue_1': -48.80000000000041}
ITERATION 114: reward=-148.18000000000106, metadata={'num_env_steps_sampled_lifetime': 690000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003210365694392004, 'timers': {'connectors': {'batch_individual_items': 9.578470033368287e-05, 'add_states_from_episodes_to_batch': 6.657640294017989e-06, 'add_time_dim_to_batch_and_zero_pad': 1.221538401554578e-05, 'numpy_to_tensor': 6.603616874200365e-05, 'agent_to_module_mapping': 8.099959011337661e-06, 'add_observations_from_episodes_to_batch': 3.774710466093286e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000858725256669118, 'timers': {'connectors': {'get_actions': 0.000442435482863519, 'un_batch_to_individual_items': 6.436926152735409e-05, 'tensor_to_numpy': 0.00011780021887233576, 'module_to_agent_unmapping': 6.308537049134329e-06, 'normalize_and_clip_actions': 7.019066139834231e-05, 'listify_data_for_vector_env': 2.3577263139462698e-05, 'remove_single_ts_time_rank_from_batch': 2.2985786598622534e-06}}}, 'sample': 88.30766840011347, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.920000000000186, 'blue_0': -42.040000000000326, 'blue_1': -46.920000000000385, 'red_0': -28.30000000000017}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 114.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 690000.0, 'blue_0': 690000.0, 'blue_1': 690000.0, 'red_0': 690000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.920000000000186, 'blue_policy': -46.920000000000385}, 'num_module_steps_sampled_lifetime': {'red_policy': 1380000.0, 'blue_policy': 1380000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003323567940953357, 'episode_return_mean': -148.18000000000106, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.30000000000068, 'episode_duration_sec_mean': 17.540888260002248, 'episode_return_min': -191.40000000000157, 'rlmodule_inference_timer': 0.012468428445860139, 'num_episodes_lifetime': 575.0, 'episode_len_min': 1200, 'time_between_sampling': 258.2966642000247, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.32333445251258, 'throughput_since_last_restore': 15.560906952478888}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.099999999999998, 'red_1': -38.40000000000026, 'blue_0': -37.700000000000266, 'blue_1': -44.80000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 359
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 698
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 710
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 726
(MultiAgentEnvRunner pid=37492) {'red_0': -34.40000000000024, 'red_1': -54.40000000000041, 'blue_0': -42.70000000000039, 'blue_1': -27.20000000000015}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 569
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 750
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 909
(MultiAgentEnvRunner pid=37492) {'red_0': -29.00000000000024, 'red_1': -3.299999999999983, 'blue_0': -50.000000000000355, 'blue_1': -45.700000000000294}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 305
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 539
(MultiAgentEnvRunner pid=37492) {'red_0': -19.99999999999998, 'red_1': -30.500000000000227, 'blue_0': -65.0000000000005, 'blue_1': -50.900000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 386
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 571
(MultiAgentEnvRunner pid=37492) {'red_0': -8.699999999999974, 'red_1': -9.299999999999963, 'blue_0': -43.400000000000304, 'blue_1': -42.200000000000365}
ITERATION 115: reward=-136.54000000000093, metadata={'num_env_steps_sampled_lifetime': 696000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003234668563569785, 'timers': {'connectors': {'batch_individual_items': 9.600451681907095e-05, 'add_states_from_episodes_to_batch': 6.30268087693068e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2183601726344099e-05, 'numpy_to_tensor': 6.740656386522644e-05, 'agent_to_module_mapping': 7.870753732301712e-06, 'add_observations_from_episodes_to_batch': 3.776391197696198e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008627027253116098, 'timers': {'connectors': {'get_actions': 0.00044321480568341423, 'un_batch_to_individual_items': 6.469802572894801e-05, 'tensor_to_numpy': 0.0001173520811461425, 'module_to_agent_unmapping': 6.346172383869484e-06, 'normalize_and_clip_actions': 7.263709979374734e-05, 'listify_data_for_vector_env': 2.3531652490079703e-05, 'remove_single_ts_time_rank_from_batch': 2.3191147712953437e-06}}}, 'sample': 88.47199500002898, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.180000000000167, 'blue_0': -47.76000000000037, 'blue_1': -42.1600000000003, 'red_0': -19.440000000000087}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 115.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 696000.0, 'blue_0': 696000.0, 'blue_1': 696000.0, 'red_0': 696000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.180000000000167, 'blue_policy': -42.1600000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 1392000.0, 'blue_policy': 1392000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000336456225062452, 'episode_return_mean': -136.54000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -103.6000000000006, 'episode_duration_sec_mean': 17.558542579994537, 'episode_return_min': -166.40000000000109, 'rlmodule_inference_timer': 0.01241307435261838, 'num_episodes_lifetime': 580.0, 'episode_len_min': 1200, 'time_between_sampling': 258.049524400034, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.285534791732907, 'throughput_since_last_restore': 15.57430180962675}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -0.4, 'red_1': -0.7999999999999999, 'blue_0': -33.60000000000021, 'blue_1': -60.300000000000566}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.199999999999964, 'red_1': -17.899999999999988, 'blue_0': -48.700000000000415, 'blue_1': -37.40000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -16.89999999999997, 'red_1': -9.099999999999984, 'blue_0': -47.7000000000004, 'blue_1': -47.4000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -46.600000000000385, 'red_1': -43.50000000000034, 'blue_0': -40.8000000000003, 'blue_1': -37.50000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 564
(MultiAgentEnvRunner pid=37492) {'red_0': -23.900000000000073, 'red_1': -35.40000000000023, 'blue_0': -43.70000000000034, 'blue_1': -53.9000000000005}
ITERATION 116: reward=-132.14000000000092, metadata={'num_env_steps_sampled_lifetime': 702000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034024105547865403, 'timers': {'connectors': {'batch_individual_items': 9.715202768930927e-05, 'add_states_from_episodes_to_batch': 6.802524788192671e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3144374571667535e-05, 'numpy_to_tensor': 7.129907814410579e-05, 'agent_to_module_mapping': 8.831319876514427e-06, 'add_observations_from_episodes_to_batch': 4.082001789168497e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009112956574969783, 'timers': {'connectors': {'get_actions': 0.0004676017884599715, 'un_batch_to_individual_items': 6.99564678974614e-05, 'tensor_to_numpy': 0.0001243385457389927, 'module_to_agent_unmapping': 6.935827843878129e-06, 'normalize_and_clip_actions': 7.497423678097721e-05, 'listify_data_for_vector_env': 2.527038993566768e-05, 'remove_single_ts_time_rank_from_batch': 2.4143417034502913e-06}}}, 'sample': 88.8116449000081, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.34000000000011, 'blue_0': -42.90000000000033, 'blue_1': -47.300000000000395, 'red_0': -20.60000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 116.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 702000.0, 'blue_0': 702000.0, 'blue_1': 702000.0, 'red_0': 702000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.34000000000011, 'blue_policy': -47.300000000000395}, 'num_module_steps_sampled_lifetime': {'red_policy': 1404000.0, 'blue_policy': 1404000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003548007102059804, 'episode_return_mean': -132.14000000000092, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -95.10000000000078, 'episode_duration_sec_mean': 17.643906420003624, 'episode_return_min': -168.40000000000128, 'rlmodule_inference_timer': 0.01355567705059663, 'num_episodes_lifetime': 585.0, 'episode_len_min': 1200, 'time_between_sampling': 258.6431439999724, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.30710749220637, 'throughput_since_last_restore': 15.587639916990614}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 262
(MultiAgentEnvRunner pid=37492) {'red_0': -18.300000000000008, 'red_1': -23.200000000000045, 'blue_0': -34.10000000000019, 'blue_1': -48.10000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 273
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 312
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 999
(MultiAgentEnvRunner pid=37492) {'red_0': -46.6000000000003, 'red_1': -52.6000000000004, 'blue_0': -52.90000000000052, 'blue_1': -26.90000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 315
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 328
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 374
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 582
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1073
(MultiAgentEnvRunner pid=37492) {'red_0': -71.50000000000016, 'red_1': -49.400000000000375, 'blue_0': -43.40000000000033, 'blue_1': -25.900000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 303
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 942
(MultiAgentEnvRunner pid=37492) {'red_0': -36.10000000000016, 'red_1': -31.700000000000152, 'blue_0': -25.80000000000031, 'blue_1': -36.700000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 361
(MultiAgentEnvRunner pid=37492) {'red_0': -24.500000000000096, 'red_1': -25.100000000000104, 'blue_0': -27.000000000000096, 'blue_1': -44.0000000000004}
ITERATION 117: reward=-148.76000000000093, metadata={'num_env_steps_sampled_lifetime': 708000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003184960419591921, 'timers': {'connectors': {'batch_individual_items': 9.438511889282912e-05, 'add_states_from_episodes_to_batch': 6.362775843458873e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2070074904090119e-05, 'numpy_to_tensor': 6.543751489080834e-05, 'agent_to_module_mapping': 8.092346054697525e-06, 'add_observations_from_episodes_to_batch': 3.774053976552788e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008577626982306432, 'timers': {'connectors': {'get_actions': 0.00043989546179490466, 'un_batch_to_individual_items': 6.446877459067117e-05, 'tensor_to_numpy': 0.00011640103313582436, 'module_to_agent_unmapping': 6.341199081996358e-06, 'normalize_and_clip_actions': 7.103932082382013e-05, 'listify_data_for_vector_env': 2.419034893750203e-05, 'remove_single_ts_time_rank_from_batch': 2.3572241689455967e-06}}}, 'sample': 88.4657947999658, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -36.40000000000022, 'blue_0': -36.640000000000285, 'blue_1': -36.32000000000029, 'red_0': -39.40000000000014}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 117.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 708000.0, 'blue_0': 708000.0, 'blue_1': 708000.0, 'red_0': 708000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -36.40000000000022, 'blue_policy': -36.32000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 1416000.0, 'blue_policy': 1416000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034296330272219004, 'episode_return_mean': -148.76000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -120.60000000000069, 'episode_duration_sec_mean': 17.560615500016137, 'episode_return_min': -190.2000000000009, 'rlmodule_inference_timer': 0.012486137082881734, 'num_episodes_lifetime': 590.0, 'episode_len_min': 1200, 'time_between_sampling': 257.8668083999073, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.32515983536721, 'throughput_since_last_restore': 15.600898263030794}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -19.0, 'red_1': -29.600000000000154, 'blue_0': -46.20000000000037, 'blue_1': -46.400000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 351
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1137
(MultiAgentEnvRunner pid=37492) {'red_0': 0.19999999999999885, 'red_1': -6.099999999999981, 'blue_0': -38.20000000000031, 'blue_1': -48.60000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 302
(MultiAgentEnvRunner pid=37492) {'red_0': -3.3000000000000016, 'red_1': -19.800000000000015, 'blue_0': -45.00000000000036, 'blue_1': -32.00000000000018}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 273
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 418
(MultiAgentEnvRunner pid=37492) {'red_0': -36.60000000000022, 'red_1': -25.200000000000113, 'blue_0': -55.200000000000486, 'blue_1': -46.000000000000384}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 730
(MultiAgentEnvRunner pid=37492) {'red_0': -4.699999999999999, 'red_1': -15.599999999999978, 'blue_0': -46.900000000000375, 'blue_1': -39.9000000000003}
ITERATION 118: reward=-120.8200000000008, metadata={'num_env_steps_sampled_lifetime': 714000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031877773698299125, 'timers': {'connectors': {'batch_individual_items': 9.405656414558278e-05, 'add_states_from_episodes_to_batch': 6.470282907832007e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2153573169854546e-05, 'numpy_to_tensor': 6.603008664816894e-05, 'agent_to_module_mapping': 7.934740604476421e-06, 'add_observations_from_episodes_to_batch': 3.7669819724730634e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008494923069991608, 'timers': {'connectors': {'get_actions': 0.00043795852050471714, 'un_batch_to_individual_items': 6.383915806775338e-05, 'tensor_to_numpy': 0.00011583655233000765, 'module_to_agent_unmapping': 6.252049360771198e-06, 'normalize_and_clip_actions': 6.973775207438104e-05, 'listify_data_for_vector_env': 2.3280753328951866e-05, 'remove_single_ts_time_rank_from_batch': 2.396157221461216e-06}}}, 'sample': 87.43164790002629, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -19.26000000000005, 'blue_0': -46.30000000000039, 'blue_1': -42.58000000000033, 'red_0': -12.680000000000044}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 118.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 714000.0, 'blue_0': 714000.0, 'blue_1': 714000.0, 'red_0': 714000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -19.26000000000005, 'blue_policy': -42.58000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 1428000.0, 'blue_policy': 1428000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033152835880914753, 'episode_return_mean': -120.8200000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -92.70000000000074, 'episode_duration_sec_mean': 17.369118880014867, 'episode_return_min': -163.0000000000012, 'rlmodule_inference_timer': 0.01239216051691328, 'num_episodes_lifetime': 595.0, 'episode_len_min': 1200, 'time_between_sampling': 257.8567622000119, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.35825850031009, 'throughput_since_last_restore': 15.614181388787653}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1078
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1130
(MultiAgentEnvRunner pid=37492) {'red_0': -22.400000000000016, 'red_1': -54.40000000000044, 'blue_0': -30.00000000000037, 'blue_1': -42.600000000000385}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 331
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 741
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 755
(MultiAgentEnvRunner pid=37492) {'red_0': -42.50000000000026, 'red_1': -42.60000000000025, 'blue_0': -25.10000000000014, 'blue_1': -36.70000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1013
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1067
(MultiAgentEnvRunner pid=37492) {'red_0': -32.300000000000146, 'red_1': -79.4, 'blue_0': -26.400000000000308, 'blue_1': -37.50000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 338
(MultiAgentEnvRunner pid=37492) {'red_0': -25.800000000000114, 'red_1': -20.800000000000043, 'blue_0': -39.000000000000256, 'blue_1': -31.40000000000015}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.199999999999994, 'red_1': -6.8999999999999915, 'blue_0': -56.000000000000504, 'blue_1': -52.800000000000466}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.2999999999975, 'red_1': 0, 'blue_0': -118.29999999999755, 'blue_1': -0.30000000000000004}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -15.299999999999963, 'red_1': -0.1, 'blue_0': -5.1999999999999975, 'blue_1': -100.89999999999854}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 04:04:27,582	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -6.799999999999992, 'red_1': -6.5999999999999925, 'blue_0': -116.29999999999767, 'blue_1': -115.99999999999768}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -118.59999999999754, 'red_1': 0, 'blue_0': -119.09999999999751, 'blue_1': -0.7}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': -14.199999999999969, 'blue_0': -4.5, 'blue_1': -105.0999999999983}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 119: reward=-142.1600000000009, metadata={'num_env_steps_sampled_lifetime': 720000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034476492759593155, 'timers': {'connectors': {'batch_individual_items': 0.00010096109875051692, 'add_states_from_episodes_to_batch': 6.855044284735426e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3193202874817496e-05, 'numpy_to_tensor': 7.120613593091978e-05, 'agent_to_module_mapping': 8.484532801757128e-06, 'add_observations_from_episodes_to_batch': 4.13339948153716e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009104851789540119, 'timers': {'connectors': {'get_actions': 0.0004633034786144132, 'un_batch_to_individual_items': 6.991227053909825e-05, 'tensor_to_numpy': 0.00012356435912162146, 'module_to_agent_unmapping': 6.865976074006949e-06, 'normalize_and_clip_actions': 7.652214990339498e-05, 'listify_data_for_vector_env': 2.565852353268601e-05, 'remove_single_ts_time_rank_from_batch': 2.493431918185944e-06}}}, 'sample': 87.89447609998751, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -40.82000000000014, 'blue_0': -35.30000000000031, 'blue_1': -40.20000000000033, 'red_0': -25.840000000000106}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 119.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 720000.0, 'blue_0': 720000.0, 'blue_1': 720000.0, 'red_0': 720000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -40.82000000000014, 'blue_policy': -40.20000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 1440000.0, 'blue_policy': 1440000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035311944014708517, 'episode_return_mean': -142.1600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -117.00000000000057, 'episode_duration_sec_mean': 17.456003820034674, 'episode_return_min': -175.6000000000008, 'rlmodule_inference_timer': 0.013416392321354905, 'num_episodes_lifetime': 600.0, 'episode_len_min': 1200, 'time_between_sampling': 258.22800950007513, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.996071814429264, 'throughput_since_last_restore': 15.599151917865862}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 412
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 668
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 770
(MultiAgentEnvRunner pid=37492) {'red_0': -25.20000000000001, 'red_1': -42.000000000000234, 'blue_0': -52.40000000000052, 'blue_1': -33.3000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1121
(MultiAgentEnvRunner pid=37492) {'red_0': -21.500000000000036, 'red_1': -43.70000000000037, 'blue_0': -46.80000000000038, 'blue_1': -47.4000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 948
(MultiAgentEnvRunner pid=37492) {'red_0': -18.200000000000006, 'red_1': -24.10000000000009, 'blue_0': -36.20000000000031, 'blue_1': -31.20000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 887
(MultiAgentEnvRunner pid=37492) {'red_0': -15.699999999999964, 'red_1': -22.30000000000005, 'blue_0': -60.300000000000566, 'blue_1': -46.400000000000404}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 752
(MultiAgentEnvRunner pid=37492) {'red_0': -48.90000000000034, 'red_1': -22.500000000000068, 'blue_0': -31.600000000000314, 'blue_1': -53.70000000000055}
ITERATION 120: reward=-144.68000000000106, metadata={'num_env_steps_sampled_lifetime': 726000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034181189803092677, 'timers': {'connectors': {'batch_individual_items': 9.765270120813663e-05, 'add_states_from_episodes_to_batch': 6.9012793015941086e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3220922245773133e-05, 'numpy_to_tensor': 7.164318519999555e-05, 'agent_to_module_mapping': 8.571556604690344e-06, 'add_observations_from_episodes_to_batch': 4.0727740941658416e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009156474469348659, 'timers': {'connectors': {'get_actions': 0.00046873869779168306, 'un_batch_to_individual_items': 6.956262996848517e-05, 'tensor_to_numpy': 0.00012550109414977316, 'module_to_agent_unmapping': 6.7396491762571985e-06, 'normalize_and_clip_actions': 7.558587573791787e-05, 'listify_data_for_vector_env': 2.561781213449257e-05, 'remove_single_ts_time_rank_from_batch': 2.4874108969652112e-06}}}, 'sample': 87.80093679996207, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.920000000000165, 'blue_0': -45.46000000000041, 'blue_1': -42.40000000000041, 'red_0': -25.90000000000007}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 120.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 726000.0, 'blue_0': 726000.0, 'blue_1': 726000.0, 'red_0': 726000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.920000000000165, 'blue_policy': -42.40000000000041}, 'num_module_steps_sampled_lifetime': {'red_policy': 1452000.0, 'blue_policy': 1452000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003637916351144324, 'episode_return_mean': -144.68000000000106, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.70000000000078, 'episode_duration_sec_mean': 17.41922935999464, 'episode_return_min': -159.4000000000012, 'rlmodule_inference_timer': 0.013515316171960536, 'num_episodes_lifetime': 605.0, 'episode_len_min': 1200, 'time_between_sampling': 340.803152199951, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.33757962598331, 'throughput_since_last_restore': 15.612088233873477}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 346.03032600006554, 'restore_env_runners': 1.1100084520876408e-05, 'training_step': 346.03006800008006, 'env_runner_sampling_timer': 87.93485749990214, 'learner_update_timer': 258.03960030002054, 'synch_weights': 0.013425800018012524, 'synch_env_connectors': 0.0018307999707758427, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 726000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034181189803092677, 'timers': {'connectors': {'batch_individual_items': 9.765270120813663e-05, 'add_states_from_episodes_to_batch': 6.9012793015941086e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3220922245773133e-05, 'numpy_to_tensor': 7.164318519999555e-05, 'agent_to_module_mapping': 8.571556604690344e-06, 'add_observations_from_episodes_to_batch': 4.0727740941658416e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009156474469348659, 'timers': {'connectors': {'get_actions': 0.00046873869779168306, 'un_batch_to_individual_items': 6.956262996848517e-05, 'tensor_to_numpy': 0.00012550109414977316, 'module_to_agent_unmapping': 6.7396491762571985e-06, 'normalize_and_clip_actions': 7.558587573791787e-05, 'listify_data_for_vector_env': 2.561781213449257e-05, 'remove_single_ts_time_rank_from_batch': 2.4874108969652112e-06}}}, 'sample': 87.80093679996207, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.920000000000165, 'blue_0': -45.46000000000041, 'blue_1': -42.40000000000041, 'red_0': -25.90000000000007}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 120.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 726000.0, 'blue_0': 726000.0, 'blue_1': 726000.0, 'red_0': 726000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.920000000000165, 'blue_policy': -42.40000000000041}, 'num_module_steps_sampled_lifetime': {'red_policy': 1452000.0, 'blue_policy': 1452000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003637916351144324, 'episode_return_mean': -144.68000000000106, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.70000000000078, 'episode_duration_sec_mean': 17.41922935999464, 'episode_return_min': -159.4000000000012, 'rlmodule_inference_timer': 0.013515316171960536, 'num_episodes_lifetime': 605.0, 'episode_len_min': 1200, 'time_between_sampling': 340.803152199951, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.33757962598331, 'throughput_since_last_restore': 15.612088233873477}}, 'learners': {'red_policy': {'policy_loss': -0.15764516592025757, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.010488126426935196, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 121.0, 'num_module_steps_trained_lifetime': 43598720.0, 'curr_entropy_coeff': 0.039110000000000006, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002274, 'vf_explained_var': -0.5460015535354614, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.818436622619629, 'entropy': 1.067291259765625, 'vf_loss_unclipped': 694.1992797851562, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1041.1849130001162, 'throughput_since_last_restore': 937.5579320990374}}, 'blue_policy': {'weights_seq_no': 121.0, 'num_module_steps_trained_lifetime': 43598720.0, 'curr_entropy_coeff': 0.039110000000000006, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002274, 'vf_explained_var': 0.3221955895423889, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 4.889270305633545, 'total_loss': 1.1354138851165771, 'entropy': 1.721428632736206, 'policy_loss': -0.39112138748168945, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.5742599964141846, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.013007495552301407, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1041.1830768424875, 'throughput_since_last_restore': 937.557933580906}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 7.00005330145359e-06, 'batch_individual_items': 0.7799054000061005, 'add_time_dim_to_batch_and_zero_pad': 2.059992402791977e-05, 'numpy_to_tensor': 0.15297449997160584, 'add_observations_from_episodes_to_batch': 0.0002571999793872237, 'agent_to_module_mapping': 0.021898500039242208, 'add_one_ts_to_episodes_and_truncate': 0.1998112000292167, 'add_columns_from_episodes_to_train_batch': 0.5024333999026567, 'general_advantage_estimation': 13.069399799918756}}, 'connector_pipeline_timer': 14.72711499989964}, 'num_module_steps_trained_lifetime': 87197440.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 2043690000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48805.40970811366, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48805.4076491177, 'throughput_since_last_restore': 43948.02820720176}, 'num_module_steps_trained_throughput': 2082.3640536777352, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2082.3640296094245, 'throughput_since_last_restore': 1875.1158693957073}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 726000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 121, 'trial_id': 'default', 'date': '2026-01-26_04-13-54', 'timestamp': 1769397234, 'time_this_iter_s': 346.04472303390503, 'time_total_s': 46480.10840678215, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 46480.10840678215, 'iterations_since_restore': 121, 'perf': {'cpu_util_percent': 14.802636916835699, 'ram_util_percent': 91.8657200811359}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 844
(MultiAgentEnvRunner pid=37492) {'red_0': -17.599999999999994, 'red_1': -17.299999999999994, 'blue_0': -42.1000000000004, 'blue_1': -32.800000000000395}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 815
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 868
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 964
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1197
(MultiAgentEnvRunner pid=37492) {'red_0': -51.200000000000344, 'red_1': -37.30000000000018, 'blue_0': -20.300000000000168, 'blue_1': -25.200000000000216}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -11.199999999999976, 'red_1': -2.700000000000001, 'blue_0': -52.50000000000046, 'blue_1': -45.50000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 378
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 620
(MultiAgentEnvRunner pid=37492) {'red_0': -27.900000000000126, 'red_1': -58.9000000000005, 'blue_0': -36.90000000000021, 'blue_1': -29.000000000000128}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.89999999999996, 'red_1': -4.799999999999999, 'blue_0': -36.10000000000024, 'blue_1': -46.60000000000039}
ITERATION 121: reward=-122.36000000000081, metadata={'num_env_steps_sampled_lifetime': 732000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031650767228913495, 'timers': {'connectors': {'batch_individual_items': 9.38023929747699e-05, 'add_states_from_episodes_to_batch': 6.1383132833915335e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1966067244529549e-05, 'numpy_to_tensor': 6.634337936192169e-05, 'agent_to_module_mapping': 7.746312580001801e-06, 'add_observations_from_episodes_to_batch': 3.749654842238225e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008480782319317881, 'timers': {'connectors': {'get_actions': 0.00043613469434578174, 'un_batch_to_individual_items': 6.371968254185401e-05, 'tensor_to_numpy': 0.00011577616162209949, 'module_to_agent_unmapping': 6.189725948732989e-06, 'normalize_and_clip_actions': 7.042108676733486e-05, 'listify_data_for_vector_env': 2.3722091371455115e-05, 'remove_single_ts_time_rank_from_batch': 2.253816049106386e-06}}}, 'sample': 88.51641060004476, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -24.200000000000138, 'blue_0': -37.5800000000003, 'blue_1': -35.82000000000029, 'red_0': -24.76000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 121.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 732000.0, 'blue_0': 732000.0, 'blue_1': 732000.0, 'red_0': 732000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -24.200000000000138, 'blue_policy': -35.82000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 1464000.0, 'blue_policy': 1464000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033284646079999596, 'episode_return_mean': -122.36000000000081, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -103.4000000000006, 'episode_duration_sec_mean': 17.58529268000275, 'episode_return_min': -152.70000000000095, 'rlmodule_inference_timer': 0.012366535565174724, 'num_episodes_lifetime': 610.0, 'episode_len_min': 1200, 'time_between_sampling': 258.35287229996175, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.282816571491946, 'throughput_since_last_restore': 15.624467950913445}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 370
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 671
(MultiAgentEnvRunner pid=37492) {'red_0': -1.5000000000000007, 'red_1': -11.299999999999963, 'blue_0': -57.70000000000049, 'blue_1': -38.60000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1117
(MultiAgentEnvRunner pid=37492) {'red_0': -24.600000000000044, 'red_1': -16.29999999999998, 'blue_0': -25.400000000000297, 'blue_1': -43.0000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1103
(MultiAgentEnvRunner pid=37492) {'red_0': -17.299999999999994, 'red_1': -24.300000000000008, 'blue_0': -28.300000000000345, 'blue_1': -47.900000000000475}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 442
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 471
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 604
(MultiAgentEnvRunner pid=37492) {'red_0': -29.200000000000188, 'red_1': -42.10000000000027, 'blue_0': -42.80000000000031, 'blue_1': -28.70000000000011}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 729
(MultiAgentEnvRunner pid=37492) {'red_0': -17.199999999999978, 'red_1': -20.90000000000003, 'blue_0': -41.00000000000031, 'blue_1': -35.80000000000027}
ITERATION 122: reward=-118.78000000000074, metadata={'num_env_steps_sampled_lifetime': 738000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031806351941751393, 'timers': {'connectors': {'batch_individual_items': 9.423521631546365e-05, 'add_states_from_episodes_to_batch': 6.301472961192967e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2121470535909245e-05, 'numpy_to_tensor': 6.588302543064556e-05, 'agent_to_module_mapping': 7.841679389714947e-06, 'add_observations_from_episodes_to_batch': 3.768238575895498e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008513732297930984, 'timers': {'connectors': {'get_actions': 0.00043936959306023574, 'un_batch_to_individual_items': 6.291781179881028e-05, 'tensor_to_numpy': 0.0001169062406863324, 'module_to_agent_unmapping': 6.130367846426855e-06, 'normalize_and_clip_actions': 7.000046359796531e-05, 'listify_data_for_vector_env': 2.3124385693359338e-05, 'remove_single_ts_time_rank_from_batch': 2.332858342626488e-06}}}, 'sample': 87.75711380003486, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.980000000000054, 'blue_0': -39.040000000000354, 'blue_1': -38.80000000000031, 'red_0': -17.96000000000004}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 122.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 738000.0, 'blue_0': 738000.0, 'blue_1': 738000.0, 'red_0': 738000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.980000000000054, 'blue_policy': -38.80000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 1476000.0, 'blue_policy': 1476000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033096877645300194, 'episode_return_mean': -118.78000000000074, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.10000000000078, 'episode_duration_sec_mean': 17.419999560015277, 'episode_return_min': -142.8000000000009, 'rlmodule_inference_timer': 0.012359019742802294, 'num_episodes_lifetime': 615.0, 'episode_len_min': 1200, 'time_between_sampling': 258.5718161999248, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.356217526582267, 'throughput_since_last_restore': 15.637151373619878}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 815
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 878
(MultiAgentEnvRunner pid=37492) {'red_0': -46.90000000000037, 'red_1': -38.6000000000003, 'blue_0': -46.90000000000038, 'blue_1': -40.800000000000324}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.699999999999996, 'red_1': -5.599999999999996, 'blue_0': -40.70000000000031, 'blue_1': -48.10000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 387
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 628
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 628
(MultiAgentEnvRunner pid=37492) {'red_0': -21.20000000000006, 'red_1': -27.300000000000118, 'blue_0': -37.30000000000022, 'blue_1': -29.000000000000128}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 556
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1190
(MultiAgentEnvRunner pid=37492) {'red_0': -40.90000000000025, 'red_1': -24.80000000000003, 'blue_0': -21.900000000000084, 'blue_1': -14.900000000000116}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -1.0999999999999999, 'red_1': -23.00000000000006, 'blue_0': -47.300000000000395, 'blue_1': -44.40000000000036}
ITERATION 123: reward=-121.2800000000008, metadata={'num_env_steps_sampled_lifetime': 744000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003284768225374949, 'timers': {'connectors': {'batch_individual_items': 9.750759987942605e-05, 'add_states_from_episodes_to_batch': 6.5909711164055584e-06, 'add_time_dim_to_batch_and_zero_pad': 1.228733237280623e-05, 'numpy_to_tensor': 6.789627383086075e-05, 'agent_to_module_mapping': 8.116644297105326e-06, 'add_observations_from_episodes_to_batch': 3.882290286623015e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008680595188528881, 'timers': {'connectors': {'get_actions': 0.0004433943253090465, 'un_batch_to_individual_items': 6.600181944801021e-05, 'tensor_to_numpy': 0.00011929265415391514, 'module_to_agent_unmapping': 6.459513482644861e-06, 'normalize_and_clip_actions': 7.27346192508495e-05, 'listify_data_for_vector_env': 2.386656304975288e-05, 'remove_single_ts_time_rank_from_batch': 2.365750368786403e-06}}}, 'sample': 88.38264369999524, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -23.8600000000001, 'blue_0': -38.82000000000028, 'blue_1': -35.44000000000027, 'red_0': -23.16000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 123.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 744000.0, 'blue_0': 744000.0, 'blue_1': 744000.0, 'red_0': 744000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -23.8600000000001, 'blue_policy': -35.44000000000027}, 'num_module_steps_sampled_lifetime': {'red_policy': 1488000.0, 'blue_policy': 1488000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003423342205052656, 'episode_return_mean': -121.2800000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -100.1000000000007, 'episode_duration_sec_mean': 17.556784879998304, 'episode_return_min': -173.20000000000138, 'rlmodule_inference_timer': 0.012607190580973168, 'num_episodes_lifetime': 620.0, 'episode_len_min': 1200, 'time_between_sampling': 257.93451729998924, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.260649307548533, 'throughput_since_last_restore': 15.649020875833676}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 634
(MultiAgentEnvRunner pid=37492) {'red_0': -22.90000000000006, 'red_1': -17.39999999999998, 'blue_0': -50.800000000000445, 'blue_1': -38.900000000000304}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 797
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 822
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 826
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 826
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 876
(MultiAgentEnvRunner pid=37492) {'red_0': -61.700000000000536, 'red_1': -43.30000000000035, 'blue_0': -27.100000000000207, 'blue_1': -35.20000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 833
(MultiAgentEnvRunner pid=37492) {'red_0': -16.79999999999997, 'red_1': -24.50000000000008, 'blue_0': -45.800000000000374, 'blue_1': -41.10000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 375
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 526
(MultiAgentEnvRunner pid=37492) {'red_0': -18.50000000000001, 'red_1': -24.600000000000016, 'blue_0': -43.30000000000031, 'blue_1': -39.90000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 528
(MultiAgentEnvRunner pid=37492) {'red_0': -20.200000000000035, 'red_1': -27.600000000000055, 'blue_0': -26.600000000000072, 'blue_1': -36.800000000000324}
ITERATION 124: reward=-132.60000000000082, metadata={'num_env_steps_sampled_lifetime': 750000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031921877780051667, 'timers': {'connectors': {'batch_individual_items': 9.57655304175351e-05, 'add_states_from_episodes_to_batch': 6.390236052551591e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1944362446508636e-05, 'numpy_to_tensor': 6.55880470798197e-05, 'agent_to_module_mapping': 7.841982500154737e-06, 'add_observations_from_episodes_to_batch': 3.7403818674123326e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008507616897208721, 'timers': {'connectors': {'get_actions': 0.00043844316391548503, 'un_batch_to_individual_items': 6.392464132737511e-05, 'tensor_to_numpy': 0.00011629995848589809, 'module_to_agent_unmapping': 6.263088549232479e-06, 'normalize_and_clip_actions': 7.01199095550154e-05, 'listify_data_for_vector_env': 2.3268268060946255e-05, 'remove_single_ts_time_rank_from_batch': 2.2764270858561286e-06}}}, 'sample': 87.91152500000317, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.480000000000096, 'blue_0': -38.72000000000029, 'blue_1': -38.38000000000031, 'red_0': -28.020000000000124}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 124.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 750000.0, 'blue_0': 750000.0, 'blue_1': 750000.0, 'red_0': 750000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.480000000000096, 'blue_policy': -38.38000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 1500000.0, 'blue_policy': 1500000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034175980576275656, 'episode_return_mean': -132.60000000000082, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.20000000000049, 'episode_duration_sec_mean': 17.458108799997717, 'episode_return_min': -167.30000000000138, 'rlmodule_inference_timer': 0.012268248245558673, 'num_episodes_lifetime': 625.0, 'episode_len_min': 1200, 'time_between_sampling': 259.2314908999251, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.360679952394356, 'throughput_since_last_restore': 15.661372812975568}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 398
(MultiAgentEnvRunner pid=37492) {'red_0': -27.500000000000053, 'red_1': -27.90000000000006, 'blue_0': -30.20000000000013, 'blue_1': -57.90000000000059}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 337
(MultiAgentEnvRunner pid=37492) {'red_0': -55.8000000000005, 'red_1': -44.00000000000034, 'blue_0': -36.900000000000254, 'blue_1': -42.70000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 475
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 747
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 785
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 936
(MultiAgentEnvRunner pid=37492) {'red_0': -64.10000000000052, 'red_1': -32.20000000000015, 'blue_0': -27.80000000000008, 'blue_1': -45.50000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 807
(MultiAgentEnvRunner pid=37492) {'red_0': -13.599999999999985, 'red_1': -12.29999999999999, 'blue_0': -49.900000000000496, 'blue_1': -38.10000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -11.499999999999975, 'red_1': -16.299999999999965, 'blue_0': -39.30000000000029, 'blue_1': -55.1000000000005}
ITERATION 125: reward=-145.72000000000102, metadata={'num_env_steps_sampled_lifetime': 756000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003162155248361167, 'timers': {'connectors': {'batch_individual_items': 9.48063751419009e-05, 'add_states_from_episodes_to_batch': 6.218715690273363e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2049956136446175e-05, 'numpy_to_tensor': 6.539742812970637e-05, 'agent_to_module_mapping': 7.81546506367898e-06, 'add_observations_from_episodes_to_batch': 3.7154375038604844e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008418837036339207, 'timers': {'connectors': {'get_actions': 0.0004309298174828158, 'un_batch_to_individual_items': 6.320805840129424e-05, 'tensor_to_numpy': 0.00011526942262602715, 'module_to_agent_unmapping': 6.486335292343796e-06, 'normalize_and_clip_actions': 7.013130111525598e-05, 'listify_data_for_vector_env': 2.3391609060731533e-05, 'remove_single_ts_time_rank_from_batch': 2.281017284896375e-06}}}, 'sample': 88.28327309992164, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.5400000000001, 'blue_0': -36.820000000000256, 'blue_1': -47.860000000000475, 'red_0': -34.500000000000206}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 125.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 756000.0, 'blue_0': 756000.0, 'blue_1': 756000.0, 'red_0': 756000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.5400000000001, 'blue_policy': -47.860000000000475}, 'num_module_steps_sampled_lifetime': {'red_policy': 1512000.0, 'blue_policy': 1512000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003303041596559775, 'episode_return_mean': -145.72000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -113.90000000000094, 'episode_duration_sec_mean': 17.519222459965384, 'episode_return_min': -179.40000000000143, 'rlmodule_inference_timer': 0.012309656479887689, 'num_episodes_lifetime': 630.0, 'episode_len_min': 1200, 'time_between_sampling': 257.7001468000235, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.339200998712364, 'throughput_since_last_restore': 15.673408898790862}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -1.4000000000000001, 'red_1': -2.3000000000000007, 'blue_0': -40.1000000000003, 'blue_1': -39.00000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1098
(MultiAgentEnvRunner pid=37492) {'red_0': -20.700000000000024, 'red_1': -40.500000000000284, 'blue_0': -53.6000000000005, 'blue_1': -42.90000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 805
(MultiAgentEnvRunner pid=37492) {'red_0': -19.1, 'red_1': -13.699999999999985, 'blue_0': -31.900000000000283, 'blue_1': -47.30000000000046}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -29.40000000000015, 'red_1': -36.800000000000246, 'blue_0': -40.400000000000304, 'blue_1': -46.00000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1166
(MultiAgentEnvRunner pid=37492) {'red_0': -44.90000000000028, 'red_1': -38.100000000000186, 'blue_0': -31.200000000000387, 'blue_1': -40.50000000000036}
ITERATION 126: reward=-131.96000000000095, metadata={'num_env_steps_sampled_lifetime': 762000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003252690404999377, 'timers': {'connectors': {'batch_individual_items': 9.650041804926704e-05, 'add_states_from_episodes_to_batch': 6.405952608659913e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2291705635967776e-05, 'numpy_to_tensor': 6.714086330149145e-05, 'agent_to_module_mapping': 8.099504811239358e-06, 'add_observations_from_episodes_to_batch': 3.8159449265295415e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000857883090414157, 'timers': {'connectors': {'get_actions': 0.00044144247453997476, 'un_batch_to_individual_items': 6.446739911020465e-05, 'tensor_to_numpy': 0.00011643371382384311, 'module_to_agent_unmapping': 6.345643997305098e-06, 'normalize_and_clip_actions': 7.07788080855357e-05, 'listify_data_for_vector_env': 2.3579457057059042e-05, 'remove_single_ts_time_rank_from_batch': 2.306869113003968e-06}}}, 'sample': 87.15672359999735, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.280000000000143, 'blue_0': -39.44000000000035, 'blue_1': -43.14000000000036, 'red_0': -23.10000000000009}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 126.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 762000.0, 'blue_0': 762000.0, 'blue_1': 762000.0, 'red_0': 762000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.280000000000143, 'blue_policy': -43.14000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 1524000.0, 'blue_policy': 1524000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003399439282205334, 'episode_return_mean': -131.96000000000095, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -82.80000000000058, 'episode_duration_sec_mean': 17.313548200018705, 'episode_return_min': -157.70000000000113, 'rlmodule_inference_timer': 0.012391845377052157, 'num_episodes_lifetime': 635.0, 'episode_len_min': 1200, 'time_between_sampling': 257.7560599000426, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.364149415216556, 'throughput_since_last_restore': 15.685434014035922}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 285
(MultiAgentEnvRunner pid=37492) {'red_0': -12.99999999999997, 'red_1': -26.400000000000095, 'blue_0': -29.200000000000138, 'blue_1': -38.800000000000274}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 482
(MultiAgentEnvRunner pid=37492) {'red_0': -43.80000000000029, 'red_1': -38.000000000000284, 'blue_0': -26.80000000000011, 'blue_1': -42.90000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.399999999999986, 'red_1': -25.40000000000009, 'blue_0': -43.30000000000034, 'blue_1': -46.50000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 479
(MultiAgentEnvRunner pid=37492) {'red_0': -18.30000000000001, 'red_1': -12.39999999999998, 'blue_0': -30.300000000000168, 'blue_1': -40.900000000000325}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 562
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 829
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 829
(MultiAgentEnvRunner pid=37492) {'red_0': -36.300000000000274, 'red_1': -35.60000000000028, 'blue_0': -30.50000000000012, 'blue_1': -44.100000000000385}
ITERATION 127: reward=-126.18000000000079, metadata={'num_env_steps_sampled_lifetime': 768000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003150687772809801, 'timers': {'connectors': {'batch_individual_items': 9.291307182366397e-05, 'add_states_from_episodes_to_batch': 6.222467025396675e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2122863956648458e-05, 'numpy_to_tensor': 6.517219095242921e-05, 'agent_to_module_mapping': 7.815281615410737e-06, 'add_observations_from_episodes_to_batch': 3.759165276636872e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008565971374438927, 'timers': {'connectors': {'get_actions': 0.0004420302040981728, 'un_batch_to_individual_items': 6.35748462352051e-05, 'tensor_to_numpy': 0.0001176614382826029, 'module_to_agent_unmapping': 6.169515002127069e-06, 'normalize_and_clip_actions': 6.96046676334371e-05, 'listify_data_for_vector_env': 2.3877139120163247e-05, 'remove_single_ts_time_rank_from_batch': 2.21289193694802e-06}}}, 'sample': 87.71670449990779, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.560000000000144, 'blue_0': -32.020000000000174, 'blue_1': -42.640000000000356, 'red_0': -23.960000000000104}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 127.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 768000.0, 'blue_0': 768000.0, 'blue_1': 768000.0, 'red_0': 768000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.560000000000144, 'blue_policy': -42.640000000000356}, 'num_module_steps_sampled_lifetime': {'red_policy': 1536000.0, 'blue_policy': 1536000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033840945184279464, 'episode_return_mean': -126.18000000000079, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.90000000000049, 'episode_duration_sec_mean': 17.425887459982185, 'episode_return_min': -151.5000000000011, 'rlmodule_inference_timer': 0.012356224652790933, 'num_episodes_lifetime': 640.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3857062999159, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.40684332081474, 'throughput_since_last_restore': 15.697561217912146}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 341
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 676
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 933
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1185
(MultiAgentEnvRunner pid=37492) {'red_0': -54.30000000000049, 'red_1': -32.30000000000015, 'blue_0': -34.90000000000023, 'blue_1': -29.600000000000307}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -32.40000000000018, 'red_1': -43.90000000000034, 'blue_0': -44.20000000000035, 'blue_1': -47.300000000000395}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -16.599999999999966, 'red_1': -45.30000000000036, 'blue_0': -50.50000000000043, 'blue_1': -50.700000000000436}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.199999999999994, 'red_1': -10.399999999999979, 'blue_0': -54.70000000000049, 'blue_1': -36.800000000000246}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 348
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 366
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 615
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1075
(MultiAgentEnvRunner pid=37492) {'red_0': -52.200000000000394, 'red_1': -37.80000000000038, 'blue_0': -42.20000000000039, 'blue_1': -22.000000000000018}
ITERATION 128: reward=-148.8600000000011, metadata={'num_env_steps_sampled_lifetime': 774000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003188852130769053, 'timers': {'connectors': {'batch_individual_items': 9.544680520052837e-05, 'add_states_from_episodes_to_batch': 6.3033469105471235e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2062983719927747e-05, 'numpy_to_tensor': 6.641107852635228e-05, 'agent_to_module_mapping': 7.825697157228253e-06, 'add_observations_from_episodes_to_batch': 3.736845140415565e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008447882275243996, 'timers': {'connectors': {'get_actions': 0.00043466807261765536, 'un_batch_to_individual_items': 6.33566372969383e-05, 'tensor_to_numpy': 0.00011595439438130221, 'module_to_agent_unmapping': 6.2028834166272935e-06, 'normalize_and_clip_actions': 6.964183134132665e-05, 'listify_data_for_vector_env': 2.3126241215875262e-05, 'remove_single_ts_time_rank_from_batch': 2.2217321421173418e-06}}}, 'sample': 87.2158752999967, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.94000000000024, 'blue_0': -45.30000000000038, 'blue_1': -37.28000000000028, 'red_0': -32.3400000000002}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 128.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 774000.0, 'blue_0': 774000.0, 'blue_1': 774000.0, 'red_0': 774000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.94000000000024, 'blue_policy': -37.28000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 1548000.0, 'blue_policy': 1548000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003333413671692419, 'episode_return_mean': -148.8600000000011, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.1000000000007, 'episode_duration_sec_mean': 17.32462172000669, 'episode_return_min': -167.80000000000126, 'rlmodule_inference_timer': 0.01229186555196307, 'num_episodes_lifetime': 645.0, 'episode_len_min': 1200, 'time_between_sampling': 256.9756666000467, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.418803104303194, 'throughput_since_last_restore': 15.709594258466833}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -34.90000000000022, 'red_1': -19.000000000000004, 'blue_0': -44.10000000000035, 'blue_1': -56.300000000000516}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 532
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 578
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 640
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 705
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 713
(MultiAgentEnvRunner pid=37492) {'red_0': -49.60000000000031, 'red_1': -70.10000000000018, 'blue_0': -19.99999999999996, 'blue_1': -21.40000000000007}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 526
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 609
(MultiAgentEnvRunner pid=37492) {'red_0': -41.10000000000029, 'red_1': -21.40000000000003, 'blue_0': -41.80000000000032, 'blue_1': -55.800000000000495}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 117
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 134
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 226
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 422
(MultiAgentEnvRunner pid=37492) {'red_0': -39.900000000000276, 'red_1': -22.50000000000002, 'blue_0': -59.30000000000057, 'blue_1': -44.40000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 491
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 602
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 737
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 803
(MultiAgentEnvRunner pid=37492) {'red_0': -61.30000000000049, 'red_1': -60.30000000000048, 'blue_0': -25.800000000000164, 'blue_1': -38.20000000000025}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': -0.1, 'blue_0': -1.3, 'blue_1': -1.8000000000000005}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.4, 'blue_1': -0.4}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 05:03:32,429	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -116.09999999999768, 'blue_0': -0.9999999999999999, 'blue_1': -115.19999999999773}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.89999999999746, 'red_1': -119.69999999999747, 'blue_0': -119.39999999999749, 'blue_1': -119.39999999999749}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.59999999999748, 'red_1': 0, 'blue_0': -118.89999999999752, 'blue_1': -118.39999999999755}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 129: reward=-165.44000000000108, metadata={'num_env_steps_sampled_lifetime': 780000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032596122889128195, 'timers': {'connectors': {'batch_individual_items': 0.00010022543118567, 'add_states_from_episodes_to_batch': 6.443541793614042e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2341444965167798e-05, 'numpy_to_tensor': 6.590016592477995e-05, 'agent_to_module_mapping': 7.99303522395254e-06, 'add_observations_from_episodes_to_batch': 3.812600291236055e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008617411496422598, 'timers': {'connectors': {'get_actions': 0.0004412174046986979, 'un_batch_to_individual_items': 6.545309633046091e-05, 'tensor_to_numpy': 0.00011817866125310829, 'module_to_agent_unmapping': 6.410172264258808e-06, 'normalize_and_clip_actions': 7.120758762009922e-05, 'listify_data_for_vector_env': 2.3461845398019036e-05, 'remove_single_ts_time_rank_from_batch': 2.2658811966184037e-06}}}, 'sample': 91.07183829997666, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -38.66000000000014, 'blue_0': -38.20000000000027, 'blue_1': -43.22000000000034, 'red_0': -45.36000000000031}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 129.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 780000.0, 'blue_0': 780000.0, 'blue_1': 780000.0, 'red_0': 780000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -38.66000000000014, 'blue_policy': -43.22000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 1560000.0, 'blue_policy': 1560000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003495581314009131, 'episode_return_mean': -165.44000000000108, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -154.3000000000011, 'episode_duration_sec_mean': 18.073108580033296, 'episode_return_min': -185.60000000000136, 'rlmodule_inference_timer': 0.012531360176554358, 'num_episodes_lifetime': 650.0, 'episode_len_min': 1200, 'time_between_sampling': 257.24971240002196, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.897197228525956, 'throughput_since_last_restore': 15.693849400553711}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 882
(MultiAgentEnvRunner pid=37492) {'red_0': -18.50000000000001, 'red_1': -17.8, 'blue_0': -35.80000000000031, 'blue_1': -33.400000000000404}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 477
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 494
(MultiAgentEnvRunner pid=37492) {'red_0': -35.100000000000215, 'red_1': -48.9000000000004, 'blue_0': -42.30000000000033, 'blue_1': -37.20000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.199999999999964, 'red_1': -32.600000000000186, 'blue_0': -41.60000000000032, 'blue_1': -47.30000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1082
(MultiAgentEnvRunner pid=37492) {'red_0': -25.800000000000026, 'red_1': -23.300000000000068, 'blue_0': -44.00000000000041, 'blue_1': -27.00000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 570
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 639
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 816
(MultiAgentEnvRunner pid=37492) {'red_0': -54.70000000000041, 'red_1': -54.40000000000042, 'blue_0': -28.800000000000235, 'blue_1': -43.60000000000046}
ITERATION 130: reward=-141.46000000000103, metadata={'num_env_steps_sampled_lifetime': 786000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032504032753377486, 'timers': {'connectors': {'batch_individual_items': 9.674711418798498e-05, 'add_states_from_episodes_to_batch': 6.426027070717529e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2262162538793796e-05, 'numpy_to_tensor': 6.746214542948711e-05, 'agent_to_module_mapping': 8.205312574492497e-06, 'add_observations_from_episodes_to_batch': 3.8211576772988464e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008590091045809864, 'timers': {'connectors': {'get_actions': 0.0004406327006238873, 'un_batch_to_individual_items': 6.504062084953266e-05, 'tensor_to_numpy': 0.00011782450768170325, 'module_to_agent_unmapping': 6.328184275516591e-06, 'normalize_and_clip_actions': 7.134617023191108e-05, 'listify_data_for_vector_env': 2.35729157301138e-05, 'remove_single_ts_time_rank_from_batch': 2.3016400971274507e-06}}}, 'sample': 88.12879640003666, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.40000000000022, 'blue_0': -38.50000000000033, 'blue_1': -37.700000000000365, 'red_0': -29.86000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 130.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 786000.0, 'blue_0': 786000.0, 'blue_1': 786000.0, 'red_0': 786000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.40000000000022, 'blue_policy': -37.700000000000365}, 'num_module_steps_sampled_lifetime': {'red_policy': 1572000.0, 'blue_policy': 1572000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034293235142414597, 'episode_return_mean': -141.46000000000103, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -105.50000000000072, 'episode_duration_sec_mean': 17.501027860026806, 'episode_return_min': -181.50000000000153, 'rlmodule_inference_timer': 0.012412240545026865, 'num_episodes_lifetime': 655.0, 'episode_len_min': 1200, 'time_between_sampling': 340.6698229999747, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.31452066184316, 'throughput_since_last_restore': 15.705070055498878}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 346.4932227000827, 'restore_env_runners': 9.199953638017178e-06, 'training_step': 346.4929841000121, 'env_runner_sampling_timer': 88.26094060007017, 'learner_update_timer': 258.1774718000088, 'synch_weights': 0.011835600016638637, 'synch_env_connectors': 0.0025333999656140804, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 786000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032504032753377486, 'timers': {'connectors': {'batch_individual_items': 9.674711418798498e-05, 'add_states_from_episodes_to_batch': 6.426027070717529e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2262162538793796e-05, 'numpy_to_tensor': 6.746214542948711e-05, 'agent_to_module_mapping': 8.205312574492497e-06, 'add_observations_from_episodes_to_batch': 3.8211576772988464e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008590091045809864, 'timers': {'connectors': {'get_actions': 0.0004406327006238873, 'un_batch_to_individual_items': 6.504062084953266e-05, 'tensor_to_numpy': 0.00011782450768170325, 'module_to_agent_unmapping': 6.328184275516591e-06, 'normalize_and_clip_actions': 7.134617023191108e-05, 'listify_data_for_vector_env': 2.35729157301138e-05, 'remove_single_ts_time_rank_from_batch': 2.3016400971274507e-06}}}, 'sample': 88.12879640003666, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.40000000000022, 'blue_0': -38.50000000000033, 'blue_1': -37.700000000000365, 'red_0': -29.86000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 130.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 786000.0, 'blue_0': 786000.0, 'blue_1': 786000.0, 'red_0': 786000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.40000000000022, 'blue_policy': -37.700000000000365}, 'num_module_steps_sampled_lifetime': {'red_policy': 1572000.0, 'blue_policy': 1572000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034293235142414597, 'episode_return_mean': -141.46000000000103, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -105.50000000000072, 'episode_duration_sec_mean': 17.501027860026806, 'episode_return_min': -181.50000000000153, 'rlmodule_inference_timer': 0.012412240545026865, 'num_episodes_lifetime': 655.0, 'episode_len_min': 1200, 'time_between_sampling': 340.6698229999747, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.31452066184316, 'throughput_since_last_restore': 15.705070055498878}}, 'learners': {'red_policy': {'policy_loss': -0.03571992367506027, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.0102681964635849, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 131.0, 'num_module_steps_trained_lifetime': 47201920.0, 'curr_entropy_coeff': 0.03821, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00022139999999999996, 'vf_explained_var': -0.17718815803527832, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.941607475280762, 'entropy': 1.0500390529632568, 'vf_loss_unclipped': 655.1429443359375, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1039.79918261889, 'throughput_since_last_restore': 943.1418005986479}}, 'blue_policy': {'weights_seq_no': 131.0, 'num_module_steps_trained_lifetime': 47201920.0, 'curr_entropy_coeff': 0.03821, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00022139999999999996, 'vf_explained_var': 0.528967022895813, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 2.2976748943328857, 'total_loss': 0.68650221824646, 'entropy': 1.7172167301177979, 'policy_loss': -0.13493715226650238, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.8648825883865356, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014700416475534439, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1039.7977747310292, 'throughput_since_last_restore': 943.1418020289774}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.600050255656242e-06, 'batch_individual_items': 0.7355463999556378, 'add_time_dim_to_batch_and_zero_pad': 2.2799940779805183e-05, 'numpy_to_tensor': 0.14773830003105104, 'add_observations_from_episodes_to_batch': 0.00024510000366717577, 'agent_to_module_mapping': 0.021985800005495548, 'add_one_ts_to_episodes_and_truncate': 0.17116200004238635, 'add_columns_from_episodes_to_train_batch': 0.46421360003296286, 'general_advantage_estimation': 13.05126099998597}}, 'connector_pipeline_timer': 14.592596000060439}, 'num_module_steps_trained_lifetime': 94403840.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 2212590000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48740.48606165376, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48740.48365646339, 'throughput_since_last_restore': 44209.77203839172}, 'num_module_steps_trained_throughput': 2079.5939477386205, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2079.593914732421, 'throughput_since_last_restore': 1886.283606217586}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 786000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 131, 'trial_id': 'default', 'date': '2026-01-26_05-12-59', 'timestamp': 1769400779, 'time_this_iter_s': 346.50720977783203, 'time_total_s': 50024.911518096924, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 50024.911518096924, 'iterations_since_restore': 131, 'perf': {'cpu_util_percent': 14.779554655870447, 'ram_util_percent': 90.24554655870446}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -20.600000000000026, 'red_1': -43.70000000000034, 'blue_0': -41.30000000000031, 'blue_1': -41.700000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -34.00000000000021, 'red_1': -1.8000000000000005, 'blue_0': -52.800000000000466, 'blue_1': -44.00000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 995
(MultiAgentEnvRunner pid=37492) {'red_0': -9.999999999999995, 'red_1': -31.700000000000113, 'blue_0': -38.60000000000034, 'blue_1': -21.600000000000232}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.499999999999993, 'red_1': -7.19999999999999, 'blue_0': -44.300000000000345, 'blue_1': -62.10000000000059}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 759
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 786
(MultiAgentEnvRunner pid=37492) {'red_0': -57.300000000000445, 'red_1': -8.9, 'blue_0': -34.70000000000029, 'blue_1': -23.500000000000146}
ITERATION 131: reward=-125.2600000000009, metadata={'num_env_steps_sampled_lifetime': 792000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003175289595940577, 'timers': {'connectors': {'batch_individual_items': 9.479393252268896e-05, 'add_states_from_episodes_to_batch': 6.171813226195307e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1881093740536213e-05, 'numpy_to_tensor': 6.650260723166525e-05, 'agent_to_module_mapping': 7.696974011725582e-06, 'add_observations_from_episodes_to_batch': 3.789166296080648e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000850498729755356, 'timers': {'connectors': {'get_actions': 0.0004412687211454113, 'un_batch_to_individual_items': 6.380223897580717e-05, 'tensor_to_numpy': 0.00011447626697163349, 'module_to_agent_unmapping': 6.171189880480576e-06, 'normalize_and_clip_actions': 6.94609727393516e-05, 'listify_data_for_vector_env': 2.2838198408778357e-05, 'remove_single_ts_time_rank_from_batch': 2.254902348568783e-06}}}, 'sample': 88.50186940003186, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.66000000000009, 'blue_0': -42.34000000000035, 'blue_1': -38.580000000000325, 'red_0': -25.68000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 131.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 792000.0, 'blue_0': 792000.0, 'blue_1': 792000.0, 'red_0': 792000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.66000000000009, 'blue_policy': -38.580000000000325}, 'num_module_steps_sampled_lifetime': {'red_policy': 1584000.0, 'blue_policy': 1584000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003388491214511566, 'episode_return_mean': -125.2600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.90000000000069, 'episode_duration_sec_mean': 17.57925093998201, 'episode_return_min': -147.30000000000098, 'rlmodule_inference_timer': 0.012431842403311766, 'num_episodes_lifetime': 660.0, 'episode_len_min': 1200, 'time_between_sampling': 258.47474460001104, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.259681992933768, 'throughput_since_last_restore': 15.715793211649338}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 315
(MultiAgentEnvRunner pid=37492) {'red_0': -27.900000000000123, 'red_1': -13.299999999999972, 'blue_0': -54.70000000000049, 'blue_1': -35.600000000000236}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 863
(MultiAgentEnvRunner pid=37492) {'red_0': -24.600000000000083, 'red_1': -16.799999999999972, 'blue_0': -60.90000000000057, 'blue_1': -40.50000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 894
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1173
(MultiAgentEnvRunner pid=37492) {'red_0': -39.70000000000022, 'red_1': -23.80000000000007, 'blue_0': -29.70000000000025, 'blue_1': -27.70000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -26.000000000000103, 'red_1': -21.300000000000033, 'blue_0': -55.70000000000051, 'blue_1': -37.20000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 349
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 433
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 693
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 693
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1002
(MultiAgentEnvRunner pid=37492) {'red_0': -24.700000000000127, 'red_1': -8.89999999999996, 'blue_0': -51.000000000000384, 'blue_1': -53.90000000000037}
ITERATION 132: reward=-134.78000000000088, metadata={'num_env_steps_sampled_lifetime': 798000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003440407352743154, 'timers': {'connectors': {'batch_individual_items': 9.733441043142127e-05, 'add_states_from_episodes_to_batch': 6.909478976164347e-06, 'add_time_dim_to_batch_and_zero_pad': 1.326821445687444e-05, 'numpy_to_tensor': 7.210163139962733e-05, 'agent_to_module_mapping': 8.624198917556306e-06, 'add_observations_from_episodes_to_batch': 4.1165535212797295e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000932914327424486, 'timers': {'connectors': {'get_actions': 0.0004765819005611209, 'un_batch_to_individual_items': 7.08817426278879e-05, 'tensor_to_numpy': 0.00012624171151179363, 'module_to_agent_unmapping': 6.9066374752162125e-06, 'normalize_and_clip_actions': 7.757392692622092e-05, 'listify_data_for_vector_env': 2.5850195762033653e-05, 'remove_single_ts_time_rank_from_batch': 2.5500009150408475e-06}}}, 'sample': 88.60249459999613, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -16.82, 'blue_0': -50.400000000000446, 'blue_1': -38.980000000000295, 'red_0': -28.580000000000133}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 132.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 798000.0, 'blue_0': 798000.0, 'blue_1': 798000.0, 'red_0': 798000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -16.82, 'blue_policy': -38.980000000000295}, 'num_module_steps_sampled_lifetime': {'red_policy': 1596000.0, 'blue_policy': 1596000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036345433669615255, 'episode_return_mean': -134.78000000000088, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -120.90000000000084, 'episode_duration_sec_mean': 17.58586393999867, 'episode_return_min': -142.80000000000095, 'rlmodule_inference_timer': 0.01389240010003661, 'num_episodes_lifetime': 665.0, 'episode_len_min': 1200, 'time_between_sampling': 259.0598124000244, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.295373757556337, 'throughput_since_last_restore': 15.726591778326426}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 962
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 976
(MultiAgentEnvRunner pid=37492) {'red_0': -26.400000000000126, 'red_1': -19.80000000000002, 'blue_0': -39.900000000000496, 'blue_1': -38.50000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 432
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 969
(MultiAgentEnvRunner pid=37492) {'red_0': -40.90000000000016, 'red_1': -21.299999999999983, 'blue_0': -25.60000000000013, 'blue_1': -30.50000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 552
(MultiAgentEnvRunner pid=37492) {'red_0': -25.200000000000077, 'red_1': -15.999999999999977, 'blue_0': -43.30000000000034, 'blue_1': -25.600000000000055}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -49.700000000000415, 'red_1': -15.199999999999962, 'blue_0': -50.30000000000043, 'blue_1': -47.9000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1075
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1148
(MultiAgentEnvRunner pid=37492) {'red_0': -23.200000000000017, 'red_1': -33.10000000000016, 'blue_0': -39.80000000000033, 'blue_1': -30.800000000000338}
ITERATION 133: reward=-128.60000000000085, metadata={'num_env_steps_sampled_lifetime': 804000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032328236574492375, 'timers': {'connectors': {'batch_individual_items': 9.663166952182275e-05, 'add_states_from_episodes_to_batch': 6.331938255129121e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2004439999621487e-05, 'numpy_to_tensor': 6.703637706182549e-05, 'agent_to_module_mapping': 8.09180204020381e-06, 'add_observations_from_episodes_to_batch': 3.8486412777124165e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008644707692985081, 'timers': {'connectors': {'get_actions': 0.00044800451019891735, 'un_batch_to_individual_items': 6.41796215822872e-05, 'tensor_to_numpy': 0.00011758424869809134, 'module_to_agent_unmapping': 6.403998661143191e-06, 'normalize_and_clip_actions': 7.098813915920694e-05, 'listify_data_for_vector_env': 2.3217519079764365e-05, 'remove_single_ts_time_rank_from_batch': 2.3027933996990017e-06}}}, 'sample': 88.99922050000168, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.08000000000002, 'blue_0': -39.78000000000034, 'blue_1': -34.66000000000032, 'red_0': -33.08000000000016}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 133.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 804000.0, 'blue_0': 804000.0, 'blue_1': 804000.0, 'red_0': 804000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.08000000000002, 'blue_policy': -34.66000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 1608000.0, 'blue_policy': 1608000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003401769527121336, 'episode_return_mean': -128.60000000000085, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -110.10000000000045, 'episode_duration_sec_mean': 17.67595617999323, 'episode_return_min': -163.10000000000122, 'rlmodule_inference_timer': 0.01275316316439205, 'num_episodes_lifetime': 670.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3191526000155, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.27605903793206, 'throughput_since_last_restore': 15.737123822991316}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 349
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 785
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 978
(MultiAgentEnvRunner pid=37492) {'red_0': -38.00000000000012, 'red_1': -28.500000000000043, 'blue_0': -34.200000000000486, 'blue_1': -37.400000000000524}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 208
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1100
(MultiAgentEnvRunner pid=37492) {'red_0': -64.90000000000049, 'red_1': -52.9000000000004, 'blue_0': -27.400000000000333, 'blue_1': -39.10000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 344
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 488
(MultiAgentEnvRunner pid=37492) {'red_0': -27.700000000000163, 'red_1': -12.299999999999983, 'blue_0': -29.800000000000143, 'blue_1': -34.70000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 676
(MultiAgentEnvRunner pid=37492) {'red_0': -30.600000000000144, 'red_1': -15.999999999999922, 'blue_0': -44.10000000000028, 'blue_1': -39.900000000000226}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 539
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 691
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 744
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 820
(MultiAgentEnvRunner pid=37492) {'red_0': -60.700000000000564, 'red_1': -20.100000000000048, 'blue_0': -21.900000000000013, 'blue_1': -25.800000000000058}
ITERATION 134: reward=-137.2000000000009, metadata={'num_env_steps_sampled_lifetime': 810000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032889931065980816, 'timers': {'connectors': {'batch_individual_items': 9.782445681034915e-05, 'add_states_from_episodes_to_batch': 6.532584027985095e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2301959133273073e-05, 'numpy_to_tensor': 6.750004372392345e-05, 'agent_to_module_mapping': 8.156721921427036e-06, 'add_observations_from_episodes_to_batch': 3.8982248790572664e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008775230064840772, 'timers': {'connectors': {'get_actions': 0.00045305447754369815, 'un_batch_to_individual_items': 6.587568232803175e-05, 'tensor_to_numpy': 0.0001188982555614978, 'module_to_agent_unmapping': 6.533727547535002e-06, 'normalize_and_clip_actions': 7.281798106684692e-05, 'listify_data_for_vector_env': 2.4058103347625517e-05, 'remove_single_ts_time_rank_from_batch': 2.3218508642865663e-06}}}, 'sample': 89.0954219000414, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.960000000000075, 'blue_0': -31.480000000000253, 'blue_1': -35.380000000000265, 'red_0': -44.380000000000294}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 134.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 810000.0, 'blue_0': 810000.0, 'blue_1': 810000.0, 'red_0': 810000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.960000000000075, 'blue_policy': -35.380000000000265}, 'num_module_steps_sampled_lifetime': {'red_policy': 1620000.0, 'blue_policy': 1620000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034476113281573916, 'episode_return_mean': -137.2000000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -104.50000000000047, 'episode_duration_sec_mean': 17.68320249998942, 'episode_return_min': -184.30000000000157, 'rlmodule_inference_timer': 0.012662760580177464, 'num_episodes_lifetime': 675.0, 'episode_len_min': 1200, 'time_between_sampling': 258.30046910000965, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.243151881583028, 'throughput_since_last_restore': 15.747310991388488}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 915
(MultiAgentEnvRunner pid=37492) {'red_0': -12.499999999999977, 'red_1': -13.399999999999975, 'blue_0': -33.50000000000028, 'blue_1': -27.700000000000276}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 861
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 875
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 919
(MultiAgentEnvRunner pid=37492) {'red_0': -30.500000000000092, 'red_1': -28.60000000000015, 'blue_0': -49.600000000000485, 'blue_1': -16.900000000000098}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 632
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 999
(MultiAgentEnvRunner pid=37492) {'red_0': -0.10000000000000142, 'red_1': -20.800000000000008, 'blue_0': -48.70000000000037, 'blue_1': -52.00000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -16.899999999999974, 'red_1': -5.999999999999995, 'blue_0': -47.8000000000004, 'blue_1': -68.80000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 581
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1153
(MultiAgentEnvRunner pid=37492) {'red_0': -41.80000000000023, 'red_1': -22.100000000000062, 'blue_0': -27.80000000000019, 'blue_1': -39.9000000000003}
ITERATION 135: reward=-121.0800000000007, metadata={'num_env_steps_sampled_lifetime': 816000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032315437890334975, 'timers': {'connectors': {'batch_individual_items': 9.557865300961125e-05, 'add_states_from_episodes_to_batch': 6.399523659287874e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2141309178127e-05, 'numpy_to_tensor': 6.650680482747271e-05, 'agent_to_module_mapping': 8.160134972124782e-06, 'add_observations_from_episodes_to_batch': 3.855879361612155e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008649784567657559, 'timers': {'connectors': {'get_actions': 0.0004474342916480923, 'un_batch_to_individual_items': 6.448802902699958e-05, 'tensor_to_numpy': 0.00011712770120366304, 'module_to_agent_unmapping': 6.400383469178062e-06, 'normalize_and_clip_actions': 7.200517941273819e-05, 'listify_data_for_vector_env': 2.351243499718019e-05, 'remove_single_ts_time_rank_from_batch': 2.344802842804477e-06}}}, 'sample': 89.3836164000677, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.180000000000042, 'blue_0': -41.480000000000345, 'blue_1': -41.06000000000028, 'red_0': -20.360000000000053}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 135.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 816000.0, 'blue_0': 816000.0, 'blue_1': 816000.0, 'red_0': 816000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.180000000000042, 'blue_policy': -41.06000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 1632000.0, 'blue_policy': 1632000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034570012003613, 'episode_return_mean': -121.0800000000007, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -87.1000000000005, 'episode_duration_sec_mean': 17.750530560011974, 'episode_return_min': -139.50000000000068, 'rlmodule_inference_timer': 0.012615944300598706, 'num_episodes_lifetime': 680.0, 'episode_len_min': 1200, 'time_between_sampling': 258.8704396999674, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.242187750947114, 'throughput_since_last_restore': 15.757355480164447}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.699999999999989, 'red_1': -21.300000000000036, 'blue_0': -44.10000000000036, 'blue_1': -45.000000000000355}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 450
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 465
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 861
(MultiAgentEnvRunner pid=37492) {'red_0': -33.00000000000014, 'red_1': -41.600000000000215, 'blue_0': -19.299999999999958, 'blue_1': -22.800000000000104}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.299999999999963, 'red_1': -18.299999999999994, 'blue_0': -36.00000000000024, 'blue_1': -42.80000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 344
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 690
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1162
(MultiAgentEnvRunner pid=37492) {'red_0': -27.200000000000085, 'red_1': -33.80000000000035, 'blue_0': -24.500000000000046, 'blue_1': -38.60000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 305
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 721
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 721
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 873
(MultiAgentEnvRunner pid=37492) {'red_0': -28.700000000000415, 'red_1': -26.500000000000224, 'blue_0': -45.40000000000025, 'blue_1': -37.50000000000018}
ITERATION 136: reward=-121.8800000000007, metadata={'num_env_steps_sampled_lifetime': 822000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032132234424111833, 'timers': {'connectors': {'batch_individual_items': 9.61978885045211e-05, 'add_states_from_episodes_to_batch': 6.353069094569218e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2325321305258383e-05, 'numpy_to_tensor': 6.573826357700178e-05, 'agent_to_module_mapping': 7.867594329830977e-06, 'add_observations_from_episodes_to_batch': 3.8025017716144364e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008611730902690236, 'timers': {'connectors': {'get_actions': 0.000445808311684647, 'un_batch_to_individual_items': 6.415097377046654e-05, 'tensor_to_numpy': 0.00011672908438163883, 'module_to_agent_unmapping': 6.427574863700945e-06, 'normalize_and_clip_actions': 7.064037433872883e-05, 'listify_data_for_vector_env': 2.3584985499865436e-05, 'remove_single_ts_time_rank_from_batch': 2.226433561442794e-06}}}, 'sample': 87.80727670004126, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.300000000000164, 'blue_0': -33.86000000000017, 'blue_1': -37.340000000000245, 'red_0': -22.380000000000116}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 136.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 822000.0, 'blue_0': 822000.0, 'blue_1': 822000.0, 'red_0': 822000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.300000000000164, 'blue_policy': -37.340000000000245}, 'num_module_steps_sampled_lifetime': {'red_policy': 1644000.0, 'blue_policy': 1644000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003427921048341766, 'episode_return_mean': -121.8800000000007, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -112.40000000000053, 'episode_duration_sec_mean': 17.44380546002649, 'episode_return_min': -138.10000000000107, 'rlmodule_inference_timer': 0.012371835405839296, 'num_episodes_lifetime': 685.0, 'episode_len_min': 1200, 'time_between_sampling': 258.6047871999908, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.33993039616804, 'throughput_since_last_restore': 15.767859154643277}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -37.400000000000254, 'red_1': -20.800000000000026, 'blue_0': -47.20000000000039, 'blue_1': -57.50000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -3.2000000000000015, 'red_1': -4.000000000000002, 'blue_0': -51.70000000000046, 'blue_1': -52.800000000000466}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 677
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 810
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 814
(MultiAgentEnvRunner pid=37492) {'red_0': -41.50000000000029, 'red_1': -45.300000000000345, 'blue_0': -39.10000000000031, 'blue_1': -49.40000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 538
(MultiAgentEnvRunner pid=37492) {'red_0': -23.40000000000008, 'red_1': -44.30000000000028, 'blue_0': -36.00000000000023, 'blue_1': -37.60000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 691
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1041
(MultiAgentEnvRunner pid=37492) {'red_0': -35.40000000000026, 'red_1': -31.100000000000136, 'blue_0': -43.500000000000306, 'blue_1': -52.6000000000004}
ITERATION 137: reward=-150.76000000000107, metadata={'num_env_steps_sampled_lifetime': 828000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003169084629820655, 'timers': {'connectors': {'batch_individual_items': 9.450389643767583e-05, 'add_states_from_episodes_to_batch': 6.123036923075374e-06, 'add_time_dim_to_batch_and_zero_pad': 1.195919380106728e-05, 'numpy_to_tensor': 6.519783174826124e-05, 'agent_to_module_mapping': 7.834259376274885e-06, 'add_observations_from_episodes_to_batch': 3.775090500269308e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000848800342039981, 'timers': {'connectors': {'get_actions': 0.00043975153030685854, 'un_batch_to_individual_items': 6.244675505971902e-05, 'tensor_to_numpy': 0.00011511181235284934, 'module_to_agent_unmapping': 6.203152247984382e-06, 'normalize_and_clip_actions': 6.989935469763558e-05, 'listify_data_for_vector_env': 2.3377602586990153e-05, 'remove_single_ts_time_rank_from_batch': 2.284916540710395e-06}}}, 'sample': 87.52881080005318, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.100000000000158, 'blue_0': -43.500000000000334, 'blue_1': -49.98000000000043, 'red_0': -28.180000000000177}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 137.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 828000.0, 'blue_0': 828000.0, 'blue_1': 828000.0, 'red_0': 828000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.100000000000158, 'blue_policy': -49.98000000000043}, 'num_module_steps_sampled_lifetime': {'red_policy': 1656000.0, 'blue_policy': 1656000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000335493209254609, 'episode_return_mean': -150.76000000000107, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.70000000000093, 'episode_duration_sec_mean': 17.364890979975463, 'episode_return_min': -175.3000000000014, 'rlmodule_inference_timer': 0.012317654783390395, 'num_episodes_lifetime': 690.0, 'episode_len_min': 1200, 'time_between_sampling': 258.21395260002464, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.30389634862553, 'throughput_since_last_restore': 15.778007506211148}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.99999999999996, 'red_1': -29.60000000000015, 'blue_0': -55.8000000000005, 'blue_1': -39.20000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 266
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 520
(MultiAgentEnvRunner pid=37492) {'red_0': -28.50000000000012, 'red_1': -8.399999999999967, 'blue_0': -41.30000000000036, 'blue_1': -66.4000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.299999999999962, 'red_1': -16.699999999999967, 'blue_0': -41.600000000000314, 'blue_1': -53.50000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 202
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 263
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 519
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 908
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 908
(MultiAgentEnvRunner pid=37492) {'red_0': -42.2000000000004, 'red_1': -35.200000000000216, 'blue_0': -25.500000000000085, 'blue_1': -42.700000000000294}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 920
(MultiAgentEnvRunner pid=37492) {'red_0': -11.099999999999993, 'red_1': -14.99999999999998, 'blue_0': -33.50000000000028, 'blue_1': -30.200000000000365}
ITERATION 138: reward=-129.54000000000082, metadata={'num_env_steps_sampled_lifetime': 834000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031781816842555333, 'timers': {'connectors': {'batch_individual_items': 9.42396627132582e-05, 'add_states_from_episodes_to_batch': 6.258285606540889e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2100479948492386e-05, 'numpy_to_tensor': 6.635866954413192e-05, 'agent_to_module_mapping': 7.765772388412773e-06, 'add_observations_from_episodes_to_batch': 3.7592564966817715e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008539791417924812, 'timers': {'connectors': {'get_actions': 0.00043837758547762357, 'un_batch_to_individual_items': 6.469551632514151e-05, 'tensor_to_numpy': 0.00011599371164156208, 'module_to_agent_unmapping': 6.353966157781906e-06, 'normalize_and_clip_actions': 7.13532015102982e-05, 'listify_data_for_vector_env': 2.3840724293659034e-05, 'remove_single_ts_time_rank_from_batch': 2.300612738238179e-06}}}, 'sample': 88.91279390000273, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -20.980000000000054, 'blue_0': -39.54000000000031, 'blue_1': -46.40000000000039, 'red_0': -22.620000000000086}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 138.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 834000.0, 'blue_0': 834000.0, 'blue_1': 834000.0, 'red_0': 834000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -20.980000000000054, 'blue_policy': -46.40000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 1668000.0, 'blue_policy': 1668000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003392805162456191, 'episode_return_mean': -129.54000000000082, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -89.80000000000061, 'episode_duration_sec_mean': 17.65350802000612, 'episode_return_min': -145.600000000001, 'rlmodule_inference_timer': 0.012420061797809358, 'num_episodes_lifetime': 695.0, 'episode_len_min': 1200, 'time_between_sampling': 259.21611170005053, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.27289334191209, 'throughput_since_last_restore': 15.787836795702136}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -44.80000000000035, 'red_1': -12.89999999999997, 'blue_0': -58.70000000000054, 'blue_1': -37.600000000000264}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1149
(MultiAgentEnvRunner pid=37492) {'red_0': -25.900000000000027, 'red_1': -25.900000000000027, 'blue_0': -22.90000000000027, 'blue_1': -33.900000000000276}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.899999999999963, 'red_1': -22.500000000000053, 'blue_0': -48.00000000000041, 'blue_1': -54.40000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1009
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1022
(MultiAgentEnvRunner pid=37492) {'red_0': -51.30000000000037, 'red_1': -20.300000000000036, 'blue_0': -38.600000000000364, 'blue_1': -32.900000000000404}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 775
(MultiAgentEnvRunner pid=37492) {'red_0': -31.800000000000182, 'red_1': -46.0000000000004, 'blue_0': -43.90000000000033, 'blue_1': -40.1000000000003}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.4, 'red_1': -1.7000000000000004, 'blue_0': -1.2, 'blue_1': -7.399999999999991}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -6.699999999999992, 'red_1': -0.1, 'blue_0': -6.499999999999993, 'blue_1': -0.2}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 06:02:45,685	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.7999999999999999, 'blue_1': -117.09999999999762}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -1.5000000000000002, 'blue_0': -0.6, 'blue_1': -0.30000000000000004}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.49999999999748, 'red_1': -118.89999999999752, 'blue_0': -118.79999999999752, 'blue_1': -118.79999999999752}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 139: reward=-141.66000000000102, metadata={'num_env_steps_sampled_lifetime': 840000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003252304774659836, 'timers': {'connectors': {'batch_individual_items': 9.554867996859476e-05, 'add_states_from_episodes_to_batch': 6.4419614554639885e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2426017763091256e-05, 'numpy_to_tensor': 6.761052891911623e-05, 'agent_to_module_mapping': 8.066746763663358e-06, 'add_observations_from_episodes_to_batch': 3.902929505412185e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008628241721574424, 'timers': {'connectors': {'get_actions': 0.0004419113842796897, 'un_batch_to_individual_items': 6.544094026633801e-05, 'tensor_to_numpy': 0.00011864779151357177, 'module_to_agent_unmapping': 6.337524515826106e-06, 'normalize_and_clip_actions': 7.113114212933629e-05, 'listify_data_for_vector_env': 2.4088741353165347e-05, 'remove_single_ts_time_rank_from_batch': 2.4004896150928927e-06}}}, 'sample': 88.03934980009217, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.5200000000001, 'blue_0': -42.42000000000038, 'blue_1': -39.78000000000035, 'red_0': -33.94000000000018}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 139.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 840000.0, 'blue_0': 840000.0, 'blue_1': 840000.0, 'red_0': 840000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.5200000000001, 'blue_policy': -39.78000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 1680000.0, 'blue_policy': 1680000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033431898097631787, 'episode_return_mean': -141.66000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.6000000000006, 'episode_duration_sec_mean': 17.483671160019004, 'episode_return_min': -161.8000000000012, 'rlmodule_inference_timer': 0.012551814705355244, 'num_episodes_lifetime': 700.0, 'episode_len_min': 1200, 'time_between_sampling': 258.4561703999061, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.01312252833693, 'throughput_since_last_restore': 15.773566908817578}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 475
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 623
(MultiAgentEnvRunner pid=37492) {'red_0': -22.100000000000094, 'red_1': -36.20000000000019, 'blue_0': -38.70000000000024, 'blue_1': -33.80000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 263
(MultiAgentEnvRunner pid=37492) {'red_0': -39.10000000000027, 'red_1': -2.800000000000001, 'blue_0': -45.00000000000036, 'blue_1': -37.10000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 455
(MultiAgentEnvRunner pid=37492) {'red_0': -55.60000000000043, 'red_1': -19.300000000000015, 'blue_0': -30.300000000000153, 'blue_1': -36.200000000000216}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 418
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 590
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 649
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 874
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1035
(MultiAgentEnvRunner pid=37492) {'red_0': -40.600000000000286, 'red_1': -44.60000000000031, 'blue_0': -53.200000000000536, 'blue_1': -30.90000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 364
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 883
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 891
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1147
(MultiAgentEnvRunner pid=37492) {'red_0': -45.40000000000035, 'red_1': -37.20000000000024, 'blue_0': -37.0000000000003, 'blue_1': -33.80000000000024}
ITERATION 140: reward=-143.780000000001, metadata={'num_env_steps_sampled_lifetime': 846000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003201587963829906, 'timers': {'connectors': {'batch_individual_items': 9.608959945243364e-05, 'add_states_from_episodes_to_batch': 6.240346301489504e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2034235853669245e-05, 'numpy_to_tensor': 6.712788799580525e-05, 'agent_to_module_mapping': 7.816258879037178e-06, 'add_observations_from_episodes_to_batch': 3.745967105416305e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008472031513996109, 'timers': {'connectors': {'get_actions': 0.00043525049482199293, 'un_batch_to_individual_items': 6.285250421196598e-05, 'tensor_to_numpy': 0.00011618015400190443, 'module_to_agent_unmapping': 6.323357365825449e-06, 'normalize_and_clip_actions': 7.074590073760987e-05, 'listify_data_for_vector_env': 2.321181549223098e-05, 'remove_single_ts_time_rank_from_batch': 2.313864662834404e-06}}}, 'sample': 87.8852056999458, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.020000000000152, 'blue_0': -40.840000000000316, 'blue_1': -34.360000000000255, 'red_0': -40.56000000000029}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 140.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 846000.0, 'blue_0': 846000.0, 'blue_1': 846000.0, 'red_0': 846000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.020000000000152, 'blue_policy': -34.360000000000255}, 'num_module_steps_sampled_lifetime': {'red_policy': 1692000.0, 'blue_policy': 1692000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032966056767522775, 'episode_return_mean': -143.780000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -124.00000000000088, 'episode_duration_sec_mean': 17.441001060022973, 'episode_return_min': -169.3000000000015, 'rlmodule_inference_timer': 0.012270147428041066, 'num_episodes_lifetime': 705.0, 'episode_len_min': 1200, 'time_between_sampling': 340.1329838000238, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.36442217379986, 'throughput_since_last_restore': 15.783821842920167}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 345.49790940003004, 'restore_env_runners': 1.1699972674250603e-05, 'training_step': 345.4976605999982, 'env_runner_sampling_timer': 88.02968200005125, 'learner_update_timer': 257.4106947000837, 'synch_weights': 0.012402000022120774, 'synch_env_connectors': 0.0020759999752044678, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 846000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003201587963829906, 'timers': {'connectors': {'batch_individual_items': 9.608959945243364e-05, 'add_states_from_episodes_to_batch': 6.240346301489504e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2034235853669245e-05, 'numpy_to_tensor': 6.712788799580525e-05, 'agent_to_module_mapping': 7.816258879037178e-06, 'add_observations_from_episodes_to_batch': 3.745967105416305e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008472031513996109, 'timers': {'connectors': {'get_actions': 0.00043525049482199293, 'un_batch_to_individual_items': 6.285250421196598e-05, 'tensor_to_numpy': 0.00011618015400190443, 'module_to_agent_unmapping': 6.323357365825449e-06, 'normalize_and_clip_actions': 7.074590073760987e-05, 'listify_data_for_vector_env': 2.321181549223098e-05, 'remove_single_ts_time_rank_from_batch': 2.313864662834404e-06}}}, 'sample': 87.8852056999458, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.020000000000152, 'blue_0': -40.840000000000316, 'blue_1': -34.360000000000255, 'red_0': -40.56000000000029}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 140.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 846000.0, 'blue_0': 846000.0, 'blue_1': 846000.0, 'red_0': 846000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.020000000000152, 'blue_policy': -34.360000000000255}, 'num_module_steps_sampled_lifetime': {'red_policy': 1692000.0, 'blue_policy': 1692000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032966056767522775, 'episode_return_mean': -143.780000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -124.00000000000088, 'episode_duration_sec_mean': 17.441001060022973, 'episode_return_min': -169.3000000000015, 'rlmodule_inference_timer': 0.012270147428041066, 'num_episodes_lifetime': 705.0, 'episode_len_min': 1200, 'time_between_sampling': 340.1329838000238, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.36442217379986, 'throughput_since_last_restore': 15.783821842920167}}, 'learners': {'red_policy': {'policy_loss': -0.19486209750175476, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.009607421234250069, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 141.0, 'num_module_steps_trained_lifetime': 50805120.0, 'curr_entropy_coeff': 0.03731, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00021539999999999998, 'vf_explained_var': -0.48393845558166504, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.787589073181152, 'entropy': 0.9081516265869141, 'vf_loss_unclipped': 697.7135009765625, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1042.7949608948963, 'throughput_since_last_restore': 947.8711072888503}}, 'blue_policy': {'weights_seq_no': 141.0, 'num_module_steps_trained_lifetime': 50805120.0, 'curr_entropy_coeff': 0.03731, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00021539999999999998, 'vf_explained_var': 0.7417545318603516, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 1.431506633758545, 'total_loss': 0.2023041546344757, 'entropy': 1.6994601488113403, 'policy_loss': -0.25980645418167114, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.5010785460472107, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.016192186623811722, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1042.793499010699, 'throughput_since_last_restore': 947.8711088857543}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 8.300063200294971e-06, 'batch_individual_items': 0.6538387999171391, 'add_time_dim_to_batch_and_zero_pad': 2.2600055672228336e-05, 'numpy_to_tensor': 0.12762179994024336, 'add_observations_from_episodes_to_batch': 0.0003626999678090215, 'agent_to_module_mapping': 0.01982679998036474, 'add_one_ts_to_episodes_and_truncate': 0.17071620002388954, 'add_columns_from_episodes_to_train_batch': 0.517207800061442, 'general_advantage_estimation': 13.19534219999332}}, 'connector_pipeline_timer': 14.685369899962097}, 'num_module_steps_trained_lifetime': 101610240.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 2381490000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48880.92277316742, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48880.919576066204, 'throughput_since_last_restore': 44431.458304206186}, 'num_module_steps_trained_throughput': 2085.5858644896584, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2085.5858125810328, 'throughput_since_last_restore': 1895.7422201695151}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 846000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 141, 'trial_id': 'default', 'date': '2026-01-26_06-12-11', 'timestamp': 1769404331, 'time_this_iter_s': 345.513578414917, 'time_total_s': 53576.25288581848, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 53576.25288581848, 'iterations_since_restore': 141, 'perf': {'cpu_util_percent': 14.965720081135903, 'ram_util_percent': 89.54523326572009}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1000
(MultiAgentEnvRunner pid=37492) {'red_0': -13.199999999999969, 'red_1': -27.70000000000012, 'blue_0': -37.40000000000029, 'blue_1': -44.60000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1040
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1197
(MultiAgentEnvRunner pid=37492) {'red_0': -21.29999999999996, 'red_1': -42.80000000000029, 'blue_0': -22.700000000000266, 'blue_1': -56.40000000000057}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -40.200000000000294, 'red_1': -33.80000000000021, 'blue_0': -41.50000000000032, 'blue_1': -42.90000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 751
(MultiAgentEnvRunner pid=37492) {'red_0': -8.599999999999978, 'red_1': -2.600000000000001, 'blue_0': -45.20000000000035, 'blue_1': -55.20000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 831
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 891
(MultiAgentEnvRunner pid=37492) {'red_0': -5.699999999999996, 'red_1': -13.199999999999955, 'blue_0': -56.30000000000051, 'blue_1': -46.400000000000375}
ITERATION 141: reward=-131.54000000000093, metadata={'num_env_steps_sampled_lifetime': 852000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003300727415846277, 'timers': {'connectors': {'batch_individual_items': 9.760514629364273e-05, 'add_states_from_episodes_to_batch': 6.4729973315536015e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2618708529012569e-05, 'numpy_to_tensor': 6.877669214552914e-05, 'agent_to_module_mapping': 8.281393513787395e-06, 'add_observations_from_episodes_to_batch': 3.8804588862797984e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008693703700840891, 'timers': {'connectors': {'get_actions': 0.00044587544701283754, 'un_batch_to_individual_items': 6.632586780758087e-05, 'tensor_to_numpy': 0.00011786994585481611, 'module_to_agent_unmapping': 6.51908908800913e-06, 'normalize_and_clip_actions': 7.208759418945305e-05, 'listify_data_for_vector_env': 2.4326932155336383e-05, 'remove_single_ts_time_rank_from_batch': 2.3301478092110485e-06}}}, 'sample': 88.01968070003204, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -24.020000000000117, 'blue_0': -40.620000000000346, 'blue_1': -49.10000000000043, 'red_0': -17.80000000000004}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 141.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 852000.0, 'blue_0': 852000.0, 'blue_1': 852000.0, 'red_0': 852000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -24.020000000000117, 'blue_policy': -49.10000000000043}, 'num_module_steps_sampled_lifetime': {'red_policy': 1704000.0, 'blue_policy': 1704000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034363989471787473, 'episode_return_mean': -131.54000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.60000000000082, 'episode_duration_sec_mean': 17.478539739968255, 'episode_return_min': -158.40000000000117, 'rlmodule_inference_timer': 0.012537165334805632, 'num_episodes_lifetime': 710.0, 'episode_len_min': 1200, 'time_between_sampling': 257.7443715999834, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.323417209905532, 'throughput_since_last_restore': 15.79370586913142}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 532
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 973
(MultiAgentEnvRunner pid=37492) {'red_0': -28.400000000000144, 'red_1': -30.600000000000307, 'blue_0': -42.200000000000394, 'blue_1': -37.70000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 415
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 574
(MultiAgentEnvRunner pid=37492) {'red_0': -50.100000000000364, 'red_1': -77.29999999999978, 'blue_0': -23.50000000000003, 'blue_1': -37.00000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 132
(MultiAgentEnvRunner pid=37492) {'red_0': -22.100000000000048, 'red_1': -51.600000000000456, 'blue_0': -43.90000000000035, 'blue_1': -42.00000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 855
(MultiAgentEnvRunner pid=37492) {'red_0': -3.4999999999999987, 'red_1': -6.399999999999993, 'blue_0': -47.40000000000038, 'blue_1': -49.90000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 309
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 699
(MultiAgentEnvRunner pid=37492) {'red_0': -40.40000000000025, 'red_1': -58.100000000000485, 'blue_0': -31.70000000000013, 'blue_1': -16.39999999999994}
ITERATION 142: reward=-148.04000000000087, metadata={'num_env_steps_sampled_lifetime': 858000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031848766362360144, 'timers': {'connectors': {'batch_individual_items': 9.519793558426263e-05, 'add_states_from_episodes_to_batch': 6.313907369382955e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2040371904503628e-05, 'numpy_to_tensor': 6.633124783074125e-05, 'agent_to_module_mapping': 7.98503205226749e-06, 'add_observations_from_episodes_to_batch': 3.699331208204682e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008443790443066986, 'timers': {'connectors': {'get_actions': 0.00043654566745448913, 'un_batch_to_individual_items': 6.287253699315654e-05, 'tensor_to_numpy': 0.00011491471291511891, 'module_to_agent_unmapping': 6.2442507823371175e-06, 'normalize_and_clip_actions': 6.928514220539137e-05, 'listify_data_for_vector_env': 2.3083187435712082e-05, 'remove_single_ts_time_rank_from_batch': 2.3517467457171755e-06}}}, 'sample': 87.86186049995013, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -44.8000000000002, 'blue_0': -37.74000000000025, 'blue_1': -36.60000000000026, 'red_0': -28.90000000000016}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 142.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 858000.0, 'blue_0': 858000.0, 'blue_1': 858000.0, 'red_0': 858000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -44.8000000000002, 'blue_policy': -36.60000000000026}, 'num_module_steps_sampled_lifetime': {'red_policy': 1716000.0, 'blue_policy': 1716000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003283090189620107, 'episode_return_mean': -148.04000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -107.20000000000078, 'episode_duration_sec_mean': 17.448067179974167, 'episode_return_min': -187.9000000000004, 'rlmodule_inference_timer': 0.012369038212935467, 'num_episodes_lifetime': 715.0, 'episode_len_min': 1200, 'time_between_sampling': 258.24258630000986, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.3413392948017, 'throughput_since_last_restore': 15.803568092633165}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 749
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 771
(MultiAgentEnvRunner pid=37492) {'red_0': -35.900000000000205, 'red_1': -8.199999999999998, 'blue_0': -41.10000000000038, 'blue_1': -29.2000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -23.500000000000068, 'red_1': -19.0, 'blue_0': -45.60000000000038, 'blue_1': -48.8000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -14.899999999999965, 'red_1': -14.099999999999966, 'blue_0': -36.00000000000024, 'blue_1': -45.00000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -21.900000000000038, 'red_1': -12.89999999999997, 'blue_0': -40.1000000000003, 'blue_1': -42.600000000000335}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1041
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1096
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1167
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1189
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1189
(MultiAgentEnvRunner pid=37492) {'red_0': -39.200000000000216, 'red_1': -20.70000000000004, 'blue_0': -40.300000000000324, 'blue_1': -31.70000000000038}
ITERATION 143: reward=-122.14000000000078, metadata={'num_env_steps_sampled_lifetime': 864000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032649868545477854, 'timers': {'connectors': {'batch_individual_items': 9.586260720462718e-05, 'add_states_from_episodes_to_batch': 6.57702196167026e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2495576797053438e-05, 'numpy_to_tensor': 6.773747967554513e-05, 'agent_to_module_mapping': 8.097403844359779e-06, 'add_observations_from_episodes_to_batch': 3.965149757156584e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008737514699205419, 'timers': {'connectors': {'get_actions': 0.0004507946000868278, 'un_batch_to_individual_items': 6.555281824914066e-05, 'tensor_to_numpy': 0.00011892635276456888, 'module_to_agent_unmapping': 6.426882288842683e-06, 'normalize_and_clip_actions': 7.1779749831982e-05, 'listify_data_for_vector_env': 2.417495169616737e-05, 'remove_single_ts_time_rank_from_batch': 2.3819561908116536e-06}}}, 'sample': 87.8244083999889, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -14.979999999999995, 'blue_0': -40.620000000000324, 'blue_1': -39.46000000000036, 'red_0': -27.080000000000098}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 143.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 864000.0, 'blue_0': 864000.0, 'blue_1': 864000.0, 'red_0': 864000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -14.979999999999995, 'blue_policy': -39.46000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 1728000.0, 'blue_policy': 1728000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034311090089695164, 'episode_return_mean': -122.14000000000078, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -110.00000000000054, 'episode_duration_sec_mean': 17.44442469999194, 'episode_return_min': -136.90000000000086, 'rlmodule_inference_timer': 0.012763866624412496, 'num_episodes_lifetime': 720.0, 'episode_len_min': 1200, 'time_between_sampling': 258.13528209994547, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.31820990135383, 'throughput_since_last_restore': 15.813171537818315}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 935
(MultiAgentEnvRunner pid=37492) {'red_0': -38.50000000000026, 'red_1': -38.90000000000027, 'blue_0': -65.30000000000054, 'blue_1': -39.700000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 570
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 732
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 941
(MultiAgentEnvRunner pid=37492) {'red_0': -35.20000000000016, 'red_1': -42.70000000000028, 'blue_0': -52.40000000000051, 'blue_1': -26.100000000000314}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 519
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 730
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 811
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 824
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 825
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1165
(MultiAgentEnvRunner pid=37492) {'red_0': -37.400000000000226, 'red_1': -43.9000000000003, 'blue_0': -17.899999999999995, 'blue_1': -27.20000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -3.4000000000000017, 'red_1': -3.1000000000000014, 'blue_0': -49.60000000000042, 'blue_1': -33.500000000000206}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1053
(MultiAgentEnvRunner pid=37492) {'red_0': -24.100000000000016, 'red_1': -37.40000000000019, 'blue_0': -20.600000000000236, 'blue_1': -48.600000000000485}
ITERATION 144: reward=-137.100000000001, metadata={'num_env_steps_sampled_lifetime': 870000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00036219392496112394, 'timers': {'connectors': {'batch_individual_items': 0.00010234363435176288, 'add_states_from_episodes_to_batch': 7.4173074648498825e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4475867002035393e-05, 'numpy_to_tensor': 7.48088612359286e-05, 'agent_to_module_mapping': 9.107245038926906e-06, 'add_observations_from_episodes_to_batch': 4.322327618231763e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009675559013104648, 'timers': {'connectors': {'get_actions': 0.0004904734438492275, 'un_batch_to_individual_items': 7.376619694698587e-05, 'tensor_to_numpy': 0.00013297764828119394, 'module_to_agent_unmapping': 7.358345203589258e-06, 'normalize_and_clip_actions': 8.079023174308912e-05, 'listify_data_for_vector_env': 2.754196378862211e-05, 'remove_single_ts_time_rank_from_batch': 2.646609894362009e-06}}}, 'sample': 88.41877430002205, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.2000000000002, 'blue_0': -41.160000000000345, 'blue_1': -35.020000000000316, 'red_0': -27.720000000000134}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 144.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 870000.0, 'blue_0': 870000.0, 'blue_1': 870000.0, 'red_0': 870000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.2000000000002, 'blue_policy': -35.020000000000316}, 'num_module_steps_sampled_lifetime': {'red_policy': 1740000.0, 'blue_policy': 1740000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00038288815541439566, 'episode_return_mean': -137.100000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -89.60000000000062, 'episode_duration_sec_mean': 17.54077580003068, 'episode_return_min': -182.4000000000014, 'rlmodule_inference_timer': 0.014377995860429553, 'num_episodes_lifetime': 725.0, 'episode_len_min': 1200, 'time_between_sampling': 258.63425400003325, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.29914763656937, 'throughput_since_last_restore': 15.822544183341362}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -13.799999999999969, 'red_1': -10.799999999999978, 'blue_0': -52.10000000000045, 'blue_1': -46.10000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 228
(MultiAgentEnvRunner pid=37492) {'red_0': -8.499999999999986, 'red_1': -43.90000000000035, 'blue_0': -37.70000000000026, 'blue_1': -40.8000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 458
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 718
(MultiAgentEnvRunner pid=37492) {'red_0': -71.40000000000015, 'red_1': -40.30000000000023, 'blue_0': -31.30000000000022, 'blue_1': -29.8000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1198
(MultiAgentEnvRunner pid=37492) {'red_0': -26.200000000000106, 'red_1': -26.000000000000128, 'blue_0': -37.10000000000024, 'blue_1': -42.70000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 241
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 556
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 633
(MultiAgentEnvRunner pid=37492) {'red_0': -16.29999999999996, 'red_1': -32.90000000000023, 'blue_0': -46.100000000000335, 'blue_1': -35.600000000000215}
ITERATION 145: reward=-137.88000000000082, metadata={'num_env_steps_sampled_lifetime': 876000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032845313117523146, 'timers': {'connectors': {'batch_individual_items': 9.65107485596057e-05, 'add_states_from_episodes_to_batch': 6.3599090469500795e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2602691806436072e-05, 'numpy_to_tensor': 6.822612107815538e-05, 'agent_to_module_mapping': 9.107957921014301e-06, 'add_observations_from_episodes_to_batch': 3.8625053902194e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008682130448371101, 'timers': {'connectors': {'get_actions': 0.0004477390631206839, 'un_batch_to_individual_items': 6.516504844550822e-05, 'tensor_to_numpy': 0.00011721675592858706, 'module_to_agent_unmapping': 6.585378471716978e-06, 'normalize_and_clip_actions': 7.224018968020171e-05, 'listify_data_for_vector_env': 2.3917942711341246e-05, 'remove_single_ts_time_rank_from_batch': 2.3021891308987497e-06}}}, 'sample': 88.20234640000854, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.780000000000182, 'blue_0': -40.860000000000305, 'blue_1': -39.0000000000003, 'red_0': -27.24000000000003}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 145.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 876000.0, 'blue_0': 876000.0, 'blue_1': 876000.0, 'red_0': 876000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.780000000000182, 'blue_policy': -39.0000000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 1752000.0, 'blue_policy': 1752000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033964383668406544, 'episode_return_mean': -137.88000000000082, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -122.80000000000075, 'episode_duration_sec_mean': 17.516643260000272, 'episode_return_min': -172.8000000000009, 'rlmodule_inference_timer': 0.012713700744873756, 'num_episodes_lifetime': 730.0, 'episode_len_min': 1200, 'time_between_sampling': 258.4180739999283, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.30690365040067, 'throughput_since_last_restore': 15.83184384398009}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 924
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1093
(MultiAgentEnvRunner pid=37492) {'red_0': -31.80000000000014, 'red_1': -40.80000000000023, 'blue_0': -38.600000000000335, 'blue_1': -33.000000000000405}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 406
(MultiAgentEnvRunner pid=37492) {'red_0': -15.09999999999997, 'red_1': -48.40000000000033, 'blue_0': -42.00000000000032, 'blue_1': -27.900000000000098}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 869
(MultiAgentEnvRunner pid=37492) {'red_0': -32.90000000000012, 'red_1': -9.4, 'blue_0': -39.70000000000035, 'blue_1': -47.400000000000595}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 438
(MultiAgentEnvRunner pid=37492) {'red_0': -18.100000000000005, 'red_1': -16.99999999999999, 'blue_0': -41.80000000000029, 'blue_1': -32.70000000000017}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -12.399999999999972, 'red_1': -22.20000000000005, 'blue_0': -47.00000000000039, 'blue_1': -51.10000000000044}
ITERATION 146: reward=-129.86000000000084, metadata={'num_env_steps_sampled_lifetime': 882000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032523475974915826, 'timers': {'connectors': {'batch_individual_items': 9.508943038397722e-05, 'add_states_from_episodes_to_batch': 6.455002943786802e-06, 'add_time_dim_to_batch_and_zero_pad': 1.248773525298798e-05, 'numpy_to_tensor': 6.742905168463297e-05, 'agent_to_module_mapping': 8.524913693727755e-06, 'add_observations_from_episodes_to_batch': 3.911170550205865e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008806031154450782, 'timers': {'connectors': {'get_actions': 0.00045058921447770143, 'un_batch_to_individual_items': 6.796714669756104e-05, 'tensor_to_numpy': 0.00012205107835044121, 'module_to_agent_unmapping': 6.406182570801174e-06, 'normalize_and_clip_actions': 7.386814695652214e-05, 'listify_data_for_vector_env': 2.424605323094749e-05, 'remove_single_ts_time_rank_from_batch': 2.402732472146442e-06}}}, 'sample': 89.37945630005561, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.560000000000123, 'blue_0': -41.82000000000034, 'blue_1': -38.42000000000034, 'red_0': -22.06000000000004}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 146.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 882000.0, 'blue_0': 882000.0, 'blue_1': 882000.0, 'red_0': 882000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.560000000000123, 'blue_policy': -38.42000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 1764000.0, 'blue_policy': 1764000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033846229596301884, 'episode_return_mean': -129.86000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.60000000000046, 'episode_duration_sec_mean': 17.752774299960585, 'episode_return_min': -144.2000000000011, 'rlmodule_inference_timer': 0.012816261574853207, 'num_episodes_lifetime': 735.0, 'episode_len_min': 1200, 'time_between_sampling': 258.4824893999612, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.252020336661836, 'throughput_since_last_restore': 15.840713945976749}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 513
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1187
(MultiAgentEnvRunner pid=37492) {'red_0': -22.600000000000207, 'red_1': -19.500000000000025, 'blue_0': -33.900000000000176, 'blue_1': -40.30000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 382
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 619
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1063
(MultiAgentEnvRunner pid=37492) {'red_0': -54.20000000000047, 'red_1': -39.30000000000027, 'blue_0': -35.30000000000023, 'blue_1': -30.900000000000183}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 776
(MultiAgentEnvRunner pid=37492) {'red_0': -36.00000000000043, 'red_1': -7.9999999999999725, 'blue_0': -46.000000000000306, 'blue_1': -46.90000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1093
(MultiAgentEnvRunner pid=37492) {'red_0': -40.40000000000022, 'red_1': -31.0000000000001, 'blue_0': -49.20000000000048, 'blue_1': -24.50000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 955
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 988
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1090
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1090
(MultiAgentEnvRunner pid=37492) {'red_0': -57.500000000000504, 'red_1': -29.0000000000001, 'blue_0': -40.50000000000038, 'blue_1': -26.60000000000022}
ITERATION 147: reward=-142.32000000000102, metadata={'num_env_steps_sampled_lifetime': 888000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003538666766355797, 'timers': {'connectors': {'batch_individual_items': 0.00010125735142017613, 'add_states_from_episodes_to_batch': 7.32047653230681e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3561770141485758e-05, 'numpy_to_tensor': 7.290788004633763e-05, 'agent_to_module_mapping': 8.930720131574985e-06, 'add_observations_from_episodes_to_batch': 4.2249975766415437e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000944940586893983, 'timers': {'connectors': {'get_actions': 0.00048235470844404203, 'un_batch_to_individual_items': 7.281877334310309e-05, 'tensor_to_numpy': 0.00012673509528171681, 'module_to_agent_unmapping': 7.0926123005237465e-06, 'normalize_and_clip_actions': 7.985044771323498e-05, 'listify_data_for_vector_env': 2.7006920047228096e-05, 'remove_single_ts_time_rank_from_batch': 2.689035753980327e-06}}}, 'sample': 89.38565650000237, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.360000000000092, 'blue_0': -40.98000000000032, 'blue_1': -33.840000000000266, 'red_0': -42.14000000000037}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 147.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 888000.0, 'blue_0': 888000.0, 'blue_1': 888000.0, 'red_0': 888000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.360000000000092, 'blue_policy': -33.840000000000266}, 'num_module_steps_sampled_lifetime': {'red_policy': 1776000.0, 'blue_policy': 1776000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036644227200534015, 'episode_return_mean': -142.32000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -116.30000000000072, 'episode_duration_sec_mean': 17.73368162000552, 'episode_return_min': -159.70000000000115, 'rlmodule_inference_timer': 0.014149405861685269, 'num_episodes_lifetime': 740.0, 'episode_len_min': 1200, 'time_between_sampling': 258.40915020008106, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.252100816780597, 'throughput_since_last_restore': 15.849474252756385}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -58.60000000000054, 'red_1': -29.90000000000015, 'blue_0': -53.000000000000476, 'blue_1': -39.40000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -14.199999999999967, 'red_1': -10.799999999999978, 'blue_0': -39.20000000000029, 'blue_1': -39.700000000000294}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 352
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 724
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1079
(MultiAgentEnvRunner pid=37492) {'red_0': -19.900000000000095, 'red_1': -35.70000000000044, 'blue_0': -50.60000000000037, 'blue_1': -51.70000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.099999999999984, 'red_1': -27.70000000000012, 'blue_0': -42.00000000000033, 'blue_1': -59.90000000000055}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 462
(MultiAgentEnvRunner pid=37492) {'red_0': -20.700000000000028, 'red_1': -17.999999999999982, 'blue_0': -46.600000000000385, 'blue_1': -41.70000000000032}
ITERATION 148: reward=-141.680000000001, metadata={'num_env_steps_sampled_lifetime': 894000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003318172191744464, 'timers': {'connectors': {'batch_individual_items': 9.890200508586723e-05, 'add_states_from_episodes_to_batch': 6.666555008380877e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3874874715472424e-05, 'numpy_to_tensor': 6.77633332554236e-05, 'agent_to_module_mapping': 8.08639436534114e-06, 'add_observations_from_episodes_to_batch': 3.887260207710239e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008859216798780868, 'timers': {'connectors': {'get_actions': 0.000455649147573475, 'un_batch_to_individual_items': 6.625955696515148e-05, 'tensor_to_numpy': 0.00011891033834624823, 'module_to_agent_unmapping': 6.497701387793794e-06, 'normalize_and_clip_actions': 7.356146215399871e-05, 'listify_data_for_vector_env': 2.5620602425223772e-05, 'remove_single_ts_time_rank_from_batch': 2.5627475751840255e-06}}}, 'sample': 88.76606759999413, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -24.420000000000137, 'blue_0': -46.28000000000036, 'blue_1': -46.480000000000366, 'red_0': -24.50000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 148.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 894000.0, 'blue_0': 894000.0, 'blue_1': 894000.0, 'red_0': 894000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -24.420000000000137, 'blue_policy': -46.480000000000366}, 'num_module_steps_sampled_lifetime': {'red_policy': 1788000.0, 'blue_policy': 1788000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003393613452380778, 'episode_return_mean': -141.680000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -103.90000000000052, 'episode_duration_sec_mean': 17.630840079975314, 'episode_return_min': -180.90000000000146, 'rlmodule_inference_timer': 0.012956991504998575, 'num_episodes_lifetime': 745.0, 'episode_len_min': 1200, 'time_between_sampling': 258.4012653999962, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.289068473262542, 'throughput_since_last_restore': 15.858335732536487}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 276
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1175
(MultiAgentEnvRunner pid=37492) {'red_0': -34.70000000000024, 'red_1': -38.2000000000003, 'blue_0': -29.00000000000011, 'blue_1': -49.4000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 420
(MultiAgentEnvRunner pid=37492) {'red_0': -17.399999999999995, 'red_1': -28.300000000000118, 'blue_0': -45.20000000000034, 'blue_1': -37.600000000000264}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 561
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 717
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 717
(MultiAgentEnvRunner pid=37492) {'red_0': -77.99999999999963, 'red_1': -18.09999999999999, 'blue_0': -26.600000000000097, 'blue_1': -37.60000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 470
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1129
(MultiAgentEnvRunner pid=37492) {'red_0': -22.199999999999978, 'red_1': -24.000000000000004, 'blue_0': -29.40000000000036, 'blue_1': -44.2000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 392
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 405
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 431
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 793
(MultiAgentEnvRunner pid=37492) {'red_0': -53.900000000000475, 'red_1': -46.20000000000041, 'blue_0': -34.00000000000016, 'blue_1': -39.9000000000003}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.30000000000000004, 'red_1': 0, 'blue_0': -1.5000000000000002, 'blue_1': -58.30000000000056}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -7.599999999999989, 'blue_1': -117.09999999999762}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 07:01:55,154	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -2.1000000000000005, 'blue_0': -39.20000000000029, 'blue_1': -2.500000000000001}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.1, 'blue_1': -118.59999999999754}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -14.799999999999967, 'red_1': 0, 'blue_0': -2.600000000000001, 'blue_1': -2.600000000000001}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 149: reward=-146.78000000000077, metadata={'num_env_steps_sampled_lifetime': 900000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032484435942106075, 'timers': {'connectors': {'batch_individual_items': 9.768010263150941e-05, 'add_states_from_episodes_to_batch': 6.359689743538681e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2274929850726706e-05, 'numpy_to_tensor': 6.687023667367879e-05, 'agent_to_module_mapping': 8.237973762130653e-06, 'add_observations_from_episodes_to_batch': 3.7879811525160786e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008704966891326554, 'timers': {'connectors': {'get_actions': 0.00044533027336405065, 'un_batch_to_individual_items': 6.517707879545448e-05, 'tensor_to_numpy': 0.00011822988055137346, 'module_to_agent_unmapping': 6.554413247235968e-06, 'normalize_and_clip_actions': 7.29196761227706e-05, 'listify_data_for_vector_env': 2.5262561532796152e-05, 'remove_single_ts_time_rank_from_batch': 2.3132161623300973e-06}}}, 'sample': 88.75585270009469, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.960000000000168, 'blue_0': -32.84000000000022, 'blue_1': -41.740000000000336, 'red_0': -41.240000000000066}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 149.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 900000.0, 'blue_0': 900000.0, 'blue_1': 900000.0, 'red_0': 900000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.960000000000168, 'blue_policy': -41.740000000000336}, 'num_module_steps_sampled_lifetime': {'red_policy': 1800000.0, 'blue_policy': 1800000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034357917931047565, 'episode_return_mean': -146.78000000000077, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -119.80000000000074, 'episode_duration_sec_mean': 17.621346860053016, 'episode_return_min': -174.00000000000134, 'rlmodule_inference_timer': 0.01261179323293561, 'num_episodes_lifetime': 750.0, 'episode_len_min': 1200, 'time_between_sampling': 258.2837433000095, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.770304129837553, 'throughput_since_last_restore': 15.842320141788724}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 589
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 800
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 973
(MultiAgentEnvRunner pid=37492) {'red_0': -45.500000000000455, 'red_1': -19.500000000000007, 'blue_0': -46.40000000000036, 'blue_1': -51.20000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.099999999999987, 'red_1': -8.099999999999987, 'blue_0': -58.40000000000054, 'blue_1': -46.10000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 628
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1193
(MultiAgentEnvRunner pid=37492) {'red_0': -54.20000000000041, 'red_1': -28.500000000000068, 'blue_0': -27.100000000000357, 'blue_1': -42.60000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -35.70000000000023, 'red_1': -17.099999999999973, 'blue_0': -49.60000000000042, 'blue_1': -59.80000000000056}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 210
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 312
(MultiAgentEnvRunner pid=37492) {'red_0': -36.00000000000025, 'red_1': -45.40000000000036, 'blue_0': -45.40000000000036, 'blue_1': -44.80000000000035}
ITERATION 150: reward=-153.90000000000117, metadata={'num_env_steps_sampled_lifetime': 906000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032226004541704913, 'timers': {'connectors': {'batch_individual_items': 9.659530680231872e-05, 'add_states_from_episodes_to_batch': 6.341314706089236e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2248449423649583e-05, 'numpy_to_tensor': 6.599203172237329e-05, 'agent_to_module_mapping': 7.972246291209494e-06, 'add_observations_from_episodes_to_batch': 3.8354618450669676e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008608940958114259, 'timers': {'connectors': {'get_actions': 0.0004434365257152954, 'un_batch_to_individual_items': 6.479579505832265e-05, 'tensor_to_numpy': 0.00011740503644984654, 'module_to_agent_unmapping': 6.390496458289344e-06, 'normalize_and_clip_actions': 7.110260940232402e-05, 'listify_data_for_vector_env': 2.362710188442071e-05, 'remove_single_ts_time_rank_from_batch': 2.304715172739713e-06}}}, 'sample': 88.14660069998354, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -23.72000000000008, 'blue_0': -45.38000000000041, 'blue_1': -48.90000000000042, 'red_0': -35.90000000000027}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 150.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 906000.0, 'blue_0': 906000.0, 'blue_1': 906000.0, 'red_0': 906000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -23.72000000000008, 'blue_policy': -48.90000000000042}, 'num_module_steps_sampled_lifetime': {'red_policy': 1812000.0, 'blue_policy': 1812000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003337561162649356, 'episode_return_mean': -153.90000000000117, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -120.70000000000087, 'episode_duration_sec_mean': 17.494904600013978, 'episode_return_min': -171.60000000000133, 'rlmodule_inference_timer': 0.012567531971834847, 'num_episodes_lifetime': 755.0, 'episode_len_min': 1200, 'time_between_sampling': 346.96488530002534, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.304531871216128, 'throughput_since_last_restore': 15.851189723263577}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 346.68858550000004, 'restore_env_runners': 8.00006091594696e-06, 'training_step': 346.6883097999962, 'env_runner_sampling_timer': 88.27275230002124, 'learner_update_timer': 258.35958140005823, 'synch_weights': 0.013255199999548495, 'synch_env_connectors': 0.0031062999041751027, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 906000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032226004541704913, 'timers': {'connectors': {'batch_individual_items': 9.659530680231872e-05, 'add_states_from_episodes_to_batch': 6.341314706089236e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2248449423649583e-05, 'numpy_to_tensor': 6.599203172237329e-05, 'agent_to_module_mapping': 7.972246291209494e-06, 'add_observations_from_episodes_to_batch': 3.8354618450669676e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008608940958114259, 'timers': {'connectors': {'get_actions': 0.0004434365257152954, 'un_batch_to_individual_items': 6.479579505832265e-05, 'tensor_to_numpy': 0.00011740503644984654, 'module_to_agent_unmapping': 6.390496458289344e-06, 'normalize_and_clip_actions': 7.110260940232402e-05, 'listify_data_for_vector_env': 2.362710188442071e-05, 'remove_single_ts_time_rank_from_batch': 2.304715172739713e-06}}}, 'sample': 88.14660069998354, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -23.72000000000008, 'blue_0': -45.38000000000041, 'blue_1': -48.90000000000042, 'red_0': -35.90000000000027}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 150.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 906000.0, 'blue_0': 906000.0, 'blue_1': 906000.0, 'red_0': 906000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -23.72000000000008, 'blue_policy': -48.90000000000042}, 'num_module_steps_sampled_lifetime': {'red_policy': 1812000.0, 'blue_policy': 1812000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003337561162649356, 'episode_return_mean': -153.90000000000117, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -120.70000000000087, 'episode_duration_sec_mean': 17.494904600013978, 'episode_return_min': -171.60000000000133, 'rlmodule_inference_timer': 0.012567531971834847, 'num_episodes_lifetime': 755.0, 'episode_len_min': 1200, 'time_between_sampling': 346.96488530002534, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.304531871216128, 'throughput_since_last_restore': 15.851189723263577}}, 'learners': {'red_policy': {'policy_loss': -0.18933139741420746, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.011922360397875309, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 151.0, 'num_module_steps_trained_lifetime': 54408320.0, 'curr_entropy_coeff': 0.03641, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002094, 'vf_explained_var': -0.2917826175689697, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.78813362121582, 'entropy': 1.1754686832427979, 'vf_loss_unclipped': 697.61962890625, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1039.1988431034254, 'throughput_since_last_restore': 951.9167737590643}}, 'blue_policy': {'weights_seq_no': 151.0, 'num_module_steps_trained_lifetime': 54408320.0, 'curr_entropy_coeff': 0.03641, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0002094, 'vf_explained_var': 0.4774388074874878, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 1.8066749572753906, 'total_loss': 0.5755410194396973, 'entropy': 1.7060986757278442, 'policy_loss': -0.20874923467636108, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.8266884088516235, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.013086041435599327, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1039.197464115042, 'throughput_since_last_restore': 951.9167750947588}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 8.500064723193645e-06, 'batch_individual_items': 0.8135636000661179, 'add_time_dim_to_batch_and_zero_pad': 3.15000070258975e-05, 'numpy_to_tensor': 0.13780769996810704, 'add_observations_from_episodes_to_batch': 0.0002397999633103609, 'agent_to_module_mapping': 0.0208996000001207, 'add_one_ts_to_episodes_and_truncate': 0.1601673000259325, 'add_columns_from_episodes_to_train_batch': 0.4623384999576956, 'general_advantage_estimation': 13.197868200019002}}, 'connector_pipeline_timer': 14.793330400018021}, 'num_module_steps_trained_lifetime': 108816640.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 2550390000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48712.34841001431, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48712.344518417165, 'throughput_since_last_restore': 44621.098903296566}, 'num_module_steps_trained_throughput': 2078.3933151680512, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2078.3932546253523, 'throughput_since_last_restore': 1903.8335522613427}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 906000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 151, 'trial_id': 'default', 'date': '2026-01-26_07-11-28', 'timestamp': 1769407888, 'time_this_iter_s': 346.70462369918823, 'time_total_s': 57133.32473731041, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 57133.32473731041, 'iterations_since_restore': 151, 'perf': {'cpu_util_percent': 14.878137651821865, 'ram_util_percent': 89.46194331983806}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 590
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 669
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 695
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1135
(MultiAgentEnvRunner pid=37492) {'red_0': -49.30000000000037, 'red_1': -50.90000000000039, 'blue_0': -23.100000000000016, 'blue_1': -38.800000000000324}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 535
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 773
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 807
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1162
(MultiAgentEnvRunner pid=37492) {'red_0': -47.70000000000037, 'red_1': -66.70000000000053, 'blue_0': -15.90000000000009, 'blue_1': -40.80000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 825
(MultiAgentEnvRunner pid=37492) {'red_0': -33.600000000000136, 'red_1': -18.50000000000001, 'blue_0': -23.40000000000014, 'blue_1': -32.40000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 383
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 547
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 547
(MultiAgentEnvRunner pid=37492) {'red_0': -19.700000000000056, 'red_1': -32.10000000000019, 'blue_0': -31.40000000000015, 'blue_1': -43.70000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 339
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 866
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 881
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1059
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1059
(MultiAgentEnvRunner pid=37492) {'red_0': -30.200000000000152, 'red_1': -45.10000000000031, 'blue_0': -44.50000000000039, 'blue_1': -26.800000000000175}
ITERATION 151: reward=-142.92000000000093, metadata={'num_env_steps_sampled_lifetime': 912000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032032001231996473, 'timers': {'connectors': {'batch_individual_items': 9.614638494460111e-05, 'add_states_from_episodes_to_batch': 6.527327029588834e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1961517031571183e-05, 'numpy_to_tensor': 6.602977669618312e-05, 'agent_to_module_mapping': 7.863809363316228e-06, 'add_observations_from_episodes_to_batch': 3.7934790471350986e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008585271866727593, 'timers': {'connectors': {'get_actions': 0.0004419917836997159, 'un_batch_to_individual_items': 6.500465464115382e-05, 'tensor_to_numpy': 0.00011612721133735806, 'module_to_agent_unmapping': 6.285503763583355e-06, 'normalize_and_clip_actions': 7.128358987878471e-05, 'listify_data_for_vector_env': 2.4079796259823225e-05, 'remove_single_ts_time_rank_from_batch': 2.3407768157680225e-06}}}, 'sample': 88.22998279996682, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -42.66000000000029, 'blue_0': -27.660000000000156, 'blue_1': -36.500000000000284, 'red_0': -36.100000000000215}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 151.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 912000.0, 'blue_0': 912000.0, 'blue_1': 912000.0, 'red_0': 912000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -42.66000000000029, 'blue_policy': -36.500000000000284}, 'num_module_steps_sampled_lifetime': {'red_policy': 1824000.0, 'blue_policy': 1824000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003273324851700261, 'episode_return_mean': -142.92000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -107.90000000000055, 'episode_duration_sec_mean': 17.532163040013984, 'episode_return_min': -171.10000000000127, 'rlmodule_inference_timer': 0.01246599567928798, 'num_episodes_lifetime': 760.0, 'episode_len_min': 1200, 'time_between_sampling': 258.6624254999915, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.329234560775134, 'throughput_since_last_restore': 15.860088510830671}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1176
(MultiAgentEnvRunner pid=37492) {'red_0': -17.799999999999986, 'red_1': -39.80000000000029, 'blue_0': -51.30000000000045, 'blue_1': -43.300000000000374}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1008
(MultiAgentEnvRunner pid=37492) {'red_0': -27.90000000000013, 'red_1': -30.700000000000195, 'blue_0': -57.300000000000516, 'blue_1': -56.10000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 110
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 185
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 212
(MultiAgentEnvRunner pid=37492) {'red_0': -16.499999999999982, 'red_1': -34.00000000000016, 'blue_0': -31.500000000000153, 'blue_1': -40.70000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 482
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 492
(MultiAgentEnvRunner pid=37492) {'red_0': -14.799999999999981, 'red_1': -9.099999999999994, 'blue_0': -26.800000000000075, 'blue_1': -41.5000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 709
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 964
(MultiAgentEnvRunner pid=37492) {'red_0': -19.900000000000027, 'red_1': -21.60000000000001, 'blue_0': -39.50000000000035, 'blue_1': -35.60000000000044}
ITERATION 152: reward=-131.14000000000084, metadata={'num_env_steps_sampled_lifetime': 918000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003196323302423407, 'timers': {'connectors': {'batch_individual_items': 9.491603653191993e-05, 'add_states_from_episodes_to_batch': 6.414858239141105e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2303207938804605e-05, 'numpy_to_tensor': 6.567869223661032e-05, 'agent_to_module_mapping': 7.899024651945533e-06, 'add_observations_from_episodes_to_batch': 3.770310568128521e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008577049248252713, 'timers': {'connectors': {'get_actions': 0.0004420305827636264, 'un_batch_to_individual_items': 6.433261941381949e-05, 'tensor_to_numpy': 0.00011650818798279451, 'module_to_agent_unmapping': 6.341744379649594e-06, 'normalize_and_clip_actions': 6.989575331222855e-05, 'listify_data_for_vector_env': 2.3532687469917244e-05, 'remove_single_ts_time_rank_from_batch': 2.271720950469769e-06}}}, 'sample': 88.19589450000785, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.040000000000134, 'blue_0': -41.2800000000003, 'blue_1': -43.44000000000038, 'red_0': -19.380000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 152.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 918000.0, 'blue_0': 918000.0, 'blue_1': 918000.0, 'red_0': 918000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.040000000000134, 'blue_policy': -43.44000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 1836000.0, 'blue_policy': 1836000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003365250598163766, 'episode_return_mean': -131.14000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -92.20000000000036, 'episode_duration_sec_mean': 17.523437999980523, 'episode_return_min': -172.00000000000134, 'rlmodule_inference_timer': 0.01242112056222214, 'num_episodes_lifetime': 765.0, 'episode_len_min': 1200, 'time_between_sampling': 257.9310523000313, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.340052605643002, 'throughput_since_last_restore': 15.868940081702723}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 621
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1000
(MultiAgentEnvRunner pid=37492) {'red_0': -39.700000000000216, 'red_1': -22.699999999999992, 'blue_0': -45.20000000000042, 'blue_1': -19.700000000000184}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -49.2000000000004, 'red_1': -32.100000000000186, 'blue_0': -51.90000000000046, 'blue_1': -36.900000000000254}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1127
(MultiAgentEnvRunner pid=37492) {'red_0': -21.099999999999987, 'red_1': -35.70000000000016, 'blue_0': -24.100000000000286, 'blue_1': -42.9000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 394
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 581
(MultiAgentEnvRunner pid=37492) {'red_0': -39.80000000000023, 'red_1': -19.000000000000018, 'blue_0': -25.60000000000007, 'blue_1': -34.900000000000276}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1184
(MultiAgentEnvRunner pid=37492) {'red_0': -27.800000000000054, 'red_1': -50.900000000000375, 'blue_0': -40.00000000000036, 'blue_1': -20.500000000000234}
ITERATION 153: reward=-135.9400000000009, metadata={'num_env_steps_sampled_lifetime': 924000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032378005078355685, 'timers': {'connectors': {'batch_individual_items': 9.58608566895918e-05, 'add_states_from_episodes_to_batch': 6.432727778214342e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2194872036846996e-05, 'numpy_to_tensor': 6.697434834123955e-05, 'agent_to_module_mapping': 8.038752818101714e-06, 'add_observations_from_episodes_to_batch': 3.884552199040722e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008567689789449811, 'timers': {'connectors': {'get_actions': 0.0004392012755493869, 'un_batch_to_individual_items': 6.436425438190845e-05, 'tensor_to_numpy': 0.00011761082147969276, 'module_to_agent_unmapping': 6.31247739341103e-06, 'normalize_and_clip_actions': 7.079860344480618e-05, 'listify_data_for_vector_env': 2.3550469644950398e-05, 'remove_single_ts_time_rank_from_batch': 2.294865646502229e-06}}}, 'sample': 87.5809494999703, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.08000000000014, 'blue_0': -37.36000000000032, 'blue_1': -30.980000000000267, 'red_0': -35.520000000000174}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 153.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 924000.0, 'blue_0': 924000.0, 'blue_1': 924000.0, 'red_0': 924000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.08000000000014, 'blue_policy': -30.980000000000267}, 'num_module_steps_sampled_lifetime': {'red_policy': 1848000.0, 'blue_policy': 1848000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033711529263406024, 'episode_return_mean': -135.9400000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -119.3000000000006, 'episode_duration_sec_mean': 17.37718485998921, 'episode_return_min': -170.1000000000013, 'rlmodule_inference_timer': 0.012461673398399918, 'num_episodes_lifetime': 770.0, 'episode_len_min': 1200, 'time_between_sampling': 257.82098379998934, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.356515315784844, 'throughput_since_last_restore': 15.877776074688041}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 542
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 578
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 747
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 747
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 890
(MultiAgentEnvRunner pid=37492) {'red_0': -28.900000000000155, 'red_1': -44.80000000000037, 'blue_0': -40.40000000000028, 'blue_1': -31.50000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 852
(MultiAgentEnvRunner pid=37492) {'red_0': -33.00000000000014, 'red_1': -17.599999999999998, 'blue_0': -23.80000000000015, 'blue_1': -51.70000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 646
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 664
(MultiAgentEnvRunner pid=37492) {'red_0': -32.000000000000185, 'red_1': -20.90000000000004, 'blue_0': -39.10000000000028, 'blue_1': -42.40000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.899999999999995, 'red_1': -4.799999999999999, 'blue_0': -42.40000000000033, 'blue_1': -45.50000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -34.90000000000022, 'red_1': -16.999999999999975, 'blue_0': -57.20000000000053, 'blue_1': -60.60000000000056}
ITERATION 154: reward=-134.88000000000096, metadata={'num_env_steps_sampled_lifetime': 930000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031335208382973545, 'timers': {'connectors': {'batch_individual_items': 9.37132089370567e-05, 'add_states_from_episodes_to_batch': 6.14695242352199e-06, 'add_time_dim_to_batch_and_zero_pad': 1.175204148174485e-05, 'numpy_to_tensor': 6.460595490173414e-05, 'agent_to_module_mapping': 7.761894247536695e-06, 'add_observations_from_episodes_to_batch': 3.7168336246036474e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008419732424647412, 'timers': {'connectors': {'get_actions': 0.0004372834672451429, 'un_batch_to_individual_items': 6.363720308999248e-05, 'tensor_to_numpy': 0.00011342397559074742, 'module_to_agent_unmapping': 6.181617421209198e-06, 'normalize_and_clip_actions': 6.857787920254119e-05, 'listify_data_for_vector_env': 2.300896969874941e-05, 'remove_single_ts_time_rank_from_batch': 2.2985074689987858e-06}}}, 'sample': 86.60467200004496, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.020000000000074, 'blue_0': -40.58000000000031, 'blue_1': -46.34000000000041, 'red_0': -26.94000000000014}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 154.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 930000.0, 'blue_0': 930000.0, 'blue_1': 930000.0, 'red_0': 930000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.020000000000074, 'blue_policy': -46.34000000000041}, 'num_module_steps_sampled_lifetime': {'red_policy': 1860000.0, 'blue_policy': 1860000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032899615988472646, 'episode_return_mean': -134.88000000000096, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -98.6000000000007, 'episode_duration_sec_mean': 17.203407459985463, 'episode_return_min': -169.7000000000013, 'rlmodule_inference_timer': 0.01234807431212136, 'num_episodes_lifetime': 775.0, 'episode_len_min': 1200, 'time_between_sampling': 258.1115078999428, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.401983395233326, 'throughput_since_last_restore': 15.88675272287593}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1125
(MultiAgentEnvRunner pid=37492) {'red_0': -6.499999999999993, 'red_1': -53.80000000000051, 'blue_0': -44.00000000000034, 'blue_1': -44.500000000000355}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 193
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 224
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 457
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 924
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 926
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1001
(MultiAgentEnvRunner pid=37492) {'red_0': -64.9000000000005, 'red_1': -26.60000000000014, 'blue_0': -37.70000000000027, 'blue_1': -34.30000000000017}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 585
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 715
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 868
(MultiAgentEnvRunner pid=37492) {'red_0': -45.6000000000003, 'red_1': -35.80000000000016, 'blue_0': -41.800000000000544, 'blue_1': -51.3000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 554
(MultiAgentEnvRunner pid=37492) {'red_0': -20.30000000000004, 'red_1': -11.099999999999993, 'blue_0': -24.500000000000043, 'blue_1': -35.60000000000021}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 776
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 866
(MultiAgentEnvRunner pid=37492) {'red_0': -17.80000000000003, 'red_1': -47.800000000000324, 'blue_0': -48.90000000000061, 'blue_1': -37.30000000000032}
ITERATION 155: reward=-146.02000000000106, metadata={'num_env_steps_sampled_lifetime': 936000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032276322801028155, 'timers': {'connectors': {'batch_individual_items': 9.36400686872991e-05, 'add_states_from_episodes_to_batch': 6.487967909135245e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2304564927644518e-05, 'numpy_to_tensor': 6.786624487518734e-05, 'agent_to_module_mapping': 8.110089634020845e-06, 'add_observations_from_episodes_to_batch': 3.8687165304927425e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008646245640099632, 'timers': {'connectors': {'get_actions': 0.0004433993267370147, 'un_batch_to_individual_items': 6.498645078806544e-05, 'tensor_to_numpy': 0.00011759193224382422, 'module_to_agent_unmapping': 6.520994300496291e-06, 'normalize_and_clip_actions': 7.285107764043713e-05, 'listify_data_for_vector_env': 2.3890145897589413e-05, 'remove_single_ts_time_rank_from_batch': 2.350472085498048e-06}}}, 'sample': 88.74779860000126, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.02000000000022, 'blue_0': -39.38000000000036, 'blue_1': -40.600000000000314, 'red_0': -31.020000000000174}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 155.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 936000.0, 'blue_0': 936000.0, 'blue_1': 936000.0, 'red_0': 936000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.02000000000022, 'blue_policy': -40.600000000000314}, 'num_module_steps_sampled_lifetime': {'red_policy': 1872000.0, 'blue_policy': 1872000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003475446161571544, 'episode_return_mean': -146.02000000000106, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -91.50000000000028, 'episode_duration_sec_mean': 17.62977684000507, 'episode_return_min': -174.5000000000015, 'rlmodule_inference_timer': 0.012593170156834017, 'num_episodes_lifetime': 780.0, 'episode_len_min': 1200, 'time_between_sampling': 258.18608359992504, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.286254924859865, 'throughput_since_last_restore': 15.895001243481964}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 281
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1146
(MultiAgentEnvRunner pid=37492) {'red_0': -32.60000000000023, 'red_1': -24.80000000000012, 'blue_0': -26.300000000000058, 'blue_1': -36.1000000000002}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 475
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1148
(MultiAgentEnvRunner pid=37492) {'red_0': -20.400000000000038, 'red_1': -15.999999999999973, 'blue_0': -54.500000000000554, 'blue_1': -21.90000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 593
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 852
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 852
(MultiAgentEnvRunner pid=37492) {'red_0': -20.30000000000003, 'red_1': -24.000000000000075, 'blue_0': -52.4000000000005, 'blue_1': -32.800000000000196}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 639
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 929
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 974
(MultiAgentEnvRunner pid=37492) {'red_0': -32.500000000000206, 'red_1': -28.10000000000015, 'blue_0': -48.20000000000042, 'blue_1': -51.40000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 940
(MultiAgentEnvRunner pid=37492) {'red_0': -19.300000000000004, 'red_1': -26.700000000000106, 'blue_0': -46.70000000000038, 'blue_1': -55.600000000000534}
ITERATION 156: reward=-134.12000000000086, metadata={'num_env_steps_sampled_lifetime': 942000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031955115262634847, 'timers': {'connectors': {'batch_individual_items': 9.534849839281452e-05, 'add_states_from_episodes_to_batch': 6.318413582117349e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2149915841394743e-05, 'numpy_to_tensor': 6.575298409463574e-05, 'agent_to_module_mapping': 7.956768095211835e-06, 'add_observations_from_episodes_to_batch': 3.756076282067671e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008544736485872034, 'timers': {'connectors': {'get_actions': 0.0004388777163683309, 'un_batch_to_individual_items': 6.503426737316887e-05, 'tensor_to_numpy': 0.00011699648781859344, 'module_to_agent_unmapping': 6.237266485586244e-06, 'normalize_and_clip_actions': 7.098657445865809e-05, 'listify_data_for_vector_env': 2.3296918936494053e-05, 'remove_single_ts_time_rank_from_batch': 2.2857843050380927e-06}}}, 'sample': 87.52056520001497, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -23.920000000000083, 'blue_0': -45.62000000000039, 'blue_1': -39.56000000000028, 'red_0': -25.020000000000103}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 156.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 942000.0, 'blue_0': 942000.0, 'blue_1': 942000.0, 'red_0': 942000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -23.920000000000083, 'blue_policy': -39.56000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 1884000.0, 'blue_policy': 1884000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033137203906515337, 'episode_return_mean': -134.12000000000086, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -112.80000000000061, 'episode_duration_sec_mean': 17.388131540012544, 'episode_return_min': -160.2000000000012, 'rlmodule_inference_timer': 0.012362627049790421, 'num_episodes_lifetime': 785.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3528078000527, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.384880765400478, 'throughput_since_last_restore': 15.90368180612939}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 404
(MultiAgentEnvRunner pid=37492) {'red_0': -17.299999999999976, 'red_1': -19.100000000000033, 'blue_0': -63.2000000000006, 'blue_1': -47.6000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -19.60000000000001, 'red_1': -18.39999999999999, 'blue_0': -54.80000000000049, 'blue_1': -39.10000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1118
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1182
(MultiAgentEnvRunner pid=37492) {'red_0': -28.600000000000065, 'red_1': -41.80000000000023, 'blue_0': -29.300000000000217, 'blue_1': -21.70000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 599
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 768
(MultiAgentEnvRunner pid=37492) {'red_0': -54.200000000000344, 'red_1': -37.30000000000019, 'blue_0': -32.30000000000042, 'blue_1': -22.600000000000136}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 230
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 257
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 284
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 764
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1108
(MultiAgentEnvRunner pid=37492) {'red_0': -10.099999999999982, 'red_1': -21.800000000000022, 'blue_0': -61.00000000000064, 'blue_1': -44.20000000000037}
ITERATION 157: reward=-136.80000000000095, metadata={'num_env_steps_sampled_lifetime': 948000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031918164066444785, 'timers': {'connectors': {'batch_individual_items': 9.693838660816299e-05, 'add_states_from_episodes_to_batch': 6.230806953133362e-06, 'add_time_dim_to_batch_and_zero_pad': 1.193469816570154e-05, 'numpy_to_tensor': 6.517473680264605e-05, 'agent_to_module_mapping': 7.904682971589203e-06, 'add_observations_from_episodes_to_batch': 3.7389950697838735e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008432947828057654, 'timers': {'connectors': {'get_actions': 0.00043801282027730506, 'un_batch_to_individual_items': 6.202694733099523e-05, 'tensor_to_numpy': 0.00011397444321275758, 'module_to_agent_unmapping': 6.157466439385334e-06, 'normalize_and_clip_actions': 6.929788350949961e-05, 'listify_data_for_vector_env': 2.302549284635392e-05, 'remove_single_ts_time_rank_from_batch': 2.33609698095985e-06}}}, 'sample': 88.69600620004348, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.680000000000092, 'blue_0': -48.12000000000047, 'blue_1': -35.04000000000029, 'red_0': -25.960000000000075}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 157.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 948000.0, 'blue_0': 948000.0, 'blue_1': 948000.0, 'red_0': 948000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.680000000000092, 'blue_policy': -35.04000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 1896000.0, 'blue_policy': 1896000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003317548684820538, 'episode_return_mean': -136.80000000000095, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -121.40000000000077, 'episode_duration_sec_mean': 17.60257757997606, 'episode_return_min': -147.200000000001, 'rlmodule_inference_timer': 0.012335506710655253, 'num_episodes_lifetime': 790.0, 'episode_len_min': 1200, 'time_between_sampling': 257.60763650003355, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.29333059736967, 'throughput_since_last_restore': 15.911773807142676}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 424
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 453
(MultiAgentEnvRunner pid=37492) {'red_0': -33.200000000000124, 'red_1': -23.400000000000027, 'blue_0': -23.900000000000045, 'blue_1': -36.50000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 594
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 754
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 787
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 787
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 808
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1016
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1183
(MultiAgentEnvRunner pid=37492) {'red_0': -41.800000000000246, 'red_1': -56.00000000000032, 'blue_0': -15.400000000000118, 'blue_1': -17.00000000000012}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 307
(MultiAgentEnvRunner pid=37492) {'red_0': -9.299999999999983, 'red_1': -14.199999999999958, 'blue_0': -38.300000000000274, 'blue_1': -47.00000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 680
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 842
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 856
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1052
(MultiAgentEnvRunner pid=37492) {'red_0': -35.300000000000196, 'red_1': -36.900000000000205, 'blue_0': -24.30000000000016, 'blue_1': -23.60000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 427
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 827
(MultiAgentEnvRunner pid=37492) {'red_0': -42.20000000000025, 'red_1': -19.20000000000005, 'blue_0': -34.800000000000196, 'blue_1': -46.30000000000038}
ITERATION 158: reward=-123.72000000000074, metadata={'num_env_steps_sampled_lifetime': 954000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003222558093069227, 'timers': {'connectors': {'batch_individual_items': 9.289310396547443e-05, 'add_states_from_episodes_to_batch': 6.473772426791638e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2307577085812973e-05, 'numpy_to_tensor': 6.67442544122336e-05, 'agent_to_module_mapping': 8.119988542934335e-06, 'add_observations_from_episodes_to_batch': 3.8862527648070054e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008692858161043507, 'timers': {'connectors': {'get_actions': 0.00044699997600488033, 'un_batch_to_individual_items': 6.518427297252492e-05, 'tensor_to_numpy': 0.00011831009899165844, 'module_to_agent_unmapping': 6.379648877539203e-06, 'normalize_and_clip_actions': 7.225745681849881e-05, 'listify_data_for_vector_env': 2.4230566870291735e-05, 'remove_single_ts_time_rank_from_batch': 2.364461017659419e-06}}}, 'sample': 87.72934550000355, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.94000000000011, 'blue_0': -27.340000000000156, 'blue_1': -34.0800000000003, 'red_0': -32.36000000000016}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 158.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 954000.0, 'blue_0': 954000.0, 'blue_1': 954000.0, 'red_0': 954000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.94000000000011, 'blue_policy': -34.0800000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 1908000.0, 'blue_policy': 1908000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034374791599384326, 'episode_return_mean': -123.72000000000074, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.80000000000061, 'episode_duration_sec_mean': 17.423426600010135, 'episode_return_min': -142.50000000000088, 'rlmodule_inference_timer': 0.01282978273230087, 'num_episodes_lifetime': 795.0, 'episode_len_min': 1200, 'time_between_sampling': 258.2622729999712, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.335473415634265, 'throughput_since_last_restore': 15.919996177683284}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 874
(MultiAgentEnvRunner pid=37492) {'red_0': -21.800000000000022, 'red_1': -26.600000000000147, 'blue_0': -51.70000000000038, 'blue_1': -43.00000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 361
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 950
(MultiAgentEnvRunner pid=37492) {'red_0': -20.700000000000028, 'red_1': -40.70000000000026, 'blue_0': -46.80000000000033, 'blue_1': -39.400000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1013
(MultiAgentEnvRunner pid=37492) {'red_0': -33.0000000000004, 'red_1': -21.1000000000001, 'blue_0': -60.0000000000005, 'blue_1': -54.60000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 657
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 930
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1190
(MultiAgentEnvRunner pid=37492) {'red_0': -47.70000000000037, 'red_1': -30.600000000000186, 'blue_0': -39.400000000000276, 'blue_1': -44.90000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -1.8000000000000005, 'red_1': -8.699999999999987, 'blue_0': -35.00000000000023, 'blue_1': -41.60000000000032}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -2.400000000000001, 'red_1': -0.1, 'blue_0': -114.69999999999776, 'blue_1': -114.59999999999776}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -1.4000000000000001, 'red_1': 0, 'blue_0': -0.6, 'blue_1': -0.5}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 08:01:06,258	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -6.799999999999992, 'red_1': 0, 'blue_0': -1.7000000000000004, 'blue_1': -7.099999999999991}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.1, 'blue_0': -116.79999999999764, 'blue_1': -117.3999999999976}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -117.3999999999976, 'red_1': -117.4999999999976, 'blue_0': -116.89999999999763, 'blue_1': -0.5}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 159: reward=-141.820000000001, metadata={'num_env_steps_sampled_lifetime': 960000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032723587553037074, 'timers': {'connectors': {'batch_individual_items': 9.697720655231504e-05, 'add_states_from_episodes_to_batch': 6.32805201596854e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2373318393545097e-05, 'numpy_to_tensor': 6.91554881529952e-05, 'agent_to_module_mapping': 8.981762182851003e-06, 'add_observations_from_episodes_to_batch': 3.805989968328909e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000871759782148776, 'timers': {'connectors': {'get_actions': 0.00044874455944384823, 'un_batch_to_individual_items': 6.640044656793372e-05, 'tensor_to_numpy': 0.00011750665327802522, 'module_to_agent_unmapping': 6.407239316221009e-06, 'normalize_and_clip_actions': 7.202755700044504e-05, 'listify_data_for_vector_env': 2.401138189433965e-05, 'remove_single_ts_time_rank_from_batch': 2.3201029965765862e-06}}}, 'sample': 89.30950129998382, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.54000000000014, 'blue_0': -46.58000000000034, 'blue_1': -44.700000000000365, 'red_0': -25.000000000000163}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 159.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 960000.0, 'blue_0': 960000.0, 'blue_1': 960000.0, 'red_0': 960000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.54000000000014, 'blue_policy': -44.700000000000365}, 'num_module_steps_sampled_lifetime': {'red_policy': 1920000.0, 'blue_policy': 1920000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003328935033174199, 'episode_return_mean': -141.820000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -87.10000000000053, 'episode_duration_sec_mean': 17.73943957998417, 'episode_return_min': -168.7000000000014, 'rlmodule_inference_timer': 0.012431430179940027, 'num_episodes_lifetime': 800.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3829741999507, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.774453971219394, 'throughput_since_last_restore': 15.904512278052623}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 747
(MultiAgentEnvRunner pid=37492) {'red_0': -15.499999999999961, 'red_1': -32.60000000000018, 'blue_0': -44.60000000000034, 'blue_1': -35.20000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -10.899999999999977, 'red_1': -6.5999999999999925, 'blue_0': -56.10000000000051, 'blue_1': -50.90000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 730
(MultiAgentEnvRunner pid=37492) {'red_0': -59.20000000000047, 'red_1': -15.999999999999977, 'blue_0': -20.300000000000026, 'blue_1': -51.00000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -2.3000000000000007, 'red_1': -2.1000000000000005, 'blue_0': -47.30000000000039, 'blue_1': -45.10000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 505
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1062
(MultiAgentEnvRunner pid=37492) {'red_0': -24.600000000000087, 'red_1': -23.000000000000068, 'blue_0': -36.10000000000029, 'blue_1': -34.90000000000034}
ITERATION 160: reward=-122.86000000000084, metadata={'num_env_steps_sampled_lifetime': 966000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003234604769793823, 'timers': {'connectors': {'batch_individual_items': 9.675962833672981e-05, 'add_states_from_episodes_to_batch': 6.4045075943725705e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2504956061736709e-05, 'numpy_to_tensor': 6.57259379289676e-05, 'agent_to_module_mapping': 8.085858508767279e-06, 'add_observations_from_episodes_to_batch': 3.793516399460272e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008608779008180651, 'timers': {'connectors': {'get_actions': 0.00043998514247254694, 'un_batch_to_individual_items': 6.601882631737065e-05, 'tensor_to_numpy': 0.00011742299769728393, 'module_to_agent_unmapping': 6.498390436253654e-06, 'normalize_and_clip_actions': 7.163125064652891e-05, 'listify_data_for_vector_env': 2.378496787418388e-05, 'remove_single_ts_time_rank_from_batch': 2.3108038297533873e-06}}}, 'sample': 89.71156129997689, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -16.060000000000045, 'blue_0': -40.880000000000315, 'blue_1': -43.42000000000038, 'red_0': -22.500000000000096}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 160.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 966000.0, 'blue_0': 966000.0, 'blue_1': 966000.0, 'red_0': 966000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -16.060000000000045, 'blue_policy': -43.42000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 1932000.0, 'blue_policy': 1932000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003416520258195405, 'episode_return_mean': -122.86000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.80000000000075, 'episode_duration_sec_mean': 17.82282870002091, 'episode_return_min': -146.50000000000097, 'rlmodule_inference_timer': 0.012502237407943534, 'num_episodes_lifetime': 805.0, 'episode_len_min': 1200, 'time_between_sampling': 346.28266919997986, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.217737931248475, 'throughput_since_last_restore': 15.91204964741086}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 348.4427768000169, 'restore_env_runners': 1.0699965059757233e-05, 'training_step': 348.44252739998046, 'env_runner_sampling_timer': 89.83896179997828, 'learner_update_timer': 258.54889340000227, 'synch_weights': 0.0117981000803411, 'synch_env_connectors': 0.0018621999770402908, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 966000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003234604769793823, 'timers': {'connectors': {'batch_individual_items': 9.675962833672981e-05, 'add_states_from_episodes_to_batch': 6.4045075943725705e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2504956061736709e-05, 'numpy_to_tensor': 6.57259379289676e-05, 'agent_to_module_mapping': 8.085858508767279e-06, 'add_observations_from_episodes_to_batch': 3.793516399460272e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008608779008180651, 'timers': {'connectors': {'get_actions': 0.00043998514247254694, 'un_batch_to_individual_items': 6.601882631737065e-05, 'tensor_to_numpy': 0.00011742299769728393, 'module_to_agent_unmapping': 6.498390436253654e-06, 'normalize_and_clip_actions': 7.163125064652891e-05, 'listify_data_for_vector_env': 2.378496787418388e-05, 'remove_single_ts_time_rank_from_batch': 2.3108038297533873e-06}}}, 'sample': 89.71156129997689, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -16.060000000000045, 'blue_0': -40.880000000000315, 'blue_1': -43.42000000000038, 'red_0': -22.500000000000096}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 160.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 966000.0, 'blue_0': 966000.0, 'blue_1': 966000.0, 'red_0': 966000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -16.060000000000045, 'blue_policy': -43.42000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 1932000.0, 'blue_policy': 1932000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003416520258195405, 'episode_return_mean': -122.86000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.80000000000075, 'episode_duration_sec_mean': 17.82282870002091, 'episode_return_min': -146.50000000000097, 'rlmodule_inference_timer': 0.012502237407943534, 'num_episodes_lifetime': 805.0, 'episode_len_min': 1200, 'time_between_sampling': 346.28266919997986, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.217737931248475, 'throughput_since_last_restore': 15.91204964741086}}, 'learners': {'red_policy': {'policy_loss': -0.1240537241101265, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.008768575266003609, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 161.0, 'num_module_steps_trained_lifetime': 58011520.0, 'curr_entropy_coeff': 0.03551, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00020339999999999998, 'vf_explained_var': -0.061658620834350586, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.863662719726562, 'entropy': 0.7658913731575012, 'vf_loss_unclipped': 744.3419189453125, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1033.9878157006929, 'throughput_since_last_restore': 955.5716161298024}}, 'blue_policy': {'weights_seq_no': 161.0, 'num_module_steps_trained_lifetime': 58011520.0, 'curr_entropy_coeff': 0.03551, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00020339999999999998, 'vf_explained_var': 0.43466585874557495, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 3.315553665161133, 'total_loss': 0.7763497829437256, 'entropy': 1.7160412073135376, 'policy_loss': -0.14062362909317017, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.9555860757827759, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014800573699176311, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1033.9860638864473, 'throughput_since_last_restore': 955.5716173780062}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.7999353632330894e-06, 'batch_individual_items': 0.7191047999076545, 'add_time_dim_to_batch_and_zero_pad': 2.0700041204690933e-05, 'numpy_to_tensor': 0.14499389997217804, 'add_observations_from_episodes_to_batch': 0.00028330006171017885, 'agent_to_module_mapping': 0.02014120004605502, 'add_one_ts_to_episodes_and_truncate': 0.1684477999806404, 'add_columns_from_episodes_to_train_batch': 0.4748141000745818, 'general_advantage_estimation': 13.040965000051074}}, 'connector_pipeline_timer': 14.569182500010356}, 'num_module_steps_trained_lifetime': 116023040.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 2719290000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 48468.05155579196, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 48468.049469511694, 'throughput_since_last_restore': 44792.419621037676}, 'num_module_steps_trained_throughput': 2067.9701053582467, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2067.970083994572, 'throughput_since_last_restore': 1911.1432365472544}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 966000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 161, 'trial_id': 'default', 'date': '2026-01-26_08-10-40', 'timestamp': 1769411440, 'time_this_iter_s': 348.456871509552, 'time_total_s': 60685.132536649704, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 60685.132536649704, 'iterations_since_restore': 161, 'perf': {'cpu_util_percent': 14.860160965794769, 'ram_util_percent': 90.30905432595571}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -25.8000000000001, 'red_1': -23.100000000000055, 'blue_0': -39.00000000000028, 'blue_1': -46.80000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 431
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 523
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 812
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1118
(MultiAgentEnvRunner pid=37492) {'red_0': -9.000000000000037, 'red_1': -39.000000000000355, 'blue_0': -62.00000000000052, 'blue_1': -45.10000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 223
(MultiAgentEnvRunner pid=37492) {'red_0': -8.099999999999987, 'red_1': -17.099999999999977, 'blue_0': -47.40000000000039, 'blue_1': -49.70000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 503
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 521
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 775
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1143
(MultiAgentEnvRunner pid=37492) {'red_0': -37.30000000000016, 'red_1': -39.30000000000021, 'blue_0': -27.200000000000315, 'blue_1': -49.80000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 275
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 405
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 471
(MultiAgentEnvRunner pid=37492) {'red_0': -38.30000000000021, 'red_1': -43.200000000000195, 'blue_0': -19.89999999999998, 'blue_1': -24.40000000000005}
ITERATION 161: reward=-138.30000000000086, metadata={'num_env_steps_sampled_lifetime': 972000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031680200535563286, 'timers': {'connectors': {'batch_individual_items': 9.460373587525174e-05, 'add_states_from_episodes_to_batch': 6.220281250729588e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1987144384136324e-05, 'numpy_to_tensor': 6.545870618936735e-05, 'agent_to_module_mapping': 7.929629601121202e-06, 'add_observations_from_episodes_to_batch': 3.731146102184975e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008615849950255822, 'timers': {'connectors': {'get_actions': 0.000440271267176733, 'un_batch_to_individual_items': 6.477646588148534e-05, 'tensor_to_numpy': 0.00011736238277641593, 'module_to_agent_unmapping': 6.529078555700424e-06, 'normalize_and_clip_actions': 7.220081046112527e-05, 'listify_data_for_vector_env': 2.4100401898174706e-05, 'remove_single_ts_time_rank_from_batch': 2.345690771425164e-06}}}, 'sample': 89.61371109995525, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.34000000000016, 'blue_0': -39.10000000000029, 'blue_1': -43.16000000000032, 'red_0': -23.7000000000001}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 161.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 972000.0, 'blue_0': 972000.0, 'blue_1': 972000.0, 'red_0': 972000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.34000000000016, 'blue_policy': -43.16000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 1944000.0, 'blue_policy': 1944000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034321957789750163, 'episode_return_mean': -138.30000000000086, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -122.30000000000078, 'episode_duration_sec_mean': 17.783558299974537, 'episode_return_min': -155.1000000000012, 'rlmodule_inference_timer': 0.012469153639315653, 'num_episodes_lifetime': 810.0, 'episode_len_min': 1200, 'time_between_sampling': 258.8436144000152, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.234767904802727, 'throughput_since_last_restore': 15.919590905132702}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 764
(MultiAgentEnvRunner pid=37492) {'red_0': -14.899999999999972, 'red_1': -20.900000000000038, 'blue_0': -41.80000000000038, 'blue_1': -23.50000000000012}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 751
(MultiAgentEnvRunner pid=37492) {'red_0': -22.60000000000005, 'red_1': -29.500000000000174, 'blue_0': -43.800000000000345, 'blue_1': -56.0000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 403
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1164
(MultiAgentEnvRunner pid=37492) {'red_0': -49.80000000000036, 'red_1': -51.400000000000375, 'blue_0': -20.00000000000022, 'blue_1': -35.60000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 864
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 865
(MultiAgentEnvRunner pid=37492) {'red_0': -32.20000000000018, 'red_1': -62.20000000000057, 'blue_0': -49.200000000000415, 'blue_1': -40.80000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 565
(MultiAgentEnvRunner pid=37492) {'red_0': -27.000000000000043, 'red_1': -36.00000000000016, 'blue_0': -37.20000000000033, 'blue_1': -31.300000000000143}
ITERATION 162: reward=-145.140000000001, metadata={'num_env_steps_sampled_lifetime': 978000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003200217982803712, 'timers': {'connectors': {'batch_individual_items': 9.592254358310518e-05, 'add_states_from_episodes_to_batch': 6.313814835886715e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2133951790151787e-05, 'numpy_to_tensor': 6.499293138195091e-05, 'agent_to_module_mapping': 7.9195037307171e-06, 'add_observations_from_episodes_to_batch': 3.80121389438605e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.00087310109206143, 'timers': {'connectors': {'get_actions': 0.0004458862978833009, 'un_batch_to_individual_items': 6.512162583340988e-05, 'tensor_to_numpy': 0.00012048236454140661, 'module_to_agent_unmapping': 6.5140383580073525e-06, 'normalize_and_clip_actions': 7.245074762366187e-05, 'listify_data_for_vector_env': 2.468612258646194e-05, 'remove_single_ts_time_rank_from_batch': 2.348430794924447e-06}}}, 'sample': 89.36428370000795, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -40.00000000000026, 'blue_0': -38.40000000000034, 'blue_1': -37.44000000000029, 'red_0': -29.300000000000125}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 162.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 978000.0, 'blue_0': 978000.0, 'blue_1': 978000.0, 'red_0': 978000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -40.00000000000026, 'blue_policy': -37.44000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 1956000.0, 'blue_policy': 1956000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003449025446442039, 'episode_return_mean': -145.140000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.1000000000005, 'episode_duration_sec_mean': 17.7515348599758, 'episode_return_min': -184.4000000000015, 'rlmodule_inference_timer': 0.012568138205621021, 'num_episodes_lifetime': 815.0, 'episode_len_min': 1200, 'time_between_sampling': 258.44858660001773, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.27369846469906, 'throughput_since_last_restore': 15.927250076877591}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 986
(MultiAgentEnvRunner pid=37492) {'red_0': -23.499999999999993, 'red_1': -23.699999999999996, 'blue_0': -22.600000000000264, 'blue_1': -36.50000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 289
(MultiAgentEnvRunner pid=37492) {'red_0': -22.699999999999985, 'red_1': -7.999999999999996, 'blue_0': -31.30000000000016, 'blue_1': -44.70000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1170
(MultiAgentEnvRunner pid=37492) {'red_0': -7.699999999999989, 'red_1': -29.400000000000173, 'blue_0': -40.700000000000294, 'blue_1': -48.30000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -23.300000000000065, 'red_1': -41.20000000000031, 'blue_0': -40.50000000000029, 'blue_1': -42.40000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -20.600000000000026, 'red_1': -12.299999999999972, 'blue_0': -41.900000000000325, 'blue_1': -49.800000000000416}
ITERATION 163: reward=-122.22000000000074, metadata={'num_env_steps_sampled_lifetime': 984000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000322374788915907, 'timers': {'connectors': {'batch_individual_items': 9.649046926283767e-05, 'add_states_from_episodes_to_batch': 6.284793676504178e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1993098082142318e-05, 'numpy_to_tensor': 6.579095062523373e-05, 'agent_to_module_mapping': 8.043414449827543e-06, 'add_observations_from_episodes_to_batch': 3.8371018550412246e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008555700377772934, 'timers': {'connectors': {'get_actions': 0.0004419169761630718, 'un_batch_to_individual_items': 6.424908785145978e-05, 'tensor_to_numpy': 0.0001153413262661745, 'module_to_agent_unmapping': 6.388966299039329e-06, 'normalize_and_clip_actions': 7.087955879556106e-05, 'listify_data_for_vector_env': 2.34042414133921e-05, 'remove_single_ts_time_rank_from_batch': 2.342406499000453e-06}}}, 'sample': 88.1312797999708, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.920000000000087, 'blue_0': -35.40000000000027, 'blue_1': -44.34000000000036, 'red_0': -19.56000000000001}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 163.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 984000.0, 'blue_0': 984000.0, 'blue_1': 984000.0, 'red_0': 984000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.920000000000087, 'blue_policy': -44.34000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 1968000.0, 'blue_policy': 1968000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000331423035641906, 'episode_return_mean': -122.22000000000074, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -106.30000000000058, 'episode_duration_sec_mean': 17.493326439964584, 'episode_return_min': -147.400000000001, 'rlmodule_inference_timer': 0.012429262114771217, 'num_episodes_lifetime': 820.0, 'episode_len_min': 1200, 'time_between_sampling': 257.98326880007517, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.318855424081907, 'throughput_since_last_restore': 15.935056887577971}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1121
(MultiAgentEnvRunner pid=37492) {'red_0': -20.10000000000002, 'red_1': -12.99999999999997, 'blue_0': -54.900000000000524, 'blue_1': -53.60000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 175
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 409
(MultiAgentEnvRunner pid=37492) {'red_0': -14.099999999999953, 'red_1': -6.999999999999993, 'blue_0': -48.50000000000046, 'blue_1': -62.200000000000635}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -13.799999999999969, 'red_1': -20.700000000000028, 'blue_0': -45.200000000000365, 'blue_1': -37.000000000000256}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 926
(MultiAgentEnvRunner pid=37492) {'red_0': -69.70000000000032, 'red_1': -21.90000000000004, 'blue_0': -54.80000000000049, 'blue_1': -49.00000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 342
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 833
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 875
(MultiAgentEnvRunner pid=37492) {'red_0': -56.300000000000445, 'red_1': -51.100000000000385, 'blue_0': -39.600000000000385, 'blue_1': -22.10000000000019}
ITERATION 164: reward=-150.92000000000104, metadata={'num_env_steps_sampled_lifetime': 990000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003255104707489921, 'timers': {'connectors': {'batch_individual_items': 9.741542496669729e-05, 'add_states_from_episodes_to_batch': 6.314149544269723e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2294479110963483e-05, 'numpy_to_tensor': 6.684819926033152e-05, 'agent_to_module_mapping': 7.995206030940898e-06, 'add_observations_from_episodes_to_batch': 3.8476040819753416e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000863458629134801, 'timers': {'connectors': {'get_actions': 0.0004429930214793786, 'un_batch_to_individual_items': 6.455270957970413e-05, 'tensor_to_numpy': 0.00011846350402461185, 'module_to_agent_unmapping': 6.306308417602818e-06, 'normalize_and_clip_actions': 7.194369111051407e-05, 'listify_data_for_vector_env': 2.4082265549774492e-05, 'remove_single_ts_time_rank_from_batch': 2.4113480977589885e-06}}}, 'sample': 88.53139929997269, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.740000000000084, 'blue_0': -48.60000000000045, 'blue_1': -44.7800000000004, 'red_0': -34.80000000000014}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 164.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 990000.0, 'blue_0': 990000.0, 'blue_1': 990000.0, 'red_0': 990000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.740000000000084, 'blue_policy': -44.7800000000004}, 'num_module_steps_sampled_lifetime': {'red_policy': 1980000.0, 'blue_policy': 1980000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000351171795525756, 'episode_return_mean': -150.92000000000104, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -116.70000000000061, 'episode_duration_sec_mean': 17.581864400021733, 'episode_return_min': -195.40000000000128, 'rlmodule_inference_timer': 0.012550439834683005, 'num_episodes_lifetime': 825.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3184650000185, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.25842617201643, 'throughput_since_last_restore': 15.942465178748137}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -17.69999999999998, 'red_1': -24.800000000000082, 'blue_0': -42.70000000000034, 'blue_1': -39.40000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 335
(MultiAgentEnvRunner pid=37492) {'red_0': -33.400000000000205, 'red_1': -16.39999999999998, 'blue_0': -32.900000000000176, 'blue_1': -27.000000000000096}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 190
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 330
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 548
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 992
(MultiAgentEnvRunner pid=37492) {'red_0': -47.6000000000003, 'red_1': -19.20000000000001, 'blue_0': -45.90000000000051, 'blue_1': -23.20000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 246
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 465
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 465
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 527
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 546
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 563
(MultiAgentEnvRunner pid=37492) {'red_0': -30.700000000000145, 'red_1': -42.200000000000315, 'blue_0': -19.299999999999976, 'blue_1': -25.80000000000006}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 492
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 510
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 566
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 709
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 709
(MultiAgentEnvRunner pid=37492) {'red_0': -5.399999999999995, 'red_1': -19.800000000000082, 'blue_0': -39.400000000000404, 'blue_1': -43.1000000000003}
ITERATION 165: reward=-119.18000000000072, metadata={'num_env_steps_sampled_lifetime': 996000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003170343768167932, 'timers': {'connectors': {'batch_individual_items': 9.55621464210855e-05, 'add_states_from_episodes_to_batch': 6.211072752136953e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3094881297609e-05, 'numpy_to_tensor': 6.455770458036505e-05, 'agent_to_module_mapping': 7.767150830250301e-06, 'add_observations_from_episodes_to_batch': 3.711449578930257e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008569393377144102, 'timers': {'connectors': {'get_actions': 0.0004409480505728886, 'un_batch_to_individual_items': 6.444395225159774e-05, 'tensor_to_numpy': 0.00011644782654331948, 'module_to_agent_unmapping': 6.581313634829947e-06, 'normalize_and_clip_actions': 6.960792030200539e-05, 'listify_data_for_vector_env': 2.5047946598184514e-05, 'remove_single_ts_time_rank_from_batch': 2.3414385265712195e-06}}}, 'sample': 89.00480589992367, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -24.480000000000096, 'blue_0': -36.04000000000028, 'blue_1': -31.700000000000216, 'red_0': -26.96000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 165.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 996000.0, 'blue_0': 996000.0, 'blue_1': 996000.0, 'red_0': 996000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -24.480000000000096, 'blue_policy': -31.700000000000216}, 'num_module_steps_sampled_lifetime': {'red_policy': 1992000.0, 'blue_policy': 1992000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033373660561956015, 'episode_return_mean': -119.18000000000072, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -107.70000000000078, 'episode_duration_sec_mean': 17.683701599994674, 'episode_return_min': -135.90000000000114, 'rlmodule_inference_timer': 0.012476064183077056, 'num_episodes_lifetime': 830.0, 'episode_len_min': 1200, 'time_between_sampling': 259.1245584000135, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.235713292477215, 'throughput_since_last_restore': 15.949673954186416}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 540
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 707
(MultiAgentEnvRunner pid=37492) {'red_0': -55.90000000000037, 'red_1': -48.50000000000027, 'blue_0': -21.49999999999998, 'blue_1': -20.89999999999997}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 801
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 980
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 980
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1008
(MultiAgentEnvRunner pid=37492) {'red_0': -42.40000000000024, 'red_1': -21.10000000000002, 'blue_0': -33.70000000000041, 'blue_1': -26.700000000000365}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 967
(MultiAgentEnvRunner pid=37492) {'red_0': -16.699999999999985, 'red_1': -9.899999999999997, 'blue_0': -30.70000000000038, 'blue_1': -35.5000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 410
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1149
(MultiAgentEnvRunner pid=37492) {'red_0': -46.200000000000294, 'red_1': -34.10000000000014, 'blue_0': -35.60000000000029, 'blue_1': -39.700000000000266}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 445
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 522
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 580
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 642
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1132
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1132
(MultiAgentEnvRunner pid=37492) {'red_0': -18.200000000000053, 'red_1': -36.00000000000024, 'blue_0': -29.900000000000166, 'blue_1': -23.900000000000002}
ITERATION 166: reward=-125.42000000000075, metadata={'num_env_steps_sampled_lifetime': 1002000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003225075626679663, 'timers': {'connectors': {'batch_individual_items': 9.606465363824163e-05, 'add_states_from_episodes_to_batch': 6.530464130082438e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2098900291978508e-05, 'numpy_to_tensor': 6.696678042679202e-05, 'agent_to_module_mapping': 7.945326518779428e-06, 'add_observations_from_episodes_to_batch': 3.806358179967126e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008568126375713943, 'timers': {'connectors': {'get_actions': 0.0004425191722664279, 'un_batch_to_individual_items': 6.298827328692945e-05, 'tensor_to_numpy': 0.00011773933516024264, 'module_to_agent_unmapping': 6.346572895195979e-06, 'normalize_and_clip_actions': 7.065048431259622e-05, 'listify_data_for_vector_env': 2.3529230259330843e-05, 'remove_single_ts_time_rank_from_batch': 2.286921492584767e-06}}}, 'sample': 89.00685010000598, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.92000000000013, 'blue_0': -30.280000000000246, 'blue_1': -29.34000000000018, 'red_0': -35.88000000000019}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 166.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1002000.0, 'blue_0': 1002000.0, 'blue_1': 1002000.0, 'red_0': 1002000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.92000000000013, 'blue_policy': -29.34000000000018}, 'num_module_steps_sampled_lifetime': {'red_policy': 2004000.0, 'blue_policy': 2004000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033972162108624404, 'episode_return_mean': -125.42000000000075, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -92.80000000000067, 'episode_duration_sec_mean': 17.657137400005013, 'episode_return_min': -155.600000000001, 'rlmodule_inference_timer': 0.012521283015871819, 'num_episodes_lifetime': 835.0, 'episode_len_min': 1200, 'time_between_sampling': 259.11076910002157, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.276698128871434, 'throughput_since_last_restore': 15.95701266445185}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -35.40000000000022, 'red_1': -41.80000000000032, 'blue_0': -37.60000000000026, 'blue_1': -41.600000000000314}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 590
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 596
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 596
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 652
(MultiAgentEnvRunner pid=37492) {'red_0': -16.09999999999998, 'red_1': -19.50000000000003, 'blue_0': -26.900000000000087, 'blue_1': -22.000000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 381
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 449
(MultiAgentEnvRunner pid=37492) {'red_0': -28.300000000000065, 'red_1': -49.20000000000033, 'blue_0': -40.9000000000003, 'blue_1': -18.199999999999967}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 530
(MultiAgentEnvRunner pid=37492) {'red_0': -12.499999999999972, 'red_1': -24.000000000000068, 'blue_0': -35.40000000000023, 'blue_1': -48.10000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 299
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 434
(MultiAgentEnvRunner pid=37492) {'red_0': -52.00000000000037, 'red_1': -52.000000000000384, 'blue_0': -55.90000000000056, 'blue_1': -31.800000000000143}
ITERATION 167: reward=-137.8400000000008, metadata={'num_env_steps_sampled_lifetime': 1008000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003289462358195569, 'timers': {'connectors': {'batch_individual_items': 0.00010055326480801918, 'add_states_from_episodes_to_batch': 6.431182753096605e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2140759348872615e-05, 'numpy_to_tensor': 6.6619023228813e-05, 'agent_to_module_mapping': 8.107498080591825e-06, 'add_observations_from_episodes_to_batch': 3.8709564137275365e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008522632207487289, 'timers': {'connectors': {'get_actions': 0.00043879144197669803, 'un_batch_to_individual_items': 6.447764279160372e-05, 'tensor_to_numpy': 0.00011657922692093523, 'module_to_agent_unmapping': 6.331137138295368e-06, 'normalize_and_clip_actions': 7.014695315971151e-05, 'listify_data_for_vector_env': 2.302309823799048e-05, 'remove_single_ts_time_rank_from_batch': 2.2409532933920357e-06}}}, 'sample': 88.94270779995713, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -37.300000000000225, 'blue_0': -39.34000000000028, 'blue_1': -32.34000000000017, 'red_0': -28.86000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 167.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1008000.0, 'blue_0': 1008000.0, 'blue_1': 1008000.0, 'red_0': 1008000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -37.300000000000225, 'blue_policy': -32.34000000000017}, 'num_module_steps_sampled_lifetime': {'red_policy': 2016000.0, 'blue_policy': 2016000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033945982872843436, 'episode_return_mean': -137.8400000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -84.50000000000013, 'episode_duration_sec_mean': 17.66441851996351, 'episode_return_min': -191.70000000000147, 'rlmodule_inference_timer': 0.012426926053461565, 'num_episodes_lifetime': 840.0, 'episode_len_min': 1200, 'time_between_sampling': 258.281329200021, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.240408402869072, 'throughput_since_last_restore': 15.964085821449165}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 311
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1026
(MultiAgentEnvRunner pid=37492) {'red_0': -15.99999999999998, 'red_1': -19.400000000000023, 'blue_0': -39.30000000000035, 'blue_1': -25.600000000000307}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -37.900000000000254, 'red_1': -27.700000000000127, 'blue_0': -35.70000000000024, 'blue_1': -42.400000000000325}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 237
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 238
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 238
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 554
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 989
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1071
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1071
(MultiAgentEnvRunner pid=37492) {'red_0': -18.699999999999992, 'red_1': -31.10000000000015, 'blue_0': -38.4000000000005, 'blue_1': -42.5000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 370
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 621
(MultiAgentEnvRunner pid=37492) {'red_0': -28.800000000000125, 'red_1': -19.800000000000043, 'blue_0': -53.70000000000048, 'blue_1': -40.40000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 297
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 806
(MultiAgentEnvRunner pid=37492) {'red_0': -43.90000000000034, 'red_1': -45.60000000000036, 'blue_0': -34.90000000000024, 'blue_1': -40.500000000000306}
ITERATION 168: reward=-136.46000000000097, metadata={'num_env_steps_sampled_lifetime': 1014000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003221989952070423, 'timers': {'connectors': {'batch_individual_items': 9.715782774931438e-05, 'add_states_from_episodes_to_batch': 6.842222855216891e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2136995218417338e-05, 'numpy_to_tensor': 6.570910362093477e-05, 'agent_to_module_mapping': 7.932194893912306e-06, 'add_observations_from_episodes_to_batch': 3.802405056973716e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008487022014549661, 'timers': {'connectors': {'get_actions': 0.00043654806803769016, 'un_batch_to_individual_items': 6.345032077222908e-05, 'tensor_to_numpy': 0.00011515056387030607, 'module_to_agent_unmapping': 6.329823770067697e-06, 'normalize_and_clip_actions': 7.073203858352857e-05, 'listify_data_for_vector_env': 2.326407105025863e-05, 'remove_single_ts_time_rank_from_batch': 2.3084249514876125e-06}}}, 'sample': 89.2419126998866, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.72000000000014, 'blue_0': -40.40000000000036, 'blue_1': -38.28000000000033, 'red_0': -29.060000000000137}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 168.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1014000.0, 'blue_0': 1014000.0, 'blue_1': 1014000.0, 'red_0': 1014000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.72000000000014, 'blue_policy': -38.28000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2028000.0, 'blue_policy': 2028000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033833494585932096, 'episode_return_mean': -136.46000000000097, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -100.30000000000067, 'episode_duration_sec_mean': 17.70987894004211, 'episode_return_min': -164.90000000000126, 'rlmodule_inference_timer': 0.012468740496955167, 'num_episodes_lifetime': 845.0, 'episode_len_min': 1200, 'time_between_sampling': 259.0851974999532, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.251753776413764, 'throughput_since_last_restore': 15.971138893632073}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 392
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 695
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 696
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 712
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 712
(MultiAgentEnvRunner pid=37492) {'red_0': -20.799999999999986, 'red_1': -19.999999999999964, 'blue_0': -51.00000000000044, 'blue_1': -53.60000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.099999999999964, 'red_1': -9.799999999999981, 'blue_0': -51.900000000000446, 'blue_1': -44.40000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 319
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 547
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 853
(MultiAgentEnvRunner pid=37492) {'red_0': -26.500000000000107, 'red_1': -24.400000000000098, 'blue_0': -30.000000000000217, 'blue_1': -15.699999999999958}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -12.89999999999997, 'red_1': -5.399999999999997, 'blue_0': -39.100000000000286, 'blue_1': -50.80000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.99999999999996, 'red_1': -25.10000000000009, 'blue_0': -41.800000000000324, 'blue_1': -40.10000000000029}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.1, 'blue_0': -0.8999999999999999, 'blue_1': -0.7}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -116.19999999999767, 'red_1': -115.7999999999977, 'blue_0': -115.7999999999977, 'blue_1': -116.09999999999768}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 09:00:31,096	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -109.09999999999808, 'red_1': -95.29999999999886, 'blue_0': -95.99999999999882, 'blue_1': -98.0999999999987}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -1.5000000000000002, 'red_1': 0, 'blue_0': -116.59999999999765, 'blue_1': -0.7999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.1, 'blue_0': -0.4, 'blue_1': -0.4}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 169: reward=-118.88000000000065, metadata={'num_env_steps_sampled_lifetime': 1020000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032013469573923144, 'timers': {'connectors': {'batch_individual_items': 9.535591057778189e-05, 'add_states_from_episodes_to_batch': 6.342366637886807e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2031239533142325e-05, 'numpy_to_tensor': 6.580580179322136e-05, 'agent_to_module_mapping': 8.10997674272619e-06, 'add_observations_from_episodes_to_batch': 3.7720776558580696e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008576734573940612, 'timers': {'connectors': {'get_actions': 0.00044454669478892527, 'un_batch_to_individual_items': 6.375512822774922e-05, 'tensor_to_numpy': 0.00011639278080836724, 'module_to_agent_unmapping': 6.364594886200661e-06, 'normalize_and_clip_actions': 6.993936366349557e-05, 'listify_data_for_vector_env': 2.3383909761071944e-05, 'remove_single_ts_time_rank_from_batch': 2.2637785577973636e-06}}}, 'sample': 89.17953910003416, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -16.940000000000026, 'blue_0': -42.760000000000346, 'blue_1': -40.920000000000286, 'red_0': -18.259999999999998}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 169.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1020000.0, 'blue_0': 1020000.0, 'blue_1': 1020000.0, 'red_0': 1020000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -16.940000000000026, 'blue_policy': -40.920000000000286}, 'num_module_steps_sampled_lifetime': {'red_policy': 2040000.0, 'blue_policy': 2040000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003361183118413654, 'episode_return_mean': -118.88000000000065, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.60000000000038, 'episode_duration_sec_mean': 17.713547820015812, 'episode_return_min': -145.40000000000077, 'rlmodule_inference_timer': 0.012464678637397053, 'num_episodes_lifetime': 850.0, 'episode_len_min': 1200, 'time_between_sampling': 258.5509373000823, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.76212325736893, 'throughput_since_last_restore': 15.956072471224825}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 360
(MultiAgentEnvRunner pid=37492) {'red_0': -28.300000000000136, 'red_1': -10.499999999999979, 'blue_0': -48.50000000000041, 'blue_1': -42.80000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 206
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 886
(MultiAgentEnvRunner pid=37492) {'red_0': -15.799999999999978, 'red_1': -10.899999999999995, 'blue_0': -37.00000000000046, 'blue_1': -47.100000000000456}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 416
(MultiAgentEnvRunner pid=37492) {'red_0': -47.600000000000364, 'red_1': -23.40000000000008, 'blue_0': -53.30000000000054, 'blue_1': -31.300000000000136}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 320
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1127
(MultiAgentEnvRunner pid=37492) {'red_0': -10.399999999999997, 'red_1': -33.20000000000013, 'blue_0': -31.00000000000037, 'blue_1': -39.200000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 383
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 544
(MultiAgentEnvRunner pid=37492) {'red_0': -13.899999999999938, 'red_1': -0.8000000000000024, 'blue_0': -40.00000000000031, 'blue_1': -41.40000000000036}
ITERATION 170: reward=-121.28000000000085, metadata={'num_env_steps_sampled_lifetime': 1026000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00030289491465532176, 'timers': {'connectors': {'batch_individual_items': 9.00206045695677e-05, 'add_states_from_episodes_to_batch': 5.938466819324469e-06, 'add_time_dim_to_batch_and_zero_pad': 1.149914615240261e-05, 'numpy_to_tensor': 6.352145317377776e-05, 'agent_to_module_mapping': 7.610458105155847e-06, 'add_observations_from_episodes_to_batch': 3.606874989770803e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008186347462841054, 'timers': {'connectors': {'get_actions': 0.0004242559950663744, 'un_batch_to_individual_items': 6.078077847547148e-05, 'tensor_to_numpy': 0.00011203655108743599, 'module_to_agent_unmapping': 5.94243551198221e-06, 'normalize_and_clip_actions': 6.673655174272945e-05, 'listify_data_for_vector_env': 2.2058543076537833e-05, 'remove_single_ts_time_rank_from_batch': 2.160299107322067e-06}}}, 'sample': 85.67418079997879, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -15.760000000000037, 'blue_0': -41.96000000000042, 'blue_1': -40.36000000000033, 'red_0': -23.20000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 170.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1026000.0, 'blue_0': 1026000.0, 'blue_1': 1026000.0, 'red_0': 1026000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -15.760000000000037, 'blue_policy': -40.36000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2052000.0, 'blue_policy': 2052000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033395332397125364, 'episode_return_mean': -121.28000000000085, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.10000000000062, 'episode_duration_sec_mean': 17.017556459992193, 'episode_return_min': -155.6000000000011, 'rlmodule_inference_timer': 0.011796356618574666, 'num_episodes_lifetime': 855.0, 'episode_len_min': 1200, 'time_between_sampling': 346.798080699984, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.683963226326078, 'throughput_since_last_restore': 15.965194462967403}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 339.2575818999903, 'restore_env_runners': 7.6000578701496124e-06, 'training_step': 339.25736479996704, 'env_runner_sampling_timer': 85.79086389997974, 'learner_update_timer': 253.41440829995554, 'synch_weights': 0.012887599994428456, 'synch_env_connectors': 0.0027305999537929893, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1026000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00030289491465532176, 'timers': {'connectors': {'batch_individual_items': 9.00206045695677e-05, 'add_states_from_episodes_to_batch': 5.938466819324469e-06, 'add_time_dim_to_batch_and_zero_pad': 1.149914615240261e-05, 'numpy_to_tensor': 6.352145317377776e-05, 'agent_to_module_mapping': 7.610458105155847e-06, 'add_observations_from_episodes_to_batch': 3.606874989770803e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008186347462841054, 'timers': {'connectors': {'get_actions': 0.0004242559950663744, 'un_batch_to_individual_items': 6.078077847547148e-05, 'tensor_to_numpy': 0.00011203655108743599, 'module_to_agent_unmapping': 5.94243551198221e-06, 'normalize_and_clip_actions': 6.673655174272945e-05, 'listify_data_for_vector_env': 2.2058543076537833e-05, 'remove_single_ts_time_rank_from_batch': 2.160299107322067e-06}}}, 'sample': 85.67418079997879, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -15.760000000000037, 'blue_0': -41.96000000000042, 'blue_1': -40.36000000000033, 'red_0': -23.20000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 170.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1026000.0, 'blue_0': 1026000.0, 'blue_1': 1026000.0, 'red_0': 1026000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -15.760000000000037, 'blue_policy': -40.36000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2052000.0, 'blue_policy': 2052000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033395332397125364, 'episode_return_mean': -121.28000000000085, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.10000000000062, 'episode_duration_sec_mean': 17.017556459992193, 'episode_return_min': -155.6000000000011, 'rlmodule_inference_timer': 0.011796356618574666, 'num_episodes_lifetime': 855.0, 'episode_len_min': 1200, 'time_between_sampling': 346.798080699984, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.683963226326078, 'throughput_since_last_restore': 15.965194462967403}}, 'learners': {'red_policy': {'policy_loss': -0.2707829475402832, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.007757293060421944, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 171.0, 'num_module_steps_trained_lifetime': 61614720.0, 'curr_entropy_coeff': 0.03461, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00019739999999999997, 'vf_explained_var': -0.6387823820114136, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.716630935668945, 'entropy': 0.7446919679641724, 'vf_loss_unclipped': 755.241943359375, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1061.9854689868666, 'throughput_since_last_restore': 958.7631399256375}}, 'blue_policy': {'weights_seq_no': 171.0, 'num_module_steps_trained_lifetime': 61614720.0, 'curr_entropy_coeff': 0.03461, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00019739999999999997, 'vf_explained_var': 0.6175493001937866, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 3.490708351135254, 'total_loss': 0.6938117742538452, 'entropy': 1.70131254196167, 'policy_loss': -0.30108243227005005, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.0319175720214844, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014493520371615887, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1061.9839011565948, 'throughput_since_last_restore': 958.7631410072624}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.4999330788850784e-06, 'batch_individual_items': 0.8067058000015095, 'add_time_dim_to_batch_and_zero_pad': 2.680008765310049e-05, 'numpy_to_tensor': 0.12307000008877367, 'add_observations_from_episodes_to_batch': 0.00024389999452978373, 'agent_to_module_mapping': 0.019809500081464648, 'add_one_ts_to_episodes_and_truncate': 0.16560559999197721, 'add_columns_from_episodes_to_train_batch': 0.44475709996186197, 'general_advantage_estimation': 13.074168700026348}}, 'connector_pipeline_timer': 14.634760600049049}, 'num_module_steps_trained_lifetime': 123229440.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 2888190000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 49780.45809989913, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 49780.455708368674, 'throughput_since_last_restore': 44942.02228667531}, 'num_module_steps_trained_throughput': 2123.966092695497, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 2123.966056387744, 'throughput_since_last_restore': 1917.5262836556076}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1026000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 171, 'trial_id': 'default', 'date': '2026-01-26_09-09-57', 'timestamp': 1769414997, 'time_this_iter_s': 339.27259397506714, 'time_total_s': 64240.9277818203, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 64240.9277818203, 'iterations_since_restore': 171, 'perf': {'cpu_util_percent': 15.245247933884299, 'ram_util_percent': 92.274173553719}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 496
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1086
(MultiAgentEnvRunner pid=37492) {'red_0': -56.60000000000044, 'red_1': -47.10000000000035, 'blue_0': -35.300000000000296, 'blue_1': -36.20000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 843
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 871
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1021
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1174
(MultiAgentEnvRunner pid=37492) {'red_0': -46.80000000000024, 'red_1': -28.500000000000178, 'blue_0': -19.900000000000183, 'blue_1': -21.50000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 895
(MultiAgentEnvRunner pid=37492) {'red_0': -53.200000000000465, 'red_1': -18.699999999999996, 'blue_0': -39.60000000000032, 'blue_1': -40.4000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 480
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 507
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 580
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 872
(MultiAgentEnvRunner pid=37492) {'red_0': -52.3000000000004, 'red_1': -32.4000000000002, 'blue_0': -37.400000000000276, 'blue_1': -38.10000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 751
(MultiAgentEnvRunner pid=37492) {'red_0': -35.10000000000043, 'red_1': -19.200000000000067, 'blue_0': -55.40000000000043, 'blue_1': -53.50000000000041}
ITERATION 171: reward=-153.4400000000012, metadata={'num_env_steps_sampled_lifetime': 1032000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031885065472116153, 'timers': {'connectors': {'batch_individual_items': 9.449093947328562e-05, 'add_states_from_episodes_to_batch': 6.2682260275704835e-06, 'add_time_dim_to_batch_and_zero_pad': 1.206014527371084e-05, 'numpy_to_tensor': 6.571941139847205e-05, 'agent_to_module_mapping': 7.910133056712101e-06, 'add_observations_from_episodes_to_batch': 3.8580348358569204e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008488987670033848, 'timers': {'connectors': {'get_actions': 0.0004393426469264192, 'un_batch_to_individual_items': 6.256239747335919e-05, 'tensor_to_numpy': 0.0001155880759272793, 'module_to_agent_unmapping': 6.2638779675368915e-06, 'normalize_and_clip_actions': 7.032244464026739e-05, 'listify_data_for_vector_env': 2.3719623086904827e-05, 'remove_single_ts_time_rank_from_batch': 2.2436470333671602e-06}}}, 'sample': 88.88573199999519, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.18000000000016, 'blue_0': -37.5200000000003, 'blue_1': -37.940000000000325, 'red_0': -48.800000000000395}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 171.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1032000.0, 'blue_0': 1032000.0, 'blue_1': 1032000.0, 'red_0': 1032000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.18000000000016, 'blue_policy': -37.940000000000325}, 'num_module_steps_sampled_lifetime': {'red_policy': 2064000.0, 'blue_policy': 2064000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003389300420310218, 'episode_return_mean': -153.4400000000012, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -116.70000000000084, 'episode_duration_sec_mean': 17.64106795999687, 'episode_return_min': -175.20000000000152, 'rlmodule_inference_timer': 0.012421664448379015, 'num_episodes_lifetime': 860.0, 'episode_len_min': 1200, 'time_between_sampling': 253.712485200027, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.28772417175752, 'throughput_since_last_restore': 15.972297976310692}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 621
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1103
(MultiAgentEnvRunner pid=37492) {'red_0': -47.300000000000395, 'red_1': -50.40000000000041, 'blue_0': -41.800000000000345, 'blue_1': -55.000000000000526}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 427
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 457
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 489
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 489
(MultiAgentEnvRunner pid=37492) {'red_0': -17.800000000000022, 'red_1': -46.100000000000335, 'blue_0': -28.400000000000098, 'blue_1': -43.60000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 392
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 555
(MultiAgentEnvRunner pid=37492) {'red_0': -15.099999999999962, 'red_1': -15.69999999999997, 'blue_0': -39.30000000000028, 'blue_1': -40.1000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.899999999999995, 'red_1': -3.3000000000000016, 'blue_0': -52.800000000000466, 'blue_1': -37.600000000000264}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 899
(MultiAgentEnvRunner pid=37492) {'red_0': -31.200000000000102, 'red_1': -68.90000000000026, 'blue_0': -34.500000000000426, 'blue_1': -36.000000000000306}
ITERATION 172: reward=-142.160000000001, metadata={'num_env_steps_sampled_lifetime': 1038000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003502509972114093, 'timers': {'connectors': {'batch_individual_items': 0.00010072122867013582, 'add_states_from_episodes_to_batch': 7.114139363656356e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3352972871115577e-05, 'numpy_to_tensor': 7.270122331131186e-05, 'agent_to_module_mapping': 8.891814299033268e-06, 'add_observations_from_episodes_to_batch': 4.185789993660664e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009288037011501251, 'timers': {'connectors': {'get_actions': 0.00047764572972852913, 'un_batch_to_individual_items': 6.95462953030948e-05, 'tensor_to_numpy': 0.00012700471235871686, 'module_to_agent_unmapping': 6.901203135691933e-06, 'normalize_and_clip_actions': 7.773464371312791e-05, 'listify_data_for_vector_env': 2.5541793498373577e-05, 'remove_single_ts_time_rank_from_batch': 2.5501082670673886e-06}}}, 'sample': 89.03821249993052, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -36.880000000000194, 'blue_0': -39.360000000000326, 'blue_1': -42.46000000000036, 'red_0': -23.460000000000097}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 172.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1038000.0, 'blue_0': 1038000.0, 'blue_1': 1038000.0, 'red_0': 1038000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -36.880000000000194, 'blue_policy': -42.46000000000036}, 'num_module_steps_sampled_lifetime': {'red_policy': 2076000.0, 'blue_policy': 2076000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003620893950373334, 'episode_return_mean': -142.160000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -99.60000000000073, 'episode_duration_sec_mean': 17.6855206000153, 'episode_return_min': -194.5000000000017, 'rlmodule_inference_timer': 0.014072500070430576, 'num_episodes_lifetime': 865.0, 'episode_len_min': 1200, 'time_between_sampling': 258.0920195999788, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.274993091185532, 'throughput_since_last_restore': 15.979262668974226}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 516
(MultiAgentEnvRunner pid=37492) {'red_0': -69.80000000000021, 'red_1': -63.1000000000006, 'blue_0': -36.800000000000296, 'blue_1': -18.999999999999968}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 936
(MultiAgentEnvRunner pid=37492) {'red_0': -33.40000000000019, 'red_1': -24.00000000000007, 'blue_0': -40.300000000000324, 'blue_1': -56.30000000000051}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 306
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 327
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 349
(MultiAgentEnvRunner pid=37492) {'red_0': -43.50000000000025, 'red_1': -17.4, 'blue_0': -49.70000000000045, 'blue_1': -41.70000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 533
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1102
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1123
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1192
(MultiAgentEnvRunner pid=37492) {'red_0': -36.40000000000024, 'red_1': -45.30000000000032, 'blue_0': -27.50000000000016, 'blue_1': -33.40000000000018}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1179
(MultiAgentEnvRunner pid=37492) {'red_0': -35.000000000000156, 'red_1': -38.4000000000002, 'blue_0': -49.00000000000047, 'blue_1': -36.50000000000045}
ITERATION 173: reward=-159.30000000000103, metadata={'num_env_steps_sampled_lifetime': 1044000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003543767013784469, 'timers': {'connectors': {'batch_individual_items': 9.880072522207313e-05, 'add_states_from_episodes_to_batch': 7.325787392191504e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3698266340186676e-05, 'numpy_to_tensor': 7.380204842501874e-05, 'agent_to_module_mapping': 8.755432710209772e-06, 'add_observations_from_episodes_to_batch': 4.405658220820079e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009346098784087922, 'timers': {'connectors': {'get_actions': 0.00047773114750242137, 'un_batch_to_individual_items': 7.208816588006085e-05, 'tensor_to_numpy': 0.0001257273577301, 'module_to_agent_unmapping': 7.016170917543415e-06, 'normalize_and_clip_actions': 7.828919558277718e-05, 'listify_data_for_vector_env': 2.6197777081405357e-05, 'remove_single_ts_time_rank_from_batch': 2.534022866762707e-06}}}, 'sample': 88.7897572000511, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -37.64000000000024, 'blue_0': -40.660000000000345, 'blue_1': -37.38000000000028, 'red_0': -43.62000000000021}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 173.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1044000.0, 'blue_0': 1044000.0, 'blue_1': 1044000.0, 'red_0': 1044000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -37.64000000000024, 'blue_policy': -37.38000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 2088000.0, 'blue_policy': 2088000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036772799459777126, 'episode_return_mean': -159.30000000000103, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -142.6000000000009, 'episode_duration_sec_mean': 17.621213159989566, 'episode_return_min': -188.70000000000107, 'rlmodule_inference_timer': 0.01389846778611126, 'num_episodes_lifetime': 870.0, 'episode_len_min': 1200, 'time_between_sampling': 258.287611000007, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.28693169672641, 'throughput_since_last_restore': 15.986211991626984}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 329
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 353
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 841
(MultiAgentEnvRunner pid=37492) {'red_0': -34.30000000000017, 'red_1': -46.40000000000025, 'blue_0': -20.499999999999968, 'blue_1': -34.800000000000274}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 652
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1124
(MultiAgentEnvRunner pid=37492) {'red_0': -20.30000000000003, 'red_1': -26.4000000000001, 'blue_0': -35.90000000000044, 'blue_1': -28.700000000000212}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 928
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 935
(MultiAgentEnvRunner pid=37492) {'red_0': -20.80000000000004, 'red_1': -8.999999999999996, 'blue_0': -30.00000000000039, 'blue_1': -40.900000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 380
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 556
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 746
(MultiAgentEnvRunner pid=37492) {'red_0': -26.10000000000004, 'red_1': -49.90000000000028, 'blue_0': -22.000000000000025, 'blue_1': -22.50000000000001}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 428
(MultiAgentEnvRunner pid=37492) {'red_0': -44.70000000000029, 'red_1': -18.400000000000002, 'blue_0': -29.600000000000133, 'blue_1': -36.800000000000395}
ITERATION 174: reward=-119.60000000000068, metadata={'num_env_steps_sampled_lifetime': 1050000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032308537505377707, 'timers': {'connectors': {'batch_individual_items': 9.514179049655438e-05, 'add_states_from_episodes_to_batch': 6.444645805971919e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2055835630777614e-05, 'numpy_to_tensor': 6.693860745628134e-05, 'agent_to_module_mapping': 7.91948951070784e-06, 'add_observations_from_episodes_to_batch': 3.901116292152926e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008585671833920602, 'timers': {'connectors': {'get_actions': 0.0004384688655738104, 'un_batch_to_individual_items': 6.49596022318076e-05, 'tensor_to_numpy': 0.0001200474922985059, 'module_to_agent_unmapping': 6.750492462656792e-06, 'normalize_and_clip_actions': 7.081435612263446e-05, 'listify_data_for_vector_env': 2.3802063827774226e-05, 'remove_single_ts_time_rank_from_batch': 2.3076127677142503e-06}}}, 'sample': 88.71851729997434, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.02000000000013, 'blue_0': -27.600000000000193, 'blue_1': -32.74000000000025, 'red_0': -29.240000000000112}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 174.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1050000.0, 'blue_0': 1050000.0, 'blue_1': 1050000.0, 'red_0': 1050000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.02000000000013, 'blue_policy': -32.74000000000025}, 'num_module_steps_sampled_lifetime': {'red_policy': 2100000.0, 'blue_policy': 2100000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034515648513718265, 'episode_return_mean': -119.60000000000068, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -100.7000000000008, 'episode_duration_sec_mean': 17.629303739988245, 'episode_return_min': -136.00000000000065, 'rlmodule_inference_timer': 0.012424367241265104, 'num_episodes_lifetime': 875.0, 'episode_len_min': 1200, 'time_between_sampling': 258.2950259000063, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.287668149615243, 'throughput_since_last_restore': 15.993091317864973}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -10.399999999999979, 'red_1': -17.699999999999985, 'blue_0': -37.90000000000027, 'blue_1': -58.00000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -38.50000000000026, 'red_1': -33.5000000000002, 'blue_0': -41.40000000000032, 'blue_1': -45.200000000000365}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -2.600000000000001, 'red_1': -6.299999999999994, 'blue_0': -45.30000000000036, 'blue_1': -54.30000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 855
(MultiAgentEnvRunner pid=37492) {'red_0': -42.30000000000031, 'red_1': -48.4000000000004, 'blue_0': -42.60000000000036, 'blue_1': -48.600000000000406}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 777
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 872
(MultiAgentEnvRunner pid=37492) {'red_0': -32.000000000000185, 'red_1': -19.30000000000005, 'blue_0': -35.50000000000035, 'blue_1': -41.700000000000365}
ITERATION 175: reward=-140.30000000000103, metadata={'num_env_steps_sampled_lifetime': 1056000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034590641495178023, 'timers': {'connectors': {'batch_individual_items': 9.934508603651905e-05, 'add_states_from_episodes_to_batch': 6.989222350609817e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2941970197806007e-05, 'numpy_to_tensor': 7.112136678903733e-05, 'agent_to_module_mapping': 9.308388102298411e-06, 'add_observations_from_episodes_to_batch': 4.132493926145669e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009171641933798438, 'timers': {'connectors': {'get_actions': 0.0004692301312216716, 'un_batch_to_individual_items': 6.988212800818238e-05, 'tensor_to_numpy': 0.00012480216606622932, 'module_to_agent_unmapping': 6.831021817083507e-06, 'normalize_and_clip_actions': 7.621499627170855e-05, 'listify_data_for_vector_env': 2.527758428363774e-05, 'remove_single_ts_time_rank_from_batch': 2.564032413308226e-06}}}, 'sample': 88.51815959997475, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.040000000000127, 'blue_0': -40.540000000000326, 'blue_1': -49.56000000000043, 'red_0': -25.160000000000146}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 175.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1056000.0, 'blue_0': 1056000.0, 'blue_1': 1056000.0, 'red_0': 1056000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.040000000000127, 'blue_policy': -49.56000000000043}, 'num_module_steps_sampled_lifetime': {'red_policy': 2112000.0, 'blue_policy': 2112000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003541435063466436, 'episode_return_mean': -140.30000000000103, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.50000000000084, 'episode_duration_sec_mean': 17.58356774004642, 'episode_return_min': -181.90000000000146, 'rlmodule_inference_timer': 0.013507904417440675, 'num_episodes_lifetime': 880.0, 'episode_len_min': 1200, 'time_between_sampling': 258.35384459991474, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.295890554896946, 'throughput_since_last_restore': 15.999938286783495}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 306
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 634
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 641
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1127
(MultiAgentEnvRunner pid=37492) {'red_0': -17.800000000000004, 'red_1': -30.50000000000015, 'blue_0': -46.20000000000044, 'blue_1': -33.70000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1144
(MultiAgentEnvRunner pid=37492) {'red_0': -8.399999999999979, 'red_1': -39.10000000000027, 'blue_0': -48.900000000000404, 'blue_1': -66.30000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 249
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1036
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1179
(MultiAgentEnvRunner pid=37492) {'red_0': -35.000000000000256, 'red_1': -53.50000000000051, 'blue_0': -37.50000000000022, 'blue_1': -32.900000000000176}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 737
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1157
(MultiAgentEnvRunner pid=37492) {'red_0': -37.70000000000021, 'red_1': -40.60000000000023, 'blue_0': -28.900000000000404, 'blue_1': -17.100000000000218}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 741
(MultiAgentEnvRunner pid=37492) {'red_0': -7.900000000000001, 'red_1': -7.200000000000001, 'blue_0': -28.000000000000195, 'blue_1': -24.500000000000135}
ITERATION 176: reward=-128.3400000000009, metadata={'num_env_steps_sampled_lifetime': 1062000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031444278556584914, 'timers': {'connectors': {'batch_individual_items': 9.378467579931849e-05, 'add_states_from_episodes_to_batch': 6.176210536468683e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1756834042038767e-05, 'numpy_to_tensor': 6.534705055905938e-05, 'agent_to_module_mapping': 7.777301385603037e-06, 'add_observations_from_episodes_to_batch': 3.759082433500466e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008461096867613992, 'timers': {'connectors': {'get_actions': 0.0004326978813036526, 'un_batch_to_individual_items': 6.59880027660291e-05, 'tensor_to_numpy': 0.00011428558770870076, 'module_to_agent_unmapping': 6.249931289271397e-06, 'normalize_and_clip_actions': 7.09881067525212e-05, 'listify_data_for_vector_env': 2.3192311489381076e-05, 'remove_single_ts_time_rank_from_batch': 2.2704801465579896e-06}}}, 'sample': 89.15138299996033, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -34.18000000000023, 'blue_0': -37.90000000000033, 'blue_1': -34.90000000000025, 'red_0': -21.360000000000092}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 176.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1062000.0, 'blue_0': 1062000.0, 'blue_1': 1062000.0, 'red_0': 1062000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -34.18000000000023, 'blue_policy': -34.90000000000025}, 'num_module_steps_sampled_lifetime': {'red_policy': 2124000.0, 'blue_policy': 2124000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033872706050650073, 'episode_return_mean': -128.3400000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -67.60000000000034, 'episode_duration_sec_mean': 17.691001680004412, 'episode_return_min': -162.70000000000113, 'rlmodule_inference_timer': 0.012435553421087474, 'num_episodes_lifetime': 885.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3831273999531, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.26510762589391, 'throughput_since_last_restore': 16.006564433341854}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 763
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 808
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 903
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 903
(MultiAgentEnvRunner pid=37492) {'red_0': -65.4000000000005, 'red_1': -24.50000000000004, 'blue_0': -41.000000000000504, 'blue_1': -50.000000000000476}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 296
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 329
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 451
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1012
(MultiAgentEnvRunner pid=37492) {'red_0': -24.900000000000095, 'red_1': -37.400000000000226, 'blue_0': -37.80000000000022, 'blue_1': -41.00000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 307
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1126
(MultiAgentEnvRunner pid=37492) {'red_0': -41.500000000000234, 'red_1': -18.500000000000014, 'blue_0': -34.30000000000029, 'blue_1': -26.700000000000323}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1148
(MultiAgentEnvRunner pid=37492) {'red_0': -9.299999999999983, 'red_1': -8.599999999999987, 'blue_0': -46.80000000000038, 'blue_1': -45.500000000000405}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 207
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 304
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 471
(MultiAgentEnvRunner pid=37492) {'red_0': -51.300000000000374, 'red_1': -37.100000000000186, 'blue_0': -26.200000000000063, 'blue_1': -41.20000000000036}
ITERATION 177: reward=-141.800000000001, metadata={'num_env_steps_sampled_lifetime': 1068000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031881802890278745, 'timers': {'connectors': {'batch_individual_items': 9.463041880748767e-05, 'add_states_from_episodes_to_batch': 6.492385432398961e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1937630737570806e-05, 'numpy_to_tensor': 6.674297872373238e-05, 'agent_to_module_mapping': 7.852685859730795e-06, 'add_observations_from_episodes_to_batch': 3.795106647238133e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008549429762065514, 'timers': {'connectors': {'get_actions': 0.00044005097129543903, 'un_batch_to_individual_items': 6.412285716074664e-05, 'tensor_to_numpy': 0.00011654328556696212, 'module_to_agent_unmapping': 6.3523700917912655e-06, 'normalize_and_clip_actions': 7.055501663176864e-05, 'listify_data_for_vector_env': 2.350643300968252e-05, 'remove_single_ts_time_rank_from_batch': 2.2949663813285064e-06}}}, 'sample': 88.87420540000312, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.22000000000009, 'blue_0': -37.2200000000003, 'blue_1': -40.88000000000038, 'red_0': -38.48000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 177.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1068000.0, 'blue_0': 1068000.0, 'blue_1': 1068000.0, 'red_0': 1068000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.22000000000009, 'blue_policy': -40.88000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 2136000.0, 'blue_policy': 2136000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034298123869537706, 'episode_return_mean': -141.800000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -110.20000000000076, 'episode_duration_sec_mean': 17.6564625200117, 'episode_return_min': -180.90000000000154, 'rlmodule_inference_timer': 0.01251982677107053, 'num_episodes_lifetime': 890.0, 'episode_len_min': 1200, 'time_between_sampling': 258.3773194999667, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.21844736652463, 'throughput_since_last_restore': 16.012895547143337}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1045
(MultiAgentEnvRunner pid=37492) {'red_0': -43.50000000000026, 'red_1': -32.500000000000114, 'blue_0': -40.900000000000375, 'blue_1': -19.200000000000173}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 566
(MultiAgentEnvRunner pid=37492) {'red_0': -28.70000000000007, 'red_1': -16.99999999999999, 'blue_0': -35.6000000000002, 'blue_1': -42.6000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 490
(MultiAgentEnvRunner pid=37492) {'red_0': -37.10000000000025, 'red_1': -26.80000000000013, 'blue_0': -37.30000000000024, 'blue_1': -31.600000000000215}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 314
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1119
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1175
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1191
(MultiAgentEnvRunner pid=37492) {'red_0': -25.800000000000146, 'red_1': -25.300000000000082, 'blue_0': -36.900000000000226, 'blue_1': -41.10000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 348
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 402
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 935
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1004
(MultiAgentEnvRunner pid=37492) {'red_0': -27.700000000000184, 'red_1': -8.299999999999951, 'blue_0': -56.700000000000465, 'blue_1': -54.10000000000041}
ITERATION 178: reward=-133.74000000000086, metadata={'num_env_steps_sampled_lifetime': 1074000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034156398305770254, 'timers': {'connectors': {'batch_individual_items': 0.00010438192412491171, 'add_states_from_episodes_to_batch': 6.560324219840709e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2949493102133997e-05, 'numpy_to_tensor': 7.070847813608631e-05, 'agent_to_module_mapping': 8.385856305164736e-06, 'add_observations_from_episodes_to_batch': 3.939793229782074e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008970941914352607, 'timers': {'connectors': {'get_actions': 0.0004593427790261012, 'un_batch_to_individual_items': 6.764093016499865e-05, 'tensor_to_numpy': 0.00012304740409436145, 'module_to_agent_unmapping': 6.666070189072906e-06, 'normalize_and_clip_actions': 7.395710101884242e-05, 'listify_data_for_vector_env': 2.4854576351323288e-05, 'remove_single_ts_time_rank_from_batch': 2.4502853197122686e-06}}}, 'sample': 95.22239050001372, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.980000000000054, 'blue_0': -41.4800000000003, 'blue_1': -37.72000000000031, 'red_0': -32.56000000000019}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 178.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1074000.0, 'blue_0': 1074000.0, 'blue_1': 1074000.0, 'red_0': 1074000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.980000000000054, 'blue_policy': -37.72000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 2148000.0, 'blue_policy': 2148000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003546735901912427, 'episode_return_mean': -133.74000000000086, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -123.90000000000066, 'episode_duration_sec_mean': 18.89518857996445, 'episode_return_min': -146.800000000001, 'rlmodule_inference_timer': 0.013280536464951012, 'num_episodes_lifetime': 895.0, 'episode_len_min': 1200, 'time_between_sampling': 259.5967398000648, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.73984855087555, 'throughput_since_last_restore': 16.01678051685257}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 721
(MultiAgentEnvRunner pid=37492) {'red_0': -17.199999999999992, 'red_1': -28.600000000000065, 'blue_0': -27.400000000000194, 'blue_1': -46.70000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 310
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 458
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 471
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 955
(MultiAgentEnvRunner pid=37492) {'red_0': -62.90000000000051, 'red_1': -35.20000000000027, 'blue_0': -40.30000000000025, 'blue_1': -23.000000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.699999999999985, 'red_1': -8.599999999999985, 'blue_0': -41.40000000000031, 'blue_1': -44.200000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 425
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1176
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1176
(MultiAgentEnvRunner pid=37492) {'red_0': -24.900000000000116, 'red_1': -34.70000000000019, 'blue_0': -34.400000000000155, 'blue_1': -43.10000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.299999999999985, 'red_1': -5.599999999999996, 'blue_0': -39.20000000000029, 'blue_1': -46.30000000000038}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.7999999999999999, 'blue_1': -79.39999999999976}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.7999999999999999, 'red_1': 0, 'blue_0': -0.5, 'blue_1': -0.7999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 09:59:56,450	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.4, 'blue_1': -0.4}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -7.19999999999999, 'red_1': 0, 'blue_0': -5.899999999999995, 'blue_1': -7.899999999999989}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': 0, 'blue_0': -1.2, 'blue_1': -0.4}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 179: reward=-124.34000000000079, metadata={'num_env_steps_sampled_lifetime': 1080000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031819619075516777, 'timers': {'connectors': {'batch_individual_items': 9.556204514269109e-05, 'add_states_from_episodes_to_batch': 6.095040420194998e-06, 'add_time_dim_to_batch_and_zero_pad': 1.277029528790655e-05, 'numpy_to_tensor': 6.565672522182115e-05, 'agent_to_module_mapping': 8.27994025897246e-06, 'add_observations_from_episodes_to_batch': 3.760463273822339e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008449702133448852, 'timers': {'connectors': {'get_actions': 0.0004373278040078367, 'un_batch_to_individual_items': 6.307752950605933e-05, 'tensor_to_numpy': 0.00011495630192425913, 'module_to_agent_unmapping': 6.124709969280385e-06, 'normalize_and_clip_actions': 6.982009448747715e-05, 'listify_data_for_vector_env': 2.3078079634954218e-05, 'remove_single_ts_time_rank_from_batch': 2.290335639172864e-06}}}, 'sample': 89.2932994000148, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.5400000000001, 'blue_0': -36.540000000000234, 'blue_1': -40.66000000000031, 'red_0': -24.60000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 179.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1080000.0, 'blue_0': 1080000.0, 'blue_1': 1080000.0, 'red_0': 1080000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.5400000000001, 'blue_policy': -40.66000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 2160000.0, 'blue_policy': 2160000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003311238120118613, 'episode_return_mean': -124.34000000000079, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -100.40000000000065, 'episode_duration_sec_mean': 17.740502760000528, 'episode_return_min': -161.40000000000109, 'rlmodule_inference_timer': 0.012498056608180975, 'num_episodes_lifetime': 900.0, 'episode_len_min': 1200, 'time_between_sampling': 263.20956770004705, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.770561962712396, 'throughput_since_last_restore': 16.00227839126362}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 195
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 675
(MultiAgentEnvRunner pid=37492) {'red_0': -14.499999999999988, 'red_1': -36.100000000000186, 'blue_0': -31.800000000000157, 'blue_1': -27.900000000000087}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 339
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 782
(MultiAgentEnvRunner pid=37492) {'red_0': -26.20000000000013, 'red_1': -36.10000000000017, 'blue_0': -36.7000000000002, 'blue_1': -52.30000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 317
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 461
(MultiAgentEnvRunner pid=37492) {'red_0': -17.399999999999974, 'red_1': -23.600000000000048, 'blue_0': -52.40000000000042, 'blue_1': -42.300000000000324}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 339
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 996
(MultiAgentEnvRunner pid=37492) {'red_0': -49.20000000000045, 'red_1': -23.100000000000104, 'blue_0': -33.300000000000146, 'blue_1': -38.50000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 525
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 724
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 803
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 824
(MultiAgentEnvRunner pid=37492) {'red_0': -38.6000000000002, 'red_1': -60.700000000000486, 'blue_0': -26.900000000000382, 'blue_1': -43.30000000000044}
ITERATION 180: reward=-142.18000000000092, metadata={'num_env_steps_sampled_lifetime': 1086000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032703460768574545, 'timers': {'connectors': {'batch_individual_items': 9.280412780658674e-05, 'add_states_from_episodes_to_batch': 6.655776535118996e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2787561177451301e-05, 'numpy_to_tensor': 6.812635695235986e-05, 'agent_to_module_mapping': 8.268121875709207e-06, 'add_observations_from_episodes_to_batch': 4.015502583407228e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008804985111679572, 'timers': {'connectors': {'get_actions': 0.000454161335334196, 'un_batch_to_individual_items': 6.56945109082213e-05, 'tensor_to_numpy': 0.00011992474661309486, 'module_to_agent_unmapping': 6.35783191134637e-06, 'normalize_and_clip_actions': 7.258100594299864e-05, 'listify_data_for_vector_env': 2.443776146639131e-05, 'remove_single_ts_time_rank_from_batch': 2.424344802497688e-06}}}, 'sample': 87.21520870004315, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.9200000000002, 'blue_0': -36.22000000000026, 'blue_1': -40.860000000000305, 'red_0': -29.18000000000015}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 180.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1086000.0, 'blue_0': 1086000.0, 'blue_1': 1086000.0, 'red_0': 1086000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.9200000000002, 'blue_policy': -40.860000000000305}, 'num_module_steps_sampled_lifetime': {'red_policy': 2172000.0, 'blue_policy': 2172000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003503522717739619, 'episode_return_mean': -142.18000000000092, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -110.30000000000042, 'episode_duration_sec_mean': 17.32209624000825, 'episode_return_min': -169.5000000000015, 'rlmodule_inference_timer': 0.013041495434796041, 'num_episodes_lifetime': 905.0, 'episode_len_min': 1200, 'time_between_sampling': 346.4169638999738, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.004593407932177, 'throughput_since_last_restore': 16.00229035723628}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 374.85378540004604, 'restore_env_runners': 1.1199968867003918e-05, 'training_step': 374.85352150001563, 'env_runner_sampling_timer': 87.35092150000855, 'learner_update_timer': 287.45067549997475, 'synch_weights': 0.013450000085867941, 'synch_env_connectors': 0.0025624000700190663, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1086000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032703460768574545, 'timers': {'connectors': {'batch_individual_items': 9.280412780658674e-05, 'add_states_from_episodes_to_batch': 6.655776535118996e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2787561177451301e-05, 'numpy_to_tensor': 6.812635695235986e-05, 'agent_to_module_mapping': 8.268121875709207e-06, 'add_observations_from_episodes_to_batch': 4.015502583407228e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008804985111679572, 'timers': {'connectors': {'get_actions': 0.000454161335334196, 'un_batch_to_individual_items': 6.56945109082213e-05, 'tensor_to_numpy': 0.00011992474661309486, 'module_to_agent_unmapping': 6.35783191134637e-06, 'normalize_and_clip_actions': 7.258100594299864e-05, 'listify_data_for_vector_env': 2.443776146639131e-05, 'remove_single_ts_time_rank_from_batch': 2.424344802497688e-06}}}, 'sample': 87.21520870004315, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.9200000000002, 'blue_0': -36.22000000000026, 'blue_1': -40.860000000000305, 'red_0': -29.18000000000015}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 180.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1086000.0, 'blue_0': 1086000.0, 'blue_1': 1086000.0, 'red_0': 1086000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.9200000000002, 'blue_policy': -40.860000000000305}, 'num_module_steps_sampled_lifetime': {'red_policy': 2172000.0, 'blue_policy': 2172000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003503522717739619, 'episode_return_mean': -142.18000000000092, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -110.30000000000042, 'episode_duration_sec_mean': 17.32209624000825, 'episode_return_min': -169.5000000000015, 'rlmodule_inference_timer': 0.013041495434796041, 'num_episodes_lifetime': 905.0, 'episode_len_min': 1200, 'time_between_sampling': 346.4169638999738, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.004593407932177, 'throughput_since_last_restore': 16.00229035723628}}, 'learners': {'red_policy': {'policy_loss': -0.02850170247256756, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.009542225860059261, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 181.0, 'num_module_steps_trained_lifetime': 65217920.0, 'curr_entropy_coeff': 0.033710000000000004, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001914, 'vf_explained_var': -0.20277011394500732, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.953755378723145, 'entropy': 1.0073182582855225, 'vf_loss_unclipped': 647.2396240234375, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 961.1337834570808, 'throughput_since_last_restore': 960.990871850518}}, 'blue_policy': {'weights_seq_no': 181.0, 'num_module_steps_trained_lifetime': 65217920.0, 'curr_entropy_coeff': 0.033710000000000004, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001914, 'vf_explained_var': 0.3997889757156372, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 3.0620079040527344, 'total_loss': 1.3935540914535522, 'entropy': 1.6945130825042725, 'policy_loss': -0.11871182918548584, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.545040488243103, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.016131741926074028, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 961.1321803334608, 'throughput_since_last_restore': 960.9908729620996}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 8.899951353669167e-06, 'batch_individual_items': 0.820230899960734, 'add_time_dim_to_batch_and_zero_pad': 2.8499984182417393e-05, 'numpy_to_tensor': 0.1282424998935312, 'add_observations_from_episodes_to_batch': 0.0003735999343916774, 'agent_to_module_mapping': 0.02245940000284463, 'add_one_ts_to_episodes_and_truncate': 0.1670370000647381, 'add_columns_from_episodes_to_train_batch': 0.5121690999949351, 'general_advantage_estimation': 13.107494099996984}}, 'connector_pipeline_timer': 14.758526399964467}, 'num_module_steps_trained_lifetime': 130435840.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 3057090000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 45053.03433541614, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 45053.03258084441, 'throughput_since_last_restore': 45046.447229438796}, 'num_module_steps_trained_throughput': 1922.262665507978, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1922.2626383325291, 'throughput_since_last_restore': 1921.9817476092712}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1086000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 181, 'trial_id': 'default', 'date': '2026-01-26_10-09-57', 'timestamp': 1769418597, 'time_this_iter_s': 374.8686411380768, 'time_total_s': 67841.08282518387, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 67841.08282518387, 'iterations_since_restore': 181, 'perf': {'cpu_util_percent': 15.09682242990654, 'ram_util_percent': 87.74710280373831}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 790
(MultiAgentEnvRunner pid=37492) {'red_0': -25.60000000000004, 'red_1': -51.20000000000039, 'blue_0': -38.600000000000335, 'blue_1': -24.200000000000127}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 419
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 598
(MultiAgentEnvRunner pid=37492) {'red_0': -24.900000000000116, 'red_1': -19.80000000000001, 'blue_0': -44.00000000000035, 'blue_1': -48.90000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 873
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 922
(MultiAgentEnvRunner pid=37492) {'red_0': -21.80000000000006, 'red_1': -28.70000000000017, 'blue_0': -32.50000000000029, 'blue_1': -48.60000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 634
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 721
(MultiAgentEnvRunner pid=37492) {'red_0': -11.699999999999985, 'red_1': -21.60000000000006, 'blue_0': -41.40000000000037, 'blue_1': -36.600000000000264}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -50.000000000000426, 'red_1': -20.800000000000026, 'blue_0': -36.50000000000025, 'blue_1': -40.5000000000003}
ITERATION 181: reward=-133.58000000000087, metadata={'num_env_steps_sampled_lifetime': 1092000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003280264292946068, 'timers': {'connectors': {'batch_individual_items': 9.800843710386045e-05, 'add_states_from_episodes_to_batch': 6.6991409096405416e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2237843793512978e-05, 'numpy_to_tensor': 6.783742605228787e-05, 'agent_to_module_mapping': 8.120778414943404e-06, 'add_observations_from_episodes_to_batch': 3.856921712842526e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008657163216226776, 'timers': {'connectors': {'get_actions': 0.0004443655910194937, 'un_batch_to_individual_items': 6.487307866966192e-05, 'tensor_to_numpy': 0.00011586006694423205, 'module_to_agent_unmapping': 6.568940174769922e-06, 'normalize_and_clip_actions': 7.384751569341275e-05, 'listify_data_for_vector_env': 2.410853785532103e-05, 'remove_single_ts_time_rank_from_batch': 2.327421535553584e-06}}}, 'sample': 93.11589659994934, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.42000000000013, 'blue_0': -38.60000000000032, 'blue_1': -39.76000000000031, 'red_0': -26.800000000000125}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 181.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1092000.0, 'blue_0': 1092000.0, 'blue_1': 1092000.0, 'red_0': 1092000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.42000000000013, 'blue_policy': -39.76000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 2184000.0, 'blue_policy': 2184000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003442380876303157, 'episode_return_mean': -133.58000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.30000000000068, 'episode_duration_sec_mean': 18.44217040000949, 'episode_return_min': -147.80000000000098, 'rlmodule_inference_timer': 0.012842903850642441, 'num_episodes_lifetime': 910.0, 'episode_len_min': 1200, 'time_between_sampling': 287.7944801999256, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.190262015388697, 'throughput_since_last_restore': 15.997590966632002}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 985
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1192
(MultiAgentEnvRunner pid=37492) {'red_0': -38.70000000000014, 'red_1': -25.50000000000004, 'blue_0': -26.300000000000388, 'blue_1': -29.60000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 506
(MultiAgentEnvRunner pid=37492) {'red_0': -25.100000000000104, 'red_1': -56.00000000000044, 'blue_0': -38.30000000000026, 'blue_1': -16.299999999999933}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -21.50000000000004, 'red_1': -36.80000000000024, 'blue_0': -42.70000000000034, 'blue_1': -60.60000000000056}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 762
(MultiAgentEnvRunner pid=37492) {'red_0': -36.50000000000031, 'red_1': -34.200000000000415, 'blue_0': -42.700000000000266, 'blue_1': -52.00000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 852
(MultiAgentEnvRunner pid=37492) {'red_0': -11.799999999999992, 'red_1': -10.399999999999993, 'blue_0': -25.300000000000303, 'blue_1': -46.70000000000045}
ITERATION 182: reward=-135.400000000001, metadata={'num_env_steps_sampled_lifetime': 1098000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003426232285469666, 'timers': {'connectors': {'batch_individual_items': 9.940202875975376e-05, 'add_states_from_episodes_to_batch': 6.644912830433587e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2910666402177523e-05, 'numpy_to_tensor': 7.200514057054981e-05, 'agent_to_module_mapping': 8.702503761339627e-06, 'add_observations_from_episodes_to_batch': 4.0772675484591644e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009075342597426522, 'timers': {'connectors': {'get_actions': 0.00046997986550507384, 'un_batch_to_individual_items': 6.764426109863645e-05, 'tensor_to_numpy': 0.00012069591827811419, 'module_to_agent_unmapping': 6.708203689508684e-06, 'normalize_and_clip_actions': 7.531322105809776e-05, 'listify_data_for_vector_env': 2.5768954867441418e-05, 'remove_single_ts_time_rank_from_batch': 2.369914437652778e-06}}}, 'sample': 92.30952219991013, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.580000000000226, 'blue_0': -35.06000000000031, 'blue_1': -41.04000000000035, 'red_0': -26.72000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 182.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1098000.0, 'blue_0': 1098000.0, 'blue_1': 1098000.0, 'red_0': 1098000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.580000000000226, 'blue_policy': -41.04000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 2196000.0, 'blue_policy': 2196000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035193138693025725, 'episode_return_mean': -135.400000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -94.20000000000073, 'episode_duration_sec_mean': 18.335874780011363, 'episode_return_min': -165.4000000000014, 'rlmodule_inference_timer': 0.013752942746835006, 'num_episodes_lifetime': 915.0, 'episode_len_min': 1200, 'time_between_sampling': 301.7570232000435, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.253772490177317, 'throughput_since_last_restore': 15.9933287807572}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 533
(MultiAgentEnvRunner pid=37492) {'red_0': -18.100000000000005, 'red_1': -37.10000000000026, 'blue_0': -41.60000000000033, 'blue_1': -20.8}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -2.1000000000000005, 'red_1': -2.9000000000000012, 'blue_0': -40.0000000000003, 'blue_1': -45.00000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 507
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 592
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1096
(MultiAgentEnvRunner pid=37492) {'red_0': -21.700000000000003, 'red_1': -35.30000000000012, 'blue_0': -35.40000000000047, 'blue_1': -20.100000000000055}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 166
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1130
(MultiAgentEnvRunner pid=37492) {'red_0': -49.60000000000035, 'red_1': -47.30000000000032, 'blue_0': -35.8000000000003, 'blue_1': -20.400000000000226}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 463
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 541
(MultiAgentEnvRunner pid=37492) {'red_0': -25.200000000000067, 'red_1': -13.299999999999969, 'blue_0': -48.100000000000406, 'blue_1': -46.60000000000038}
ITERATION 183: reward=-121.2800000000008, metadata={'num_env_steps_sampled_lifetime': 1104000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003384343559696093, 'timers': {'connectors': {'batch_individual_items': 9.84855979259135e-05, 'add_states_from_episodes_to_batch': 6.579040114376765e-06, 'add_time_dim_to_batch_and_zero_pad': 1.288463870734479e-05, 'numpy_to_tensor': 6.997626191976849e-05, 'agent_to_module_mapping': 8.333771003311032e-06, 'add_observations_from_episodes_to_batch': 4.153676871494656e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009252100428739854, 'timers': {'connectors': {'get_actions': 0.00047727057639525977, 'un_batch_to_individual_items': 6.988760349107579e-05, 'tensor_to_numpy': 0.0001264604153074637, 'module_to_agent_unmapping': 6.737055651818667e-06, 'normalize_and_clip_actions': 7.48257665974055e-05, 'listify_data_for_vector_env': 2.4773107987399298e-05, 'remove_single_ts_time_rank_from_batch': 2.3184960737865865e-06}}}, 'sample': 93.1875687999418, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.18000000000013, 'blue_0': -40.18000000000036, 'blue_1': -30.5800000000002, 'red_0': -23.34000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 183.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1104000.0, 'blue_0': 1104000.0, 'blue_1': 1104000.0, 'red_0': 1104000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.18000000000013, 'blue_policy': -30.5800000000002}, 'num_module_steps_sampled_lifetime': {'red_policy': 2208000.0, 'blue_policy': 2208000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035386959840705615, 'episode_return_mean': -121.2800000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -90.00000000000067, 'episode_duration_sec_mean': 18.487962360004893, 'episode_return_min': -153.1000000000012, 'rlmodule_inference_timer': 0.013534332961071732, 'num_episodes_lifetime': 920.0, 'episode_len_min': 1200, 'time_between_sampling': 301.04715640004724, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.999017481861069, 'throughput_since_last_restore': 15.987568226313153}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 613
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 710
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 777
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 779
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 779
(MultiAgentEnvRunner pid=37492) {'red_0': -22.50000000000004, 'red_1': -20.599999999999994, 'blue_0': -39.70000000000038, 'blue_1': -40.30000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 472
(MultiAgentEnvRunner pid=37492) {'red_0': -17.900000000000006, 'red_1': -12.599999999999989, 'blue_0': -61.70000000000062, 'blue_1': -31.600000000000147}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 557
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1172
(MultiAgentEnvRunner pid=37492) {'red_0': -49.70000000000042, 'red_1': -24.20000000000009, 'blue_0': -26.500000000000117, 'blue_1': -37.90000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 256
(MultiAgentEnvRunner pid=37492) {'red_0': -23.50000000000008, 'red_1': -30.900000000000098, 'blue_0': -31.20000000000015, 'blue_1': -35.50000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 407
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1034
(MultiAgentEnvRunner pid=37492) {'red_0': -20.400000000000027, 'red_1': -20.100000000000033, 'blue_0': -36.20000000000021, 'blue_1': -36.60000000000021}
ITERATION 184: reward=-123.92000000000073, metadata={'num_env_steps_sampled_lifetime': 1110000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033814099214948156, 'timers': {'connectors': {'batch_individual_items': 9.943489309236606e-05, 'add_states_from_episodes_to_batch': 6.572828171572678e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3482553271340357e-05, 'numpy_to_tensor': 7.004426767453466e-05, 'agent_to_module_mapping': 8.40175359975587e-06, 'add_observations_from_episodes_to_batch': 4.116616455493237e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008986678059395697, 'timers': {'connectors': {'get_actions': 0.00046190919061951365, 'un_batch_to_individual_items': 6.775074747759243e-05, 'tensor_to_numpy': 0.00012257636964329175, 'module_to_agent_unmapping': 6.6381750778281665e-06, 'normalize_and_clip_actions': 7.450345561916067e-05, 'listify_data_for_vector_env': 2.4702045129883637e-05, 'remove_single_ts_time_rank_from_batch': 2.414962630023886e-06}}}, 'sample': 95.84464429994114, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.680000000000042, 'blue_0': -39.0600000000003, 'blue_1': -36.38000000000028, 'red_0': -26.800000000000118}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 184.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1110000.0, 'blue_0': 1110000.0, 'blue_1': 1110000.0, 'red_0': 1110000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.680000000000042, 'blue_policy': -36.38000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 2220000.0, 'blue_policy': 2220000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003608571286276371, 'episode_return_mean': -123.92000000000073, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -113.30000000000048, 'episode_duration_sec_mean': 19.031563219986857, 'episode_return_min': -138.30000000000098, 'rlmodule_inference_timer': 0.01356546131853614, 'num_episodes_lifetime': 925.0, 'episode_len_min': 1200, 'time_between_sampling': 306.83749539998826, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.010234424388727, 'throughput_since_last_restore': 15.981942822945832}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 647
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1014
(MultiAgentEnvRunner pid=37492) {'red_0': -39.30000000000024, 'red_1': -24.500000000000007, 'blue_0': -38.400000000000475, 'blue_1': -49.30000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.799999999999992, 'red_1': -8.399999999999986, 'blue_0': -42.90000000000034, 'blue_1': -57.20000000000052}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 433
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 554
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 586
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 673
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 673
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 936
(MultiAgentEnvRunner pid=37492) {'red_0': -55.30000000000038, 'red_1': -32.200000000000145, 'blue_0': -23.500000000000078, 'blue_1': -19.200000000000017}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 476
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 493
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 759
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 925
(MultiAgentEnvRunner pid=37492) {'red_0': -36.000000000000156, 'red_1': -47.30000000000035, 'blue_0': -57.60000000000058, 'blue_1': -28.60000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -34.50000000000021, 'red_1': -34.700000000000216, 'blue_0': -42.70000000000033, 'blue_1': -43.000000000000334}
ITERATION 185: reward=-144.28000000000105, metadata={'num_env_steps_sampled_lifetime': 1116000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033770244642054175, 'timers': {'connectors': {'batch_individual_items': 9.804258378588261e-05, 'add_states_from_episodes_to_batch': 6.806391580305155e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2793530413369464e-05, 'numpy_to_tensor': 7.071303118334096e-05, 'agent_to_module_mapping': 8.49139633547615e-06, 'add_observations_from_episodes_to_batch': 4.030657037971204e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008894874077607551, 'timers': {'connectors': {'get_actions': 0.0004583922645678918, 'un_batch_to_individual_items': 6.699561321383211e-05, 'tensor_to_numpy': 0.0001194147370679291, 'module_to_agent_unmapping': 6.950009529548552e-06, 'normalize_and_clip_actions': 7.34460591482428e-05, 'listify_data_for_vector_env': 2.490030935435942e-05, 'remove_single_ts_time_rank_from_batch': 2.394140211961448e-06}}}, 'sample': 92.2394154999638, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.42000000000014, 'blue_0': -41.02000000000036, 'blue_1': -39.460000000000335, 'red_0': -34.3800000000002}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 185.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1116000.0, 'blue_0': 1116000.0, 'blue_1': 1116000.0, 'red_0': 1116000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.42000000000014, 'blue_policy': -39.460000000000335}, 'num_module_steps_sampled_lifetime': {'red_policy': 2232000.0, 'blue_policy': 2232000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034505151786114543, 'episode_return_mean': -144.28000000000105, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -115.30000000000084, 'episode_duration_sec_mean': 18.321055700001306, 'episode_return_min': -169.50000000000142, 'rlmodule_inference_timer': 0.013416409085789742, 'num_episodes_lifetime': 930.0, 'episode_len_min': 1200, 'time_between_sampling': 303.8796164999949, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.782735042402965, 'throughput_since_last_restore': 15.97497494536977}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 564
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 730
(MultiAgentEnvRunner pid=37492) {'red_0': -36.10000000000027, 'red_1': -19.900000000000027, 'blue_0': -54.40000000000045, 'blue_1': -40.70000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1089
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1121
(MultiAgentEnvRunner pid=37492) {'red_0': -39.40000000000021, 'red_1': -43.50000000000026, 'blue_0': -31.200000000000244, 'blue_1': -26.700000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -27.300000000000114, 'red_1': -5.599999999999996, 'blue_0': -47.90000000000041, 'blue_1': -39.500000000000284}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 450
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1031
(MultiAgentEnvRunner pid=37492) {'red_0': -47.60000000000035, 'red_1': -22.900000000000073, 'blue_0': -25.100000000000062, 'blue_1': -50.800000000000445}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 464
(MultiAgentEnvRunner pid=37492) {'red_0': -28.100000000000122, 'red_1': -52.600000000000385, 'blue_0': -41.30000000000038, 'blue_1': -22.400000000000016}
ITERATION 186: reward=-140.60000000000093, metadata={'num_env_steps_sampled_lifetime': 1122000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003516842025191965, 'timers': {'connectors': {'batch_individual_items': 0.00010547525935235177, 'add_states_from_episodes_to_batch': 6.608942668543337e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3689160334175418e-05, 'numpy_to_tensor': 7.419342022479824e-05, 'agent_to_module_mapping': 8.580357442971149e-06, 'add_observations_from_episodes_to_batch': 4.081947506855398e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009219282569986831, 'timers': {'connectors': {'get_actions': 0.0004762118679291539, 'un_batch_to_individual_items': 7.068548416279019e-05, 'tensor_to_numpy': 0.00012319734075640087, 'module_to_agent_unmapping': 6.937265698302909e-06, 'normalize_and_clip_actions': 7.574799081189251e-05, 'listify_data_for_vector_env': 2.4345451529868557e-05, 'remove_single_ts_time_rank_from_batch': 2.5213540558801413e-06}}}, 'sample': 97.79648449993692, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -28.900000000000148, 'blue_0': -39.98000000000031, 'blue_1': -36.020000000000266, 'red_0': -35.700000000000216}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 186.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1122000.0, 'blue_0': 1122000.0, 'blue_1': 1122000.0, 'red_0': 1122000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -28.900000000000148, 'blue_policy': -36.020000000000266}, 'num_module_steps_sampled_lifetime': {'red_policy': 2244000.0, 'blue_policy': 2244000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003542252193985356, 'episode_return_mean': -140.60000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -120.30000000000081, 'episode_duration_sec_mean': 19.38788564002607, 'episode_return_min': -151.100000000001, 'rlmodule_inference_timer': 0.013441695606715744, 'num_episodes_lifetime': 935.0, 'episode_len_min': 1200, 'time_between_sampling': 313.6449805999873, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.595009609610035, 'throughput_since_last_restore': 15.966901193484183}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.899999999999988, 'red_1': -6.8999999999999915, 'blue_0': -48.4000000000004, 'blue_1': -43.60000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 731
(MultiAgentEnvRunner pid=37492) {'red_0': -19.400000000000023, 'red_1': -22.800000000000026, 'blue_0': -25.200000000000134, 'blue_1': -43.100000000000406}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 874
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 958
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1141
(MultiAgentEnvRunner pid=37492) {'red_0': -44.20000000000037, 'red_1': -14.09999999999994, 'blue_0': -50.7000000000004, 'blue_1': -42.30000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 511
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 521
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 840
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 946
(MultiAgentEnvRunner pid=37492) {'red_0': -27.400000000000087, 'red_1': -21.600000000000083, 'blue_0': -48.30000000000042, 'blue_1': -34.20000000000016}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 494
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 504
(MultiAgentEnvRunner pid=37492) {'red_0': -25.80000000000004, 'red_1': -16.39999999999998, 'blue_0': -22.900000000000016, 'blue_1': -33.60000000000019}
ITERATION 187: reward=-119.76000000000067, metadata={'num_env_steps_sampled_lifetime': 1128000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034784845169993786, 'timers': {'connectors': {'batch_individual_items': 0.00010392441074300095, 'add_states_from_episodes_to_batch': 7.121833337916953e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2460909881928646e-05, 'numpy_to_tensor': 7.257854558648293e-05, 'agent_to_module_mapping': 8.135583986696465e-06, 'add_observations_from_episodes_to_batch': 4.034140612141383e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009010403552615529, 'timers': {'connectors': {'get_actions': 0.00045384891268740235, 'un_batch_to_individual_items': 7.210366055473852e-05, 'tensor_to_numpy': 0.0001228363063649529, 'module_to_agent_unmapping': 6.473089257987595e-06, 'normalize_and_clip_actions': 7.875641225655577e-05, 'listify_data_for_vector_env': 2.4138593109274535e-05, 'remove_single_ts_time_rank_from_batch': 2.3184793587952223e-06}}}, 'sample': 92.61903379997239, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -16.360000000000003, 'blue_0': -39.10000000000027, 'blue_1': -39.360000000000284, 'red_0': -24.940000000000104}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 187.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1128000.0, 'blue_0': 1128000.0, 'blue_1': 1128000.0, 'red_0': 1128000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -16.360000000000003, 'blue_policy': -39.360000000000284}, 'num_module_steps_sampled_lifetime': {'red_policy': 2256000.0, 'blue_policy': 2256000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035258685286167413, 'episode_return_mean': -119.76000000000067, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -98.70000000000023, 'episode_duration_sec_mean': 18.390005400031804, 'episode_return_min': -151.30000000000103, 'rlmodule_inference_timer': 0.012992904641769824, 'num_episodes_lifetime': 940.0, 'episode_len_min': 1200, 'time_between_sampling': 313.3013903000392, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.304677788905847, 'throughput_since_last_restore': 15.963226653016754}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 703
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 835
(MultiAgentEnvRunner pid=37492) {'red_0': -48.60000000000039, 'red_1': -36.20000000000023, 'blue_0': -54.000000000000476, 'blue_1': -42.30000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 842
(MultiAgentEnvRunner pid=37492) {'red_0': -7.599999999999989, 'red_1': -14.09999999999997, 'blue_0': -49.50000000000041, 'blue_1': -41.700000000000344}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 296
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1019
(MultiAgentEnvRunner pid=37492) {'red_0': -25.300000000000082, 'red_1': -22.800000000000086, 'blue_0': -48.80000000000041, 'blue_1': -41.100000000000314}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -9.799999999999981, 'red_1': -9.799999999999981, 'blue_0': -45.20000000000036, 'blue_1': -46.00000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 397
(MultiAgentEnvRunner pid=37492) {'red_0': -9.099999999999993, 'red_1': -9.09999999999999, 'blue_0': -40.70000000000026, 'blue_1': -44.40000000000033}
ITERATION 188: reward=-129.22000000000088, metadata={'num_env_steps_sampled_lifetime': 1134000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000324978934096103, 'timers': {'connectors': {'batch_individual_items': 9.736305644306377e-05, 'add_states_from_episodes_to_batch': 7.070758887959634e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2332425966433011e-05, 'numpy_to_tensor': 6.674779736436086e-05, 'agent_to_module_mapping': 8.126242269817868e-06, 'add_observations_from_episodes_to_batch': 3.798668427902189e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008509764134259828, 'timers': {'connectors': {'get_actions': 0.00043896150312021885, 'un_batch_to_individual_items': 6.300845463666606e-05, 'tensor_to_numpy': 0.00011598503536068384, 'module_to_agent_unmapping': 6.770779571393594e-06, 'normalize_and_clip_actions': 7.004646708438937e-05, 'listify_data_for_vector_env': 2.3265001134980014e-05, 'remove_single_ts_time_rank_from_batch': 2.4535500566170946e-06}}}, 'sample': 92.38482409995049, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.400000000000052, 'blue_0': -47.640000000000384, 'blue_1': -43.10000000000034, 'red_0': -20.080000000000087}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 188.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1134000.0, 'blue_0': 1134000.0, 'blue_1': 1134000.0, 'red_0': 1134000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.400000000000052, 'blue_policy': -43.10000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 2268000.0, 'blue_policy': 2268000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003375705314599281, 'episode_return_mean': -129.22000000000088, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -103.30000000000058, 'episode_duration_sec_mean': 18.35020241998136, 'episode_return_min': -181.10000000000144, 'rlmodule_inference_timer': 0.012610205975489428, 'num_episodes_lifetime': 945.0, 'episode_len_min': 1200, 'time_between_sampling': 299.4197724000551, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.967682688825047, 'throughput_since_last_restore': 15.957610322459141}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 438
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 591
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 591
(MultiAgentEnvRunner pid=37492) {'red_0': -39.800000000000246, 'red_1': -32.700000000000266, 'blue_0': -48.7000000000004, 'blue_1': -28.500000000000114}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 709
(MultiAgentEnvRunner pid=37492) {'red_0': -27.900000000000144, 'red_1': -31.300000000000185, 'blue_0': -35.4000000000003, 'blue_1': -22.200000000000077}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 590
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 747
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1118
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1137
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1191
(MultiAgentEnvRunner pid=37492) {'red_0': -39.70000000000029, 'red_1': -27.30000000000012, 'blue_0': -34.5000000000004, 'blue_1': -20.09999999999998}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 929
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 965
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1083
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1164
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1191
(MultiAgentEnvRunner pid=37492) {'red_0': -32.00000000000013, 'red_1': -31.000000000000114, 'blue_0': -45.300000000000445, 'blue_1': -28.00000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 436
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 675
(MultiAgentEnvRunner pid=37492) {'red_0': -23.500000000000085, 'red_1': -42.40000000000025, 'blue_0': -38.700000000000244, 'blue_1': -22.700000000000024}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -118.39999999999755, 'red_1': -0.4, 'blue_0': -118.29999999999755, 'blue_1': -118.09999999999756}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -60.70000000000059, 'blue_0': -59.30000000000057, 'blue_1': -0.5}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 11:06:47,347	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -58.800000000000566, 'blue_1': -59.60000000000058}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -7.29999999999999, 'red_1': 0, 'blue_0': -111.69999999999793, 'blue_1': -5.4999999999999964}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) PICKED UP by blue_0 at STEP 295
(MultiAgentEnvRunner pid=41856) {'red_0': -97.69999999999872, 'red_1': -114.89999999999766, 'blue_0': -79.89999999999971, 'blue_1': -89.2999999999992}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 189: reward=-130.34000000000086, metadata={'num_env_steps_sampled_lifetime': 1140000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033000667108116014, 'timers': {'connectors': {'batch_individual_items': 9.711647214049514e-05, 'add_states_from_episodes_to_batch': 6.36624833694115e-06, 'add_time_dim_to_batch_and_zero_pad': 1.259756490705671e-05, 'numpy_to_tensor': 6.997230714765533e-05, 'agent_to_module_mapping': 8.054005891767007e-06, 'add_observations_from_episodes_to_batch': 3.848476982384831e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000863220792619153, 'timers': {'connectors': {'get_actions': 0.00044697035801843276, 'un_batch_to_individual_items': 6.402018971098397e-05, 'tensor_to_numpy': 0.0001174515868156835, 'module_to_agent_unmapping': 6.3766283055232094e-06, 'normalize_and_clip_actions': 7.157678578049275e-05, 'listify_data_for_vector_env': 2.3267642521384726e-05, 'remove_single_ts_time_rank_from_batch': 2.255919899310684e-06}}}, 'sample': 91.49979190004524, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.94000000000019, 'blue_0': -40.52000000000036, 'blue_1': -24.300000000000114, 'red_0': -32.580000000000176}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 189.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1140000.0, 'blue_0': 1140000.0, 'blue_1': 1140000.0, 'red_0': 1140000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.94000000000019, 'blue_policy': -24.300000000000114}, 'num_module_steps_sampled_lifetime': {'red_policy': 2280000.0, 'blue_policy': 2280000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003420768225517986, 'episode_return_mean': -130.34000000000086, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -116.8000000000007, 'episode_duration_sec_mean': 18.18131097999867, 'episode_return_min': -149.700000000001, 'rlmodule_inference_timer': 0.012989195525819685, 'num_episodes_lifetime': 950.0, 'episode_len_min': 1200, 'time_between_sampling': 308.4900225000456, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.8644950419485, 'throughput_since_last_restore': 15.937441507483467}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 797
(MultiAgentEnvRunner pid=37492) {'red_0': -19.300000000000022, 'red_1': -44.10000000000027, 'blue_0': -40.80000000000037, 'blue_1': -21.80000000000011}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 670
(MultiAgentEnvRunner pid=37492) {'red_0': -30.000000000000096, 'red_1': -37.20000000000018, 'blue_0': -45.30000000000039, 'blue_1': -19.300000000000008}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 857
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1164
(MultiAgentEnvRunner pid=37492) {'red_0': -56.200000000000465, 'red_1': -53.00000000000043, 'blue_0': -24.500000000000142, 'blue_1': -47.20000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 505
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 587
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1173
(MultiAgentEnvRunner pid=37492) {'red_0': -38.70000000000023, 'red_1': -50.7000000000005, 'blue_0': -51.8000000000004, 'blue_1': -29.400000000000073}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 419
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 495
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 947
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 953
(MultiAgentEnvRunner pid=37492) {'red_0': -38.70000000000028, 'red_1': -11.29999999999998, 'blue_0': -42.2000000000003, 'blue_1': -30.100000000000108}
ITERATION 190: reward=-146.32000000000093, metadata={'num_env_steps_sampled_lifetime': 1146000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003209856410672239, 'timers': {'connectors': {'batch_individual_items': 9.421453103925036e-05, 'add_states_from_episodes_to_batch': 6.545001224036402e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2725552636772181e-05, 'numpy_to_tensor': 6.605455173104013e-05, 'agent_to_module_mapping': 7.956327582677838e-06, 'add_observations_from_episodes_to_batch': 3.850310394175154e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008565829606133163, 'timers': {'connectors': {'get_actions': 0.00044129217526594746, 'un_batch_to_individual_items': 6.371088512335143e-05, 'tensor_to_numpy': 0.00011525386310742787, 'module_to_agent_unmapping': 6.326089740690119e-06, 'normalize_and_clip_actions': 7.082031053542688e-05, 'listify_data_for_vector_env': 2.412277350542495e-05, 'remove_single_ts_time_rank_from_batch': 2.511901705152566e-06}}}, 'sample': 91.6997070000507, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -39.260000000000275, 'blue_0': -40.92000000000032, 'blue_1': -29.56000000000015, 'red_0': -36.58000000000022}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 190.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1146000.0, 'blue_0': 1146000.0, 'blue_1': 1146000.0, 'red_0': 1146000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -39.260000000000275, 'blue_policy': -29.56000000000015}, 'num_module_steps_sampled_lifetime': {'red_policy': 2292000.0, 'blue_policy': 2292000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003446901893257298, 'episode_return_mean': -146.32000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -122.30000000000067, 'episode_duration_sec_mean': 18.179886940028517, 'episode_return_min': -180.90000000000146, 'rlmodule_inference_timer': 0.012679993027266941, 'num_episodes_lifetime': 955.0, 'episode_len_min': 1200, 'time_between_sampling': 374.9015004999237, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.225541318160747, 'throughput_since_last_restore': 15.93353997003297}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 394.0351953000063, 'restore_env_runners': 1.3199984095990658e-05, 'training_step': 394.03487259999383, 'env_runner_sampling_timer': 91.83486240007915, 'learner_update_timer': 302.1366341999965, 'synch_weights': 0.018258700030855834, 'synch_env_connectors': 0.001988200005143881, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1146000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003209856410672239, 'timers': {'connectors': {'batch_individual_items': 9.421453103925036e-05, 'add_states_from_episodes_to_batch': 6.545001224036402e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2725552636772181e-05, 'numpy_to_tensor': 6.605455173104013e-05, 'agent_to_module_mapping': 7.956327582677838e-06, 'add_observations_from_episodes_to_batch': 3.850310394175154e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008565829606133163, 'timers': {'connectors': {'get_actions': 0.00044129217526594746, 'un_batch_to_individual_items': 6.371088512335143e-05, 'tensor_to_numpy': 0.00011525386310742787, 'module_to_agent_unmapping': 6.326089740690119e-06, 'normalize_and_clip_actions': 7.082031053542688e-05, 'listify_data_for_vector_env': 2.412277350542495e-05, 'remove_single_ts_time_rank_from_batch': 2.511901705152566e-06}}}, 'sample': 91.6997070000507, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -39.260000000000275, 'blue_0': -40.92000000000032, 'blue_1': -29.56000000000015, 'red_0': -36.58000000000022}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 190.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1146000.0, 'blue_0': 1146000.0, 'blue_1': 1146000.0, 'red_0': 1146000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -39.260000000000275, 'blue_policy': -29.56000000000015}, 'num_module_steps_sampled_lifetime': {'red_policy': 2292000.0, 'blue_policy': 2292000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003446901893257298, 'episode_return_mean': -146.32000000000093, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -122.30000000000067, 'episode_duration_sec_mean': 18.179886940028517, 'episode_return_min': -180.90000000000146, 'rlmodule_inference_timer': 0.012679993027266941, 'num_episodes_lifetime': 955.0, 'episode_len_min': 1200, 'time_between_sampling': 374.9015004999237, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.225541318160747, 'throughput_since_last_restore': 15.93353997003297}}, 'learners': {'red_policy': {'policy_loss': -0.027118220925331116, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.010342977941036224, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 191.0, 'num_module_steps_trained_lifetime': 68821120.0, 'curr_entropy_coeff': 0.032810000000000006, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001854, 'vf_explained_var': -0.8322515487670898, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.952397346496582, 'entropy': 1.1597497463226318, 'vf_loss_unclipped': 652.2608642578125, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 914.3512401616775, 'throughput_since_last_restore': 956.8621824345009}}, 'blue_policy': {'weights_seq_no': 191.0, 'num_module_steps_trained_lifetime': 68821120.0, 'curr_entropy_coeff': 0.032810000000000006, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001854, 'vf_explained_var': 0.6443446278572083, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 1.176459789276123, 'total_loss': 0.7471733093261719, 'entropy': 1.7005654573440552, 'policy_loss': -0.012558147311210632, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.7943401336669922, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.014050979167222977, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 914.3491029700036, 'throughput_since_last_restore': 956.8621834123345}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 7.3999399319291115e-06, 'batch_individual_items': 0.8630161999026313, 'add_time_dim_to_batch_and_zero_pad': 2.3600063286721706e-05, 'numpy_to_tensor': 0.14691680006217211, 'add_observations_from_episodes_to_batch': 0.0002514999359846115, 'agent_to_module_mapping': 0.021204399992711842, 'add_one_ts_to_episodes_and_truncate': 0.1860224000411108, 'add_columns_from_episodes_to_train_batch': 0.48949949990492314, 'general_advantage_estimation': 13.539724300033413}}, 'connector_pipeline_timer': 15.24707749998197}, 'num_module_steps_trained_lifetime': 137642240.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 3225990000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 42860.054806902146, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 42860.05316458876, 'throughput_since_last_restore': 44852.91489597063}, 'num_module_steps_trained_throughput': 1828.6956077218124, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1828.6955900883306, 'throughput_since_last_restore': 1913.7243683333236}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1146000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 191, 'trial_id': 'default', 'date': '2026-01-26_11-17-36', 'timestamp': 1769422656, 'time_this_iter_s': 394.051385641098, 'time_total_s': 71899.1997179985, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 71899.1997179985, 'iterations_since_restore': 191, 'perf': {'cpu_util_percent': 15.394306049822065, 'ram_util_percent': 86.21601423487543}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1155
(MultiAgentEnvRunner pid=37492) {'red_0': -30.000000000000156, 'red_1': -46.400000000000354, 'blue_0': -44.40000000000039, 'blue_1': -40.00000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 452
(MultiAgentEnvRunner pid=37492) {'red_0': -13.299999999999986, 'red_1': -13.999999999999984, 'blue_0': -32.00000000000016, 'blue_1': -36.60000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1051
(MultiAgentEnvRunner pid=37492) {'red_0': -30.400000000000095, 'red_1': -11.099999999999994, 'blue_0': -21.200000000000244, 'blue_1': -51.700000000000514}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 592
(MultiAgentEnvRunner pid=37492) {'red_0': -28.800000000000168, 'red_1': -51.80000000000045, 'blue_0': -41.10000000000031, 'blue_1': -39.10000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.699999999999989, 'red_1': -3.900000000000002, 'blue_0': -55.60000000000049, 'blue_1': -43.90000000000035}
ITERATION 191: reward=-128.60000000000088, metadata={'num_env_steps_sampled_lifetime': 1152000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003354854163152261, 'timers': {'connectors': {'batch_individual_items': 0.00010709840112505949, 'add_states_from_episodes_to_batch': 6.450042767786673e-06, 'add_time_dim_to_batch_and_zero_pad': 1.219310368987432e-05, 'numpy_to_tensor': 6.845765604829329e-05, 'agent_to_module_mapping': 8.224738520207841e-06, 'add_observations_from_episodes_to_batch': 3.776081172512834e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008662285290801293, 'timers': {'connectors': {'get_actions': 0.00044232573130683825, 'un_batch_to_individual_items': 6.502173041182285e-05, 'tensor_to_numpy': 0.0001189499098873748, 'module_to_agent_unmapping': 6.377780456215856e-06, 'normalize_and_clip_actions': 7.22651171852715e-05, 'listify_data_for_vector_env': 2.453444700687265e-05, 'remove_single_ts_time_rank_from_batch': 2.3222236098266893e-06}}}, 'sample': 93.92401089996565, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.44000000000016, 'blue_0': -38.86000000000032, 'blue_1': -42.26000000000034, 'red_0': -22.040000000000077}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 191.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1152000.0, 'blue_0': 1152000.0, 'blue_1': 1152000.0, 'red_0': 1152000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.44000000000016, 'blue_policy': -42.26000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 2304000.0, 'blue_policy': 2304000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033505953251382066, 'episode_return_mean': -128.60000000000088, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -95.90000000000039, 'episode_duration_sec_mean': 18.640642859996298, 'episode_return_min': -160.8000000000012, 'rlmodule_inference_timer': 0.01274192207053819, 'num_episodes_lifetime': 960.0, 'episode_len_min': 1200, 'time_between_sampling': 302.49505230004434, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.369280302929903, 'throughput_since_last_restore': 15.930493292495102}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 421
(MultiAgentEnvRunner pid=37492) {'red_0': -43.20000000000031, 'red_1': -34.00000000000021, 'blue_0': -38.50000000000028, 'blue_1': -39.9000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 378
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 680
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1014
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1074
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1087
(MultiAgentEnvRunner pid=37492) {'red_0': -41.70000000000018, 'red_1': -61.90000000000045, 'blue_0': -2.8999999999999155, 'blue_1': -27.500000000000263}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 881
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1114
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1128
(MultiAgentEnvRunner pid=37492) {'red_0': -33.3000000000002, 'red_1': -21.400000000000055, 'blue_0': -32.30000000000026, 'blue_1': -23.000000000000327}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 161
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 918
(MultiAgentEnvRunner pid=37492) {'red_0': -26.60000000000012, 'red_1': -24.90000000000013, 'blue_0': -47.30000000000038, 'blue_1': -26.00000000000008}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 455
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 753
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 753
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1084
(MultiAgentEnvRunner pid=37492) {'red_0': -31.40000000000018, 'red_1': -24.10000000000004, 'blue_0': -22.000000000000096, 'blue_1': -27.500000000000334}
ITERATION 192: reward=-125.88000000000083, metadata={'num_env_steps_sampled_lifetime': 1158000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034927561369731645, 'timers': {'connectors': {'batch_individual_items': 9.993940199000765e-05, 'add_states_from_episodes_to_batch': 7.088704583541076e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3309185982162643e-05, 'numpy_to_tensor': 7.236953675546799e-05, 'agent_to_module_mapping': 8.697119078241283e-06, 'add_observations_from_episodes_to_batch': 4.252924729775806e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009360963059165234, 'timers': {'connectors': {'get_actions': 0.0004762665575831836, 'un_batch_to_individual_items': 7.181282584450305e-05, 'tensor_to_numpy': 0.0001283079468963266, 'module_to_agent_unmapping': 6.93800442093882e-06, 'normalize_and_clip_actions': 7.852826382818507e-05, 'listify_data_for_vector_env': 2.6012030592029223e-05, 'remove_single_ts_time_rank_from_batch': 2.495990105947137e-06}}}, 'sample': 92.09533549996559, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.260000000000176, 'blue_0': -28.600000000000183, 'blue_1': -28.780000000000264, 'red_0': -35.240000000000194}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 192.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1158000.0, 'blue_0': 1158000.0, 'blue_1': 1158000.0, 'red_0': 1158000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.260000000000176, 'blue_policy': -28.780000000000264}, 'num_module_steps_sampled_lifetime': {'red_policy': 2316000.0, 'blue_policy': 2316000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036906883619315705, 'episode_return_mean': -125.88000000000083, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -105.00000000000065, 'episode_duration_sec_mean': 18.263537599961275, 'episode_return_min': -155.6000000000011, 'rlmodule_inference_timer': 0.014392383536966397, 'num_episodes_lifetime': 965.0, 'episode_len_min': 1200, 'time_between_sampling': 296.3473568999907, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.958372882608035, 'throughput_since_last_restore': 15.92513034590425}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 222
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 225
(MultiAgentEnvRunner pid=37492) {'red_0': -47.40000000000031, 'red_1': -24.20000000000009, 'blue_0': -32.600000000000165, 'blue_1': -36.80000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 264
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 784
(MultiAgentEnvRunner pid=37492) {'red_0': -56.60000000000046, 'red_1': -36.50000000000018, 'blue_0': -23.60000000000003, 'blue_1': -27.800000000000214}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -0.30000000000000004, 'red_1': -0.4, 'blue_0': -56.50000000000052, 'blue_1': -45.70000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 556
(MultiAgentEnvRunner pid=37492) {'red_0': -35.80000000000026, 'red_1': -26.80000000000011, 'blue_0': -40.900000000000304, 'blue_1': -51.000000000000426}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 654
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 776
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1188
(MultiAgentEnvRunner pid=37492) {'red_0': -22.00000000000002, 'red_1': -18.399999999999984, 'blue_0': -50.800000000000374, 'blue_1': -44.300000000000416}
ITERATION 193: reward=-135.68000000000092, metadata={'num_env_steps_sampled_lifetime': 1164000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00030229894784453, 'timers': {'connectors': {'batch_individual_items': 9.024699803574343e-05, 'add_states_from_episodes_to_batch': 5.9458829176972045e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1672841950676829e-05, 'numpy_to_tensor': 6.325422523852075e-05, 'agent_to_module_mapping': 7.708729475815766e-06, 'add_observations_from_episodes_to_batch': 3.556443555476275e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008166722625482405, 'timers': {'connectors': {'get_actions': 0.0004262417396193564, 'un_batch_to_individual_items': 6.0685761640684054e-05, 'tensor_to_numpy': 0.00011015374628018037, 'module_to_agent_unmapping': 5.900135766232916e-06, 'normalize_and_clip_actions': 6.610555541088274e-05, 'listify_data_for_vector_env': 2.1953439282387804e-05, 'remove_single_ts_time_rank_from_batch': 2.1635781127355106e-06}}}, 'sample': 88.66956579999533, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.26000000000007, 'blue_0': -40.88000000000028, 'blue_1': -41.12000000000033, 'red_0': -32.42000000000021}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 193.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1164000.0, 'blue_0': 1164000.0, 'blue_1': 1164000.0, 'red_0': 1164000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.26000000000007, 'blue_policy': -41.12000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2328000.0, 'blue_policy': 2328000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032557691167017674, 'episode_return_mean': -135.68000000000092, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -102.90000000000089, 'episode_duration_sec_mean': 17.597013879963196, 'episode_return_min': -154.5000000000011, 'rlmodule_inference_timer': 0.011857135914608548, 'num_episodes_lifetime': 970.0, 'episode_len_min': 1200, 'time_between_sampling': 309.01691310002934, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.673509641758066, 'throughput_since_last_restore': 15.92881512854169}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1031
(MultiAgentEnvRunner pid=37492) {'red_0': -26.000000000000032, 'red_1': -50.30000000000037, 'blue_0': -26.50000000000032, 'blue_1': -42.100000000000385}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 851
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 906
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1089
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1089
(MultiAgentEnvRunner pid=37492) {'red_0': -32.900000000000176, 'red_1': -11.899999999999984, 'blue_0': -44.7000000000004, 'blue_1': -27.800000000000235}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 805
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 963
(MultiAgentEnvRunner pid=37492) {'red_0': -16.59999999999995, 'red_1': -29.100000000000147, 'blue_0': -48.80000000000041, 'blue_1': -47.100000000000385}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 752
(MultiAgentEnvRunner pid=37492) {'red_0': -16.59999999999996, 'red_1': -25.400000000000095, 'blue_0': -39.300000000000274, 'blue_1': -40.1000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 559
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 802
(MultiAgentEnvRunner pid=37492) {'red_0': -6.9999999999999725, 'red_1': -19.800000000000086, 'blue_0': -61.50000000000051, 'blue_1': -53.00000000000039}
ITERATION 194: reward=-133.3000000000009, metadata={'num_env_steps_sampled_lifetime': 1170000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033230981907814936, 'timers': {'connectors': {'batch_individual_items': 0.00011225479360010658, 'add_states_from_episodes_to_batch': 6.142207632324016e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1807699253743113e-05, 'numpy_to_tensor': 6.47212441416442e-05, 'agent_to_module_mapping': 7.646318981805861e-06, 'add_observations_from_episodes_to_batch': 3.740370067470077e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008432002083065533, 'timers': {'connectors': {'get_actions': 0.00043572460165670733, 'un_batch_to_individual_items': 6.296433273233923e-05, 'tensor_to_numpy': 0.00011437253038702631, 'module_to_agent_unmapping': 6.209908962330237e-06, 'normalize_and_clip_actions': 6.967629638398342e-05, 'listify_data_for_vector_env': 2.3153392406683644e-05, 'remove_single_ts_time_rank_from_batch': 2.2878877248415748e-06}}}, 'sample': 87.66501120000612, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.300000000000136, 'blue_0': -44.16000000000038, 'blue_1': -42.02000000000034, 'red_0': -19.820000000000014}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 194.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1170000.0, 'blue_0': 1170000.0, 'blue_1': 1170000.0, 'red_0': 1170000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.300000000000136, 'blue_policy': -42.02000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 2340000.0, 'blue_policy': 2340000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003382551657440372, 'episode_return_mean': -133.3000000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -117.30000000000081, 'episode_duration_sec_mean': 17.408770380029456, 'episode_return_min': -144.9000000000011, 'rlmodule_inference_timer': 0.012225209688827933, 'num_episodes_lifetime': 975.0, 'episode_len_min': 1200, 'time_between_sampling': 271.18808770005126, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.235504880717173, 'throughput_since_last_restore': 15.935010013706496}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 885
(MultiAgentEnvRunner pid=37492) {'red_0': -26.100000000000033, 'red_1': -18.29999999999999, 'blue_0': -35.6000000000003, 'blue_1': -21.700000000000188}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 995
(MultiAgentEnvRunner pid=37492) {'red_0': -31.80000000000011, 'red_1': -24.600000000000083, 'blue_0': -31.00000000000024, 'blue_1': -28.900000000000347}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 331
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 507
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1140
(MultiAgentEnvRunner pid=37492) {'red_0': -64.50000000000058, 'red_1': -49.60000000000036, 'blue_0': -50.500000000000504, 'blue_1': -18.999999999999986}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 990
(MultiAgentEnvRunner pid=37492) {'red_0': -19.80000000000003, 'red_1': -36.20000000000017, 'blue_0': -25.80000000000031, 'blue_1': -44.300000000000416}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 402
(MultiAgentEnvRunner pid=37492) {'red_0': -16.799999999999986, 'red_1': -30.300000000000093, 'blue_0': -33.800000000000196, 'blue_1': -37.800000000000225}
ITERATION 195: reward=-129.28000000000083, metadata={'num_env_steps_sampled_lifetime': 1176000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032314318650365905, 'timers': {'connectors': {'batch_individual_items': 0.00010085647710439475, 'add_states_from_episodes_to_batch': 6.213439163313019e-06, 'add_time_dim_to_batch_and_zero_pad': 1.224955372774324e-05, 'numpy_to_tensor': 6.57699652245119e-05, 'agent_to_module_mapping': 7.830964535249001e-06, 'add_observations_from_episodes_to_batch': 3.772878183346802e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008578093110431683, 'timers': {'connectors': {'get_actions': 0.00044566632392902835, 'un_batch_to_individual_items': 6.320831225447714e-05, 'tensor_to_numpy': 0.00011667366473100747, 'module_to_agent_unmapping': 6.275541263320314e-06, 'normalize_and_clip_actions': 7.00180608830633e-05, 'listify_data_for_vector_env': 2.3302513237289712e-05, 'remove_single_ts_time_rank_from_batch': 2.2960183668750994e-06}}}, 'sample': 89.47807299997658, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -31.800000000000143, 'blue_0': -35.3400000000003, 'blue_1': -30.340000000000238, 'red_0': -31.800000000000146}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 195.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1176000.0, 'blue_0': 1176000.0, 'blue_1': 1176000.0, 'red_0': 1176000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -31.800000000000143, 'blue_policy': -30.340000000000238}, 'num_module_steps_sampled_lifetime': {'red_policy': 2352000.0, 'blue_policy': 2352000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033967989863529173, 'episode_return_mean': -129.28000000000083, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.7000000000005, 'episode_duration_sec_mean': 17.746951180021277, 'episode_return_min': -183.60000000000144, 'rlmodule_inference_timer': 0.012554017070969073, 'num_episodes_lifetime': 980.0, 'episode_len_min': 1200, 'time_between_sampling': 260.45758460008074, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.883562705649137, 'throughput_since_last_restore': 15.939578503114953}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 626
(MultiAgentEnvRunner pid=37492) {'red_0': -43.300000000000274, 'red_1': -56.000000000000426, 'blue_0': -44.70000000000043, 'blue_1': -32.50000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.599999999999996, 'red_1': -6.299999999999994, 'blue_0': -35.600000000000236, 'blue_1': -48.800000000000416}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 443
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 864
(MultiAgentEnvRunner pid=37492) {'red_0': -36.400000000000105, 'red_1': -27.000000000000107, 'blue_0': -7.9999999999999245, 'blue_1': -29.90000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 258
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 436
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 467
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 556
(MultiAgentEnvRunner pid=37492) {'red_0': -40.70000000000033, 'red_1': -19.69999999999999, 'blue_0': -50.6000000000004, 'blue_1': -39.4000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 522
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 631
(MultiAgentEnvRunner pid=37492) {'red_0': -31.70000000000018, 'red_1': -38.50000000000029, 'blue_0': -50.60000000000041, 'blue_1': -34.70000000000022}
ITERATION 196: reward=-136.0000000000009, metadata={'num_env_steps_sampled_lifetime': 1182000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00036707561037291835, 'timers': {'connectors': {'batch_individual_items': 0.00010216518365977721, 'add_states_from_episodes_to_batch': 7.039356683218597e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4630808097724337e-05, 'numpy_to_tensor': 8.174629984499047e-05, 'agent_to_module_mapping': 9.118638748085455e-06, 'add_observations_from_episodes_to_batch': 4.4337372476861914e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009963309532103177, 'timers': {'connectors': {'get_actions': 0.0005089269097134491, 'un_batch_to_individual_items': 7.529066304532416e-05, 'tensor_to_numpy': 0.0001356793188829487, 'module_to_agent_unmapping': 7.626344908879058e-06, 'normalize_and_clip_actions': 8.234588638018192e-05, 'listify_data_for_vector_env': 2.7568700134613213e-05, 'remove_single_ts_time_rank_from_batch': 2.751006470739678e-06}}}, 'sample': 93.53630919998977, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.50000000000016, 'blue_0': -37.90000000000028, 'blue_1': -37.06000000000028, 'red_0': -31.54000000000018}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 196.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1182000.0, 'blue_0': 1182000.0, 'blue_1': 1182000.0, 'red_0': 1182000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.50000000000016, 'blue_policy': -37.06000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 2364000.0, 'blue_policy': 2364000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00037991974328611353, 'episode_return_mean': -136.0000000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -96.30000000000064, 'episode_duration_sec_mean': 18.561353820026852, 'episode_return_min': -176.5000000000014, 'rlmodule_inference_timer': 0.01487549146084061, 'num_episodes_lifetime': 985.0, 'episode_len_min': 1200, 'time_between_sampling': 265.9002803999465, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.562805696684144, 'throughput_since_last_restore': 15.942623149915114}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1026
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1030
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1030
(MultiAgentEnvRunner pid=37492) {'red_0': -18.699999999999996, 'red_1': -7.799999999999992, 'blue_0': -40.500000000000476, 'blue_1': -33.50000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 907
(MultiAgentEnvRunner pid=37492) {'red_0': -44.90000000000029, 'red_1': -46.30000000000031, 'blue_0': -34.00000000000028, 'blue_1': -22.200000000000223}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 279
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 375
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 613
(MultiAgentEnvRunner pid=37492) {'red_0': -25.90000000000013, 'red_1': -13.699999999999978, 'blue_0': -38.700000000000244, 'blue_1': -35.40000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 777
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1033
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1078
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1146
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1146
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1146
(MultiAgentEnvRunner pid=37492) {'red_0': -31.40000000000024, 'red_1': -10.89999999999998, 'blue_0': -35.9000000000003, 'blue_1': -60.20000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 939
(MultiAgentEnvRunner pid=37492) {'red_0': -38.90000000000021, 'red_1': -11.299999999999994, 'blue_0': -28.50000000000034, 'blue_1': -44.400000000000425}
ITERATION 197: reward=-124.62000000000087, metadata={'num_env_steps_sampled_lifetime': 1188000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00046671285701304824, 'timers': {'connectors': {'batch_individual_items': 0.00013997695318745756, 'add_states_from_episodes_to_batch': 9.616180965789064e-06, 'add_time_dim_to_batch_and_zero_pad': 1.769251893410104e-05, 'numpy_to_tensor': 9.352560998137739e-05, 'agent_to_module_mapping': 1.1322487971617662e-05, 'add_observations_from_episodes_to_batch': 5.1396317028466535e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0011705329896454044, 'timers': {'connectors': {'get_actions': 0.0005855823257964762, 'un_batch_to_individual_items': 9.199802162091701e-05, 'tensor_to_numpy': 0.00015640223818527967, 'module_to_agent_unmapping': 9.74690207885568e-06, 'normalize_and_clip_actions': 9.828505318205686e-05, 'listify_data_for_vector_env': 3.422549471120389e-05, 'remove_single_ts_time_rank_from_batch': 3.349090046271902e-06}}}, 'sample': 117.50951529992744, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.00000000000005, 'blue_0': -35.52000000000033, 'blue_1': -39.14000000000032, 'red_0': -31.96000000000017}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 197.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1188000.0, 'blue_0': 1188000.0, 'blue_1': 1188000.0, 'red_0': 1188000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.00000000000005, 'blue_policy': -39.14000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 2376000.0, 'blue_policy': 2376000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0004676317630715075, 'episode_return_mean': -124.62000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -100.50000000000071, 'episode_duration_sec_mean': 23.314203080022708, 'episode_return_min': -147.40000000000109, 'rlmodule_inference_timer': 0.01877786449265317, 'num_episodes_lifetime': 990.0, 'episode_len_min': 1200, 'time_between_sampling': 268.7207266999176, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.487920261514883, 'throughput_since_last_restore': 15.940259099736899}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1053
(MultiAgentEnvRunner pid=37492) {'red_0': -31.000000000000096, 'red_1': -22.799999999999986, 'blue_0': -34.500000000000426, 'blue_1': -39.10000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -38.300000000000274, 'red_1': -24.50000000000007, 'blue_0': -41.60000000000032, 'blue_1': -47.3000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 472
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 711
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 1042
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1192
(MultiAgentEnvRunner pid=37492) {'red_0': -43.30000000000022, 'red_1': -34.20000000000016, 'blue_0': -53.6000000000005, 'blue_1': -33.900000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 386
(MultiAgentEnvRunner pid=37492) {'red_0': -36.500000000000256, 'red_1': -18.6, 'blue_0': -28.800000000000093, 'blue_1': -45.30000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 527
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 679
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 728
(MultiAgentEnvRunner pid=37492) {'red_0': -24.000000000000092, 'red_1': -33.40000000000014, 'blue_0': -17.199999999999974, 'blue_1': -30.500000000000234}
ITERATION 198: reward=-135.68000000000092, metadata={'num_env_steps_sampled_lifetime': 1194000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003084062639812222, 'timers': {'connectors': {'batch_individual_items': 9.09133778315045e-05, 'add_states_from_episodes_to_batch': 5.975290260607497e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1885339859406877e-05, 'numpy_to_tensor': 6.488147907997829e-05, 'agent_to_module_mapping': 7.615455401035552e-06, 'add_observations_from_episodes_to_batch': 3.725745667411603e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008343355232038587, 'timers': {'connectors': {'get_actions': 0.0004345642531101059, 'un_batch_to_individual_items': 6.127410050842127e-05, 'tensor_to_numpy': 0.00011359869280474755, 'module_to_agent_unmapping': 6.146977670732089e-06, 'normalize_and_clip_actions': 6.827590667291121e-05, 'listify_data_for_vector_env': 2.2578751445385235e-05, 'remove_single_ts_time_rank_from_batch': 2.197943441600452e-06}}}, 'sample': 83.55249669996556, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.700000000000074, 'blue_0': -35.140000000000256, 'blue_1': -39.220000000000354, 'red_0': -34.62000000000019}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 198.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1194000.0, 'blue_0': 1194000.0, 'blue_1': 1194000.0, 'red_0': 1194000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.700000000000074, 'blue_policy': -39.220000000000354}, 'num_module_steps_sampled_lifetime': {'red_policy': 2388000.0, 'blue_policy': 2388000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00032593102488298446, 'episode_return_mean': -135.68000000000092, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -105.10000000000045, 'episode_duration_sec_mean': 16.587643579998986, 'episode_return_min': -165.00000000000125, 'rlmodule_inference_timer': 0.012103671673485859, 'num_episodes_lifetime': 995.0, 'episode_len_min': 1200, 'time_between_sampling': 269.8931584999664, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.268540673299245, 'throughput_since_last_restore': 15.936735469458165}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 213
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 773
(MultiAgentEnvRunner pid=37492) {'red_0': -26.80000000000004, 'red_1': -39.60000000000016, 'blue_0': -26.7000000000001, 'blue_1': -30.500000000000377}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 993
(MultiAgentEnvRunner pid=37492) {'red_0': -19.500000000000025, 'red_1': -25.200000000000045, 'blue_0': -39.000000000000355, 'blue_1': -35.30000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 511
(MultiAgentEnvRunner pid=37492) {'red_0': -28.20000000000014, 'red_1': -39.60000000000029, 'blue_0': -29.20000000000011, 'blue_1': -49.60000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 283
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 414
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 733
(MultiAgentEnvRunner pid=37492) {'red_0': -58.20000000000045, 'red_1': -22.300000000000068, 'blue_0': -44.00000000000042, 'blue_1': -31.600000000000364}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 333
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1163
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1185
(MultiAgentEnvRunner pid=37492) {'red_0': -25.200000000000028, 'red_1': -38.700000000000216, 'blue_0': -31.2000000000003, 'blue_1': -15.900000000000132}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.4, 'blue_1': -59.60000000000058}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.7999999999999999, 'blue_0': -6.8999999999999915, 'blue_1': -0.1}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 12:11:06,491	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -0.30000000000000004, 'red_1': 0, 'blue_0': -39.60000000000029, 'blue_1': -38.50000000000028}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.7999999999999999, 'blue_1': -0.6}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.5, 'blue_1': -39.700000000000294}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 199: reward=-131.2600000000009, metadata={'num_env_steps_sampled_lifetime': 1200000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003551938723566142, 'timers': {'connectors': {'batch_individual_items': 0.00010362063886494878, 'add_states_from_episodes_to_batch': 6.8156846529950245e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4194531614220301e-05, 'numpy_to_tensor': 7.509698318051517e-05, 'agent_to_module_mapping': 8.781444962399774e-06, 'add_observations_from_episodes_to_batch': 4.116686772023263e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009592012419114529, 'timers': {'connectors': {'get_actions': 0.000491351084495886, 'un_batch_to_individual_items': 7.417157501035913e-05, 'tensor_to_numpy': 0.00012866083752911727, 'module_to_agent_unmapping': 6.918542383977065e-06, 'normalize_and_clip_actions': 7.821417201762168e-05, 'listify_data_for_vector_env': 2.6586784462269617e-05, 'remove_single_ts_time_rank_from_batch': 2.4780734456570695e-06}}}, 'sample': 92.55921850004233, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.080000000000155, 'blue_0': -34.02000000000026, 'blue_1': -32.580000000000354, 'red_0': -31.580000000000137}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 199.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1200000.0, 'blue_0': 1200000.0, 'blue_1': 1200000.0, 'red_0': 1200000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.080000000000155, 'blue_policy': -32.580000000000354}, 'num_module_steps_sampled_lifetime': {'red_policy': 2400000.0, 'blue_policy': 2400000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00037935196790676217, 'episode_return_mean': -131.2600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.00000000000068, 'episode_duration_sec_mean': 18.38343962000217, 'episode_return_min': -156.1000000000013, 'rlmodule_inference_timer': 0.014086352407878009, 'num_episodes_lifetime': 1000.0, 'episode_len_min': 1200, 'time_between_sampling': 309.4136584000662, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.683865201620819, 'throughput_since_last_restore': 15.916325699442101}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 447
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 651
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 723
(MultiAgentEnvRunner pid=37492) {'red_0': -35.10000000000021, 'red_1': -43.20000000000037, 'blue_0': -22.900000000000027, 'blue_1': -36.90000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 650
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1101
(MultiAgentEnvRunner pid=37492) {'red_0': -27.30000000000012, 'red_1': -15.499999999999979, 'blue_0': -28.400000000000325, 'blue_1': -38.40000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 490
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1127
(MultiAgentEnvRunner pid=37492) {'red_0': -19.099999999999984, 'red_1': -30.700000000000166, 'blue_0': -44.900000000000304, 'blue_1': -29.20000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 991
(MultiAgentEnvRunner pid=37492) {'red_0': -33.400000000000155, 'red_1': -30.00000000000009, 'blue_0': -28.700000000000344, 'blue_1': -41.500000000000384}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 361
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 753
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 775
(MultiAgentEnvRunner pid=37492) {'red_0': -32.90000000000019, 'red_1': -10.79999999999997, 'blue_0': -45.50000000000037, 'blue_1': -41.40000000000033}
ITERATION 200: reward=-127.16000000000084, metadata={'num_env_steps_sampled_lifetime': 1206000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003504474785995676, 'timers': {'connectors': {'batch_individual_items': 0.00010058723937408906, 'add_states_from_episodes_to_batch': 6.883827153529201e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4156659526168097e-05, 'numpy_to_tensor': 7.314936080809392e-05, 'agent_to_module_mapping': 8.716404203032369e-06, 'add_observations_from_episodes_to_batch': 4.217339028304924e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009347900839575294, 'timers': {'connectors': {'get_actions': 0.00047806464533715616, 'un_batch_to_individual_items': 7.008235276909995e-05, 'tensor_to_numpy': 0.0001272848820506919, 'module_to_agent_unmapping': 6.880148634290671e-06, 'normalize_and_clip_actions': 7.83288132485751e-05, 'listify_data_for_vector_env': 2.6398297160620608e-05, 'remove_single_ts_time_rank_from_batch': 2.4989643768834144e-06}}}, 'sample': 92.61818820005283, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.040000000000116, 'blue_0': -34.080000000000275, 'blue_1': -37.4800000000003, 'red_0': -29.560000000000127}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 200.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1206000.0, 'blue_0': 1206000.0, 'blue_1': 1206000.0, 'red_0': 1206000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.040000000000116, 'blue_policy': -37.4800000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 2412000.0, 'blue_policy': 2412000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036414889888994404, 'episode_return_mean': -127.16000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.60000000000076, 'episode_duration_sec_mean': 18.33869351998437, 'episode_return_min': -138.10000000000085, 'rlmodule_inference_timer': 0.014235176067289601, 'num_episodes_lifetime': 1005.0, 'episode_len_min': 1200, 'time_between_sampling': 380.48266159999184, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.11502862512406, 'throughput_since_last_restore': 15.912128368603152}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 396.91982960002497, 'restore_env_runners': 9.699957445263863e-06, 'training_step': 396.9196000000229, 'env_runner_sampling_timer': 92.7850159999216, 'learner_update_timer': 304.0771110999631, 'synch_weights': 0.013006500084884465, 'synch_env_connectors': 0.001984099973924458, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1206000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003504474785995676, 'timers': {'connectors': {'batch_individual_items': 0.00010058723937408906, 'add_states_from_episodes_to_batch': 6.883827153529201e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4156659526168097e-05, 'numpy_to_tensor': 7.314936080809392e-05, 'agent_to_module_mapping': 8.716404203032369e-06, 'add_observations_from_episodes_to_batch': 4.217339028304924e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009347900839575294, 'timers': {'connectors': {'get_actions': 0.00047806464533715616, 'un_batch_to_individual_items': 7.008235276909995e-05, 'tensor_to_numpy': 0.0001272848820506919, 'module_to_agent_unmapping': 6.880148634290671e-06, 'normalize_and_clip_actions': 7.83288132485751e-05, 'listify_data_for_vector_env': 2.6398297160620608e-05, 'remove_single_ts_time_rank_from_batch': 2.4989643768834144e-06}}}, 'sample': 92.61818820005283, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.040000000000116, 'blue_0': -34.080000000000275, 'blue_1': -37.4800000000003, 'red_0': -29.560000000000127}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 200.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1206000.0, 'blue_0': 1206000.0, 'blue_1': 1206000.0, 'red_0': 1206000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.040000000000116, 'blue_policy': -37.4800000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 2412000.0, 'blue_policy': 2412000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036414889888994404, 'episode_return_mean': -127.16000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.60000000000076, 'episode_duration_sec_mean': 18.33869351998437, 'episode_return_min': -138.10000000000085, 'rlmodule_inference_timer': 0.014235176067289601, 'num_episodes_lifetime': 1005.0, 'episode_len_min': 1200, 'time_between_sampling': 380.48266159999184, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.11502862512406, 'throughput_since_last_restore': 15.912128368603152}}, 'learners': {'red_policy': {'policy_loss': -0.14597228169441223, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.009532838128507137, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 201.0, 'num_module_steps_trained_lifetime': 72424320.0, 'curr_entropy_coeff': 0.03191, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00017939999999999997, 'vf_explained_var': -0.5475550889968872, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.839279174804688, 'entropy': 0.9698790311813354, 'vf_loss_unclipped': 718.84619140625, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 907.7112590638458, 'throughput_since_last_restore': 955.5763447564847}}, 'blue_policy': {'weights_seq_no': 201.0, 'num_module_steps_trained_lifetime': 72424320.0, 'curr_entropy_coeff': 0.03191, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00017939999999999997, 'vf_explained_var': 0.38455140590667725, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 2.929960250854492, 'total_loss': 1.316045880317688, 'entropy': 1.6804907321929932, 'policy_loss': -0.2225407063961029, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.5695148706436157, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.015043544583022594, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 907.7100500931675, 'throughput_since_last_restore': 955.5763457020848}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 1.1499971151351929e-05, 'batch_individual_items': 0.7938097000587732, 'add_time_dim_to_batch_and_zero_pad': 3.0800001695752144e-05, 'numpy_to_tensor': 0.14388930005952716, 'add_observations_from_episodes_to_batch': 0.0004755000118166208, 'agent_to_module_mapping': 0.020226099994033575, 'add_one_ts_to_episodes_and_truncate': 0.21830739988945425, 'add_columns_from_episodes_to_train_batch': 0.5251371000194922, 'general_advantage_estimation': 13.534452700056136}}, 'connector_pipeline_timer': 15.236835299991071}, 'num_module_steps_trained_lifetime': 144848640.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 3394890000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 42548.88179034713, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 42548.88008605559, 'throughput_since_last_restore': 44792.64124928754}, 'num_module_steps_trained_throughput': 1815.4188731528986, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1815.4188507432675, 'throughput_since_last_restore': 1911.1526928213084}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1206000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 201, 'trial_id': 'default', 'date': '2026-01-26_12-22-03', 'timestamp': 1769426523, 'time_this_iter_s': 396.9359540939331, 'time_total_s': 75766.3298728466, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 75766.3298728466, 'iterations_since_restore': 201, 'perf': {'cpu_util_percent': 14.938095238095238, 'ram_util_percent': 86.96067019400353}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 615
(MultiAgentEnvRunner pid=37492) {'red_0': -24.70000000000008, 'red_1': -33.3000000000002, 'blue_0': -40.300000000000296, 'blue_1': -44.90000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 539
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 974
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1118
(MultiAgentEnvRunner pid=37492) {'red_0': -35.70000000000024, 'red_1': -45.40000000000036, 'blue_0': -39.00000000000041, 'blue_1': -20.79999999999999}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -32.00000000000018, 'red_1': -14.699999999999964, 'blue_0': -36.10000000000024, 'blue_1': -40.4000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 755
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1022
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1061
(MultiAgentEnvRunner pid=37492) {'red_0': -60.20000000000048, 'red_1': -35.700000000000195, 'blue_0': -26.10000000000028, 'blue_1': -49.4000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1062
(MultiAgentEnvRunner pid=37492) {'red_0': -35.30000000000016, 'red_1': -52.700000000000394, 'blue_0': -41.90000000000038, 'blue_1': -33.90000000000042}
ITERATION 201: reward=-148.50000000000108, metadata={'num_env_steps_sampled_lifetime': 1212000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00039563065920586845, 'timers': {'connectors': {'batch_individual_items': 0.00012054236072227293, 'add_states_from_episodes_to_batch': 7.52693762477924e-06, 'add_time_dim_to_batch_and_zero_pad': 1.463713812423721e-05, 'numpy_to_tensor': 8.10860294634747e-05, 'agent_to_module_mapping': 9.494888524038083e-06, 'add_observations_from_episodes_to_batch': 4.570483945381186e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.001006677474036936, 'timers': {'connectors': {'get_actions': 0.0005106753241627284, 'un_batch_to_individual_items': 7.530249724277707e-05, 'tensor_to_numpy': 0.0001404478378159807, 'module_to_agent_unmapping': 8.153817865769836e-06, 'normalize_and_clip_actions': 8.366800305275423e-05, 'listify_data_for_vector_env': 2.842796504304578e-05, 'remove_single_ts_time_rank_from_batch': 2.7361175159096317e-06}}}, 'sample': 94.07016120001208, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -36.36000000000022, 'blue_0': -36.68000000000032, 'blue_1': -37.88000000000032, 'red_0': -37.580000000000226}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 201.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1212000.0, 'blue_0': 1212000.0, 'blue_1': 1212000.0, 'red_0': 1212000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -36.36000000000022, 'blue_policy': -37.88000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 2424000.0, 'blue_policy': 2424000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00039303005188556886, 'episode_return_mean': -148.50000000000108, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -123.20000000000067, 'episode_duration_sec_mean': 18.674712539999746, 'episode_return_min': -171.40000000000146, 'rlmodule_inference_timer': 0.015304076889125787, 'num_episodes_lifetime': 1010.0, 'episode_len_min': 1200, 'time_between_sampling': 304.44857719994616, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.999732074784252, 'throughput_since_last_restore': 15.907337775497924}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 331
(MultiAgentEnvRunner pid=37492) {'red_0': -31.600000000000172, 'red_1': -29.200000000000163, 'blue_0': -37.00000000000022, 'blue_1': -34.100000000000186}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 868
(MultiAgentEnvRunner pid=37492) {'red_0': -36.60000000000027, 'red_1': -54.10000000000048, 'blue_0': -42.60000000000033, 'blue_1': -50.10000000000042}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 472
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 551
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 583
(MultiAgentEnvRunner pid=37492) {'red_0': -42.00000000000026, 'red_1': -20.300000000000065, 'blue_0': -29.800000000000153, 'blue_1': -30.800000000000157}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -27.20000000000012, 'red_1': -9.89999999999998, 'blue_0': -43.70000000000035, 'blue_1': -37.30000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 542
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 857
(MultiAgentEnvRunner pid=37492) {'red_0': -29.100000000000144, 'red_1': -37.000000000000185, 'blue_0': -37.500000000000334, 'blue_1': -26.300000000000185}
ITERATION 202: reward=-137.2400000000009, metadata={'num_env_steps_sampled_lifetime': 1218000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033458081590446813, 'timers': {'connectors': {'batch_individual_items': 0.00010358273786734719, 'add_states_from_episodes_to_batch': 6.3382213388074645e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2431625249323258e-05, 'numpy_to_tensor': 6.853742089580891e-05, 'agent_to_module_mapping': 8.09426175815571e-06, 'add_observations_from_episodes_to_batch': 3.864128026281161e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008758496365919075, 'timers': {'connectors': {'get_actions': 0.00044755165810855726, 'un_batch_to_individual_items': 6.638906109786798e-05, 'tensor_to_numpy': 0.00012099619809494907, 'module_to_agent_unmapping': 6.403441238459954e-06, 'normalize_and_clip_actions': 7.315670002377892e-05, 'listify_data_for_vector_env': 2.3933763913865002e-05, 'remove_single_ts_time_rank_from_batch': 2.3376710892681047e-06}}}, 'sample': 94.65018979995511, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.100000000000172, 'blue_0': -38.120000000000275, 'blue_1': -35.72000000000024, 'red_0': -33.300000000000196}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 202.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1218000.0, 'blue_0': 1218000.0, 'blue_1': 1218000.0, 'red_0': 1218000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.100000000000172, 'blue_policy': -35.72000000000024}, 'num_module_steps_sampled_lifetime': {'red_policy': 2436000.0, 'blue_policy': 2436000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003460843400750242, 'episode_return_mean': -137.2400000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -118.1000000000007, 'episode_duration_sec_mean': 18.797859139996582, 'episode_return_min': -183.4000000000015, 'rlmodule_inference_timer': 0.013165652683928074, 'num_episodes_lifetime': 1015.0, 'episode_len_min': 1200, 'time_between_sampling': 305.835820499924, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.980689853329817, 'throughput_since_last_restore': 15.90249151911776}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 236
(MultiAgentEnvRunner pid=37492) {'red_0': -22.200000000000063, 'red_1': -16.39999999999998, 'blue_0': -35.20000000000021, 'blue_1': -21.300000000000015}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 541
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 782
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1000
(MultiAgentEnvRunner pid=37492) {'red_0': -51.70000000000037, 'red_1': -71.10000000000012, 'blue_0': -60.50000000000063, 'blue_1': -21.000000000000075}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -20.500000000000025, 'red_1': -8.899999999999984, 'blue_0': -44.20000000000035, 'blue_1': -37.80000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 841
(MultiAgentEnvRunner pid=37492) {'red_0': -9.399999999999983, 'red_1': -23.50000000000007, 'blue_0': -38.6000000000003, 'blue_1': -52.80000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 949
(MultiAgentEnvRunner pid=37492) {'red_0': -11.399999999999993, 'red_1': -9.200000000000001, 'blue_0': -57.6000000000006, 'blue_1': -39.20000000000048}
ITERATION 203: reward=-130.50000000000077, metadata={'num_env_steps_sampled_lifetime': 1224000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003305506179622149, 'timers': {'connectors': {'batch_individual_items': 9.753302498943965e-05, 'add_states_from_episodes_to_batch': 6.496437140267572e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3380895497360807e-05, 'numpy_to_tensor': 6.781221742365045e-05, 'agent_to_module_mapping': 8.10401419495929e-06, 'add_observations_from_episodes_to_batch': 3.9112820469527416e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008705049288738377, 'timers': {'connectors': {'get_actions': 0.00044930265619326737, 'un_batch_to_individual_items': 6.608291179009927e-05, 'tensor_to_numpy': 0.00011767722525237783, 'module_to_agent_unmapping': 6.53017624847212e-06, 'normalize_and_clip_actions': 7.183045212483653e-05, 'listify_data_for_vector_env': 2.3798405056895775e-05, 'remove_single_ts_time_rank_from_batch': 2.2991673745734442e-06}}}, 'sample': 92.4355971000623, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.820000000000032, 'blue_0': -47.22000000000042, 'blue_1': -34.42000000000026, 'red_0': -23.040000000000084}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 203.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1224000.0, 'blue_0': 1224000.0, 'blue_1': 1224000.0, 'red_0': 1224000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.820000000000032, 'blue_policy': -34.42000000000026}, 'num_module_steps_sampled_lifetime': {'red_policy': 2448000.0, 'blue_policy': 2448000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033997403565732024, 'episode_return_mean': -130.50000000000077, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -95.10000000000026, 'episode_duration_sec_mean': 18.353219460020775, 'episode_return_min': -204.3000000000012, 'rlmodule_inference_timer': 0.012985567650056864, 'num_episodes_lifetime': 1020.0, 'episode_len_min': 1200, 'time_between_sampling': 305.85979190003127, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.24309823004666, 'throughput_since_last_restore': 15.89911971170638}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 255
(MultiAgentEnvRunner pid=37492) {'red_0': -22.80000000000005, 'red_1': -21.300000000000033, 'blue_0': -46.6000000000004, 'blue_1': -43.50000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1122
(MultiAgentEnvRunner pid=37492) {'red_0': -20.500000000000203, 'red_1': -27.80000000000019, 'blue_0': -51.30000000000038, 'blue_1': -51.70000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 387
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1192
(MultiAgentEnvRunner pid=37492) {'red_0': -24.30000000000001, 'red_1': -35.00000000000016, 'blue_0': -41.80000000000046, 'blue_1': -11.50000000000007}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 601
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 655
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 892
(MultiAgentEnvRunner pid=37492) {'red_0': -50.10000000000035, 'red_1': -27.700000000000166, 'blue_0': -41.70000000000041, 'blue_1': -24.000000000000092}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -0.30000000000000004, 'red_1': -1.2, 'blue_0': -40.2000000000003, 'blue_1': -46.20000000000037}
ITERATION 204: reward=-125.90000000000086, metadata={'num_env_steps_sampled_lifetime': 1230000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003230258882966661, 'timers': {'connectors': {'batch_individual_items': 9.486179877958417e-05, 'add_states_from_episodes_to_batch': 6.381842822066864e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2510807011383235e-05, 'numpy_to_tensor': 6.78753448348525e-05, 'agent_to_module_mapping': 8.030365449603424e-06, 'add_observations_from_episodes_to_batch': 3.7990120937787e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008570712590291619, 'timers': {'connectors': {'get_actions': 0.0004415325105667478, 'un_batch_to_individual_items': 6.402457568424745e-05, 'tensor_to_numpy': 0.00011635112858801568, 'module_to_agent_unmapping': 6.43668713955737e-06, 'normalize_and_clip_actions': 7.071213528911496e-05, 'listify_data_for_vector_env': 2.3725998335858925e-05, 'remove_single_ts_time_rank_from_batch': 2.2908487181283632e-06}}}, 'sample': 92.4632050000364, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.60000000000011, 'blue_0': -44.32000000000039, 'blue_1': -35.38000000000026, 'red_0': -23.600000000000122}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 204.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1230000.0, 'blue_0': 1230000.0, 'blue_1': 1230000.0, 'red_0': 1230000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.60000000000011, 'blue_policy': -35.38000000000026}, 'num_module_steps_sampled_lifetime': {'red_policy': 2460000.0, 'blue_policy': 2460000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003306800240704368, 'episode_return_mean': -125.90000000000086, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -87.90000000000067, 'episode_duration_sec_mean': 18.335228959983215, 'episode_return_min': -151.30000000000115, 'rlmodule_inference_timer': 0.012635201112794401, 'num_episodes_lifetime': 1025.0, 'episode_len_min': 1200, 'time_between_sampling': 301.19104209996294, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.240512787903263, 'throughput_since_last_restore': 15.895768378830859}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1079
(MultiAgentEnvRunner pid=37492) {'red_0': -13.199999999999987, 'red_1': -17.800000000000004, 'blue_0': -27.600000000000193, 'blue_1': -24.800000000000296}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.299999999999994, 'red_1': -27.70000000000013, 'blue_0': -38.20000000000027, 'blue_1': -44.600000000000364}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 738
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 776
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1060
(MultiAgentEnvRunner pid=37492) {'red_0': -17.700000000000017, 'red_1': -21.7000000000001, 'blue_0': -38.80000000000024, 'blue_1': -78.59999999999961}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1138
(MultiAgentEnvRunner pid=37492) {'red_0': -18.8, 'red_1': -14.699999999999958, 'blue_0': -49.700000000000415, 'blue_1': -40.10000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 909
(MultiAgentEnvRunner pid=37492) {'red_0': -15.799999999999978, 'red_1': -20.900000000000034, 'blue_0': -19.800000000000146, 'blue_1': -35.0000000000003}
ITERATION 205: reward=-114.36000000000047, metadata={'num_env_steps_sampled_lifetime': 1236000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003496085256894924, 'timers': {'connectors': {'batch_individual_items': 0.00010133489493289422, 'add_states_from_episodes_to_batch': 6.747287981981725e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3571002348354068e-05, 'numpy_to_tensor': 7.389552302646325e-05, 'agent_to_module_mapping': 8.500074809907643e-06, 'add_observations_from_episodes_to_batch': 4.243862809590373e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009371234936918544, 'timers': {'connectors': {'get_actions': 0.00047729299222805616, 'un_batch_to_individual_items': 7.153384070515097e-05, 'tensor_to_numpy': 0.00012616943775942013, 'module_to_agent_unmapping': 6.732868149216804e-06, 'normalize_and_clip_actions': 7.996633378124101e-05, 'listify_data_for_vector_env': 2.593016398747872e-05, 'remove_single_ts_time_rank_from_batch': 2.512397850858215e-06}}}, 'sample': 90.59276909998152, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -20.560000000000045, 'blue_0': -34.820000000000256, 'blue_1': -44.620000000000175, 'red_0': -14.359999999999996}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 205.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1236000.0, 'blue_0': 1236000.0, 'blue_1': 1236000.0, 'red_0': 1236000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -20.560000000000045, 'blue_policy': -44.620000000000175}, 'num_module_steps_sampled_lifetime': {'red_policy': 2472000.0, 'blue_policy': 2472000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003683174748073992, 'episode_return_mean': -114.36000000000047, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -83.40000000000049, 'episode_duration_sec_mean': 17.992882120027208, 'episode_return_min': -156.79999999999995, 'rlmodule_inference_timer': 0.014087627679932612, 'num_episodes_lifetime': 1030.0, 'episode_len_min': 1200, 'time_between_sampling': 301.22977140010335, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.099802374063666, 'throughput_since_last_restore': 15.891701451707238}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 443
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 796
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 826
(MultiAgentEnvRunner pid=37492) {'red_0': -42.90000000000029, 'red_1': -38.40000000000023, 'blue_0': -23.800000000000235, 'blue_1': -41.00000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1070
(MultiAgentEnvRunner pid=37492) {'red_0': -22.60000000000005, 'red_1': -31.00000000000016, 'blue_0': -37.30000000000029, 'blue_1': -49.90000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 633
(MultiAgentEnvRunner pid=37492) {'red_0': -17.499999999999993, 'red_1': -16.299999999999976, 'blue_0': -25.200000000000184, 'blue_1': -32.300000000000175}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 767
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 830
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 830
(MultiAgentEnvRunner pid=37492) {'red_0': -32.90000000000018, 'red_1': -20.599999999999994, 'blue_0': -36.30000000000038, 'blue_1': -39.200000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 378
(MultiAgentEnvRunner pid=37492) {'red_0': -40.00000000000029, 'red_1': -32.80000000000022, 'blue_0': -23.600000000000037, 'blue_1': -41.500000000000306}
ITERATION 206: reward=-129.0200000000008, metadata={'num_env_steps_sampled_lifetime': 1242000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003464249091946113, 'timers': {'connectors': {'batch_individual_items': 9.676784482198735e-05, 'add_states_from_episodes_to_batch': 6.879399273291927e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3216061210812754e-05, 'numpy_to_tensor': 7.363393105474393e-05, 'agent_to_module_mapping': 8.915825635302067e-06, 'add_observations_from_episodes_to_batch': 4.122251352869044e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009224232813738944, 'timers': {'connectors': {'get_actions': 0.00047256322860450106, 'un_batch_to_individual_items': 6.979424451022195e-05, 'tensor_to_numpy': 0.0001241286453289991, 'module_to_agent_unmapping': 6.9590341670668035e-06, 'normalize_and_clip_actions': 7.604883593032454e-05, 'listify_data_for_vector_env': 2.5898995700777374e-05, 'remove_single_ts_time_rank_from_batch': 2.4783362424968674e-06}}}, 'sample': 91.56718470004853, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.820000000000114, 'blue_0': -29.240000000000226, 'blue_1': -40.78000000000032, 'red_0': -31.18000000000016}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 206.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1242000.0, 'blue_0': 1242000.0, 'blue_1': 1242000.0, 'red_0': 1242000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.820000000000114, 'blue_policy': -40.78000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 2484000.0, 'blue_policy': 2484000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003590739463766713, 'episode_return_mean': -129.0200000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -91.30000000000032, 'episode_duration_sec_mean': 18.15527400004212, 'episode_return_min': -146.10000000000113, 'rlmodule_inference_timer': 0.013924869530802183, 'num_episodes_lifetime': 1035.0, 'episode_len_min': 1200, 'time_between_sampling': 306.7602343999315, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.32975140712453, 'throughput_since_last_restore': 15.888887188674325}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 270
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 495
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 676
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 692
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 692
(MultiAgentEnvRunner pid=37492) {'red_0': -13.099999999999932, 'red_1': -24.10000000000011, 'blue_0': -54.700000000000465, 'blue_1': -53.60000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1000
(MultiAgentEnvRunner pid=37492) {'red_0': -18.600000000000012, 'red_1': -17.4, 'blue_0': -29.100000000000215, 'blue_1': -29.700000000000358}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -19.500000000000007, 'red_1': -26.000000000000103, 'blue_0': -67.80000000000041, 'blue_1': -51.00000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 606
(MultiAgentEnvRunner pid=37492) {'red_0': -21.600000000000037, 'red_1': -34.70000000000021, 'blue_0': -39.80000000000032, 'blue_1': -51.00000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.299999999999986, 'red_1': -16.19999999999996, 'blue_0': -56.40000000000052, 'blue_1': -45.600000000000364}
ITERATION 207: reward=-135.64000000000084, metadata={'num_env_steps_sampled_lifetime': 1248000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033271060773243936, 'timers': {'connectors': {'batch_individual_items': 9.907681423747197e-05, 'add_states_from_episodes_to_batch': 6.562260133725901e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2544246400765001e-05, 'numpy_to_tensor': 7.002126101660426e-05, 'agent_to_module_mapping': 8.208402209303176e-06, 'add_observations_from_episodes_to_batch': 3.889981858391328e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008744674084380336, 'timers': {'connectors': {'get_actions': 0.0004475138837578697, 'un_batch_to_individual_items': 6.743936210832565e-05, 'tensor_to_numpy': 0.00011817447806964363, 'module_to_agent_unmapping': 6.669014032929307e-06, 'normalize_and_clip_actions': 7.242225331202574e-05, 'listify_data_for_vector_env': 2.4177163586056772e-05, 'remove_single_ts_time_rank_from_batch': 2.5084176783012832e-06}}}, 'sample': 92.37088329996914, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -23.680000000000074, 'blue_0': -49.560000000000386, 'blue_1': -46.180000000000405, 'red_0': -16.21999999999999}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 207.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1248000.0, 'blue_0': 1248000.0, 'blue_1': 1248000.0, 'red_0': 1248000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -23.680000000000074, 'blue_policy': -46.180000000000405}, 'num_module_steps_sampled_lifetime': {'red_policy': 2496000.0, 'blue_policy': 2496000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034450995010229334, 'episode_return_mean': -135.64000000000084, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -94.80000000000058, 'episode_duration_sec_mean': 18.336136960005387, 'episode_return_min': -164.30000000000098, 'rlmodule_inference_timer': 0.013088383241731902, 'num_episodes_lifetime': 1040.0, 'episode_len_min': 1200, 'time_between_sampling': 299.8306939000031, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.35708646043517, 'throughput_since_last_restore': 15.886241901195397}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -4.899999999999999, 'red_1': -4.200000000000001, 'blue_0': -49.80000000000043, 'blue_1': -44.50000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 262
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 493
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 513
(MultiAgentEnvRunner pid=37492) {'red_0': -37.90000000000026, 'red_1': -41.7000000000003, 'blue_0': -48.10000000000044, 'blue_1': -40.60000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 418
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 932
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 947
(MultiAgentEnvRunner pid=37492) {'red_0': -33.60000000000012, 'red_1': -20.700000000000035, 'blue_0': -29.500000000000355, 'blue_1': -60.900000000000624}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 862
(MultiAgentEnvRunner pid=37492) {'red_0': -52.30000000000038, 'red_1': -22.799999999999983, 'blue_0': -19.70000000000008, 'blue_1': -59.100000000000605}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 364
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 639
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1042
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1147
(MultiAgentEnvRunner pid=37492) {'red_0': -1.9999999999999645, 'red_1': -20.100000000000072, 'blue_0': -52.10000000000043, 'blue_1': -50.900000000000276}
ITERATION 208: reward=-139.080000000001, metadata={'num_env_steps_sampled_lifetime': 1254000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003345327770339453, 'timers': {'connectors': {'batch_individual_items': 9.819979108292277e-05, 'add_states_from_episodes_to_batch': 6.467138171072166e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2515157376801162e-05, 'numpy_to_tensor': 7.09839418454841e-05, 'agent_to_module_mapping': 9.172412194843882e-06, 'add_observations_from_episodes_to_batch': 3.9151595037082424e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008770281640912979, 'timers': {'connectors': {'get_actions': 0.0004495298495048256, 'un_batch_to_individual_items': 6.611131623507589e-05, 'tensor_to_numpy': 0.00011858269946963008, 'module_to_agent_unmapping': 6.6977599487923955e-06, 'normalize_and_clip_actions': 7.348942342531896e-05, 'listify_data_for_vector_env': 2.4174168115868834e-05, 'remove_single_ts_time_rank_from_batch': 2.337214612295653e-06}}}, 'sample': 92.74472010007594, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.90000000000008, 'blue_0': -39.840000000000344, 'blue_1': -51.20000000000043, 'red_0': -26.140000000000146}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 208.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1254000.0, 'blue_0': 1254000.0, 'blue_1': 1254000.0, 'red_0': 1254000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.90000000000008, 'blue_policy': -51.20000000000043}, 'num_module_steps_sampled_lifetime': {'red_policy': 2508000.0, 'blue_policy': 2508000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00035068153304950963, 'episode_return_mean': -139.080000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -103.40000000000077, 'episode_duration_sec_mean': 18.41746477999259, 'episode_return_min': -168.30000000000135, 'rlmodule_inference_timer': 0.013154095920118507, 'num_episodes_lifetime': 1045.0, 'episode_len_min': 1200, 'time_between_sampling': 298.33907129999716, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.268782637143229, 'throughput_since_last_restore': 15.883168186498747}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 447
(MultiAgentEnvRunner pid=37492) {'red_0': -36.300000000000175, 'red_1': -41.60000000000024, 'blue_0': -44.70000000000034, 'blue_1': -20.099999999999994}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.399999999999961, 'red_1': -6.8999999999999915, 'blue_0': -35.600000000000236, 'blue_1': -50.50000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 353
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1107
(MultiAgentEnvRunner pid=37492) {'red_0': -24.40000000000002, 'red_1': -30.3000000000001, 'blue_0': -28.600000000000243, 'blue_1': -29.700000000000415}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 909
(MultiAgentEnvRunner pid=37492) {'red_0': -26.10000000000013, 'red_1': -53.200000000000465, 'blue_0': -49.70000000000042, 'blue_1': -34.200000000000216}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 676
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 683
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1103
(MultiAgentEnvRunner pid=37492) {'red_0': -33.100000000000215, 'red_1': -20.800000000000022, 'blue_0': -46.70000000000038, 'blue_1': -56.70000000000054}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -7.099999999999992, 'red_1': 0, 'blue_0': -5.999999999999995, 'blue_1': -6.499999999999993}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -6.999999999999992, 'red_1': -1.9000000000000006, 'blue_0': -0.7, 'blue_1': -0.7999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 13:18:16,803	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -8.699999999999985, 'red_1': 0, 'blue_0': -1.5000000000000002, 'blue_1': -1.6000000000000003}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -1.5000000000000002, 'blue_0': -114.49999999999777, 'blue_1': -115.29999999999772}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -7.299999999999991, 'red_1': 0, 'blue_0': -0.6, 'blue_1': -0.4}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 209: reward=-136.9200000000009, metadata={'num_env_steps_sampled_lifetime': 1260000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003326823762942536, 'timers': {'connectors': {'batch_individual_items': 0.00010068817211348028, 'add_states_from_episodes_to_batch': 6.5563418720464255e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2409185719056974e-05, 'numpy_to_tensor': 6.889093032208803e-05, 'agent_to_module_mapping': 8.51151798569272e-06, 'add_observations_from_episodes_to_batch': 3.865303268425849e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000880210278448203, 'timers': {'connectors': {'get_actions': 0.000452294350204971, 'un_batch_to_individual_items': 6.442448972301632e-05, 'tensor_to_numpy': 0.00011865405107223519, 'module_to_agent_unmapping': 6.594596980643384e-06, 'normalize_and_clip_actions': 7.427603821452782e-05, 'listify_data_for_vector_env': 2.5260662277838038e-05, 'remove_single_ts_time_rank_from_batch': 2.3231955287006685e-06}}}, 'sample': 92.56191309995484, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.560000000000162, 'blue_0': -41.06000000000033, 'blue_1': -38.24000000000033, 'red_0': -27.060000000000105}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 209.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1260000.0, 'blue_0': 1260000.0, 'blue_1': 1260000.0, 'red_0': 1260000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.560000000000162, 'blue_policy': -38.24000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2520000.0, 'blue_policy': 2520000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003481393600028985, 'episode_return_mean': -136.9200000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.40000000000063, 'episode_duration_sec_mean': 18.362240200024097, 'episode_return_min': -163.20000000000124, 'rlmodule_inference_timer': 0.013101696251152718, 'num_episodes_lifetime': 1050.0, 'episode_len_min': 1200, 'time_between_sampling': 300.2067607999779, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.947243636538904, 'throughput_since_last_restore': 15.866035381613647}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 985
(MultiAgentEnvRunner pid=37492) {'red_0': -25.200000000000045, 'red_1': -27.30000000000005, 'blue_0': -29.000000000000355, 'blue_1': -50.800000000000495}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 230
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 804
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1035
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1126
(MultiAgentEnvRunner pid=37492) {'red_0': -48.60000000000044, 'red_1': -36.4000000000003, 'blue_0': -27.000000000000096, 'blue_1': -42.900000000000276}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 751
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 875
(MultiAgentEnvRunner pid=37492) {'red_0': -18.200000000000017, 'red_1': -35.90000000000022, 'blue_0': -47.10000000000042, 'blue_1': -38.50000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 369
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 543
(MultiAgentEnvRunner pid=37492) {'red_0': -35.70000000000027, 'red_1': -34.50000000000025, 'blue_0': -36.300000000000225, 'blue_1': -31.00000000000016}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 511
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 548
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 598
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 686
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 686
(MultiAgentEnvRunner pid=37492) {'red_0': -25.00000000000012, 'red_1': -35.400000000000205, 'blue_0': -27.20000000000008, 'blue_1': -36.50000000000023}
ITERATION 210: reward=-137.7000000000009, metadata={'num_env_steps_sampled_lifetime': 1266000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003273214854087993, 'timers': {'connectors': {'batch_individual_items': 9.885204280419409e-05, 'add_states_from_episodes_to_batch': 6.347223411176874e-06, 'add_time_dim_to_batch_and_zero_pad': 1.217309798902092e-05, 'numpy_to_tensor': 6.737685750030306e-05, 'agent_to_module_mapping': 8.015878144179274e-06, 'add_observations_from_episodes_to_batch': 3.856482706417225e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008643385035619941, 'timers': {'connectors': {'get_actions': 0.0004420178242495528, 'un_batch_to_individual_items': 6.542787138085289e-05, 'tensor_to_numpy': 0.00011769285200298341, 'module_to_agent_unmapping': 6.6394849022331985e-06, 'normalize_and_clip_actions': 7.154453352666807e-05, 'listify_data_for_vector_env': 2.3897044481353607e-05, 'remove_single_ts_time_rank_from_batch': 2.3167984888342152e-06}}}, 'sample': 91.33750110003166, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.900000000000205, 'blue_0': -33.320000000000235, 'blue_1': -39.94000000000029, 'red_0': -30.54000000000018}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 210.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1266000.0, 'blue_0': 1266000.0, 'blue_1': 1266000.0, 'red_0': 1266000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.900000000000205, 'blue_policy': -39.94000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 2532000.0, 'blue_policy': 2532000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034073533961354437, 'episode_return_mean': -137.7000000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -124.10000000000063, 'episode_duration_sec_mean': 18.13945104002487, 'episode_return_min': -154.9000000000011, 'rlmodule_inference_timer': 0.012997338093351934, 'num_episodes_lifetime': 1055.0, 'episode_len_min': 1200, 'time_between_sampling': 370.86209559999406, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.34551378391681, 'throughput_since_last_restore': 15.863484679468684}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 390.9567839000374, 'restore_env_runners': 9.999959729611874e-06, 'training_step': 390.9565451000817, 'env_runner_sampling_timer': 91.4717674999265, 'learner_update_timer': 299.4280099000316, 'synch_weights': 0.014398899977095425, 'synch_env_connectors': 0.002284299931488931, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1266000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003273214854087993, 'timers': {'connectors': {'batch_individual_items': 9.885204280419409e-05, 'add_states_from_episodes_to_batch': 6.347223411176874e-06, 'add_time_dim_to_batch_and_zero_pad': 1.217309798902092e-05, 'numpy_to_tensor': 6.737685750030306e-05, 'agent_to_module_mapping': 8.015878144179274e-06, 'add_observations_from_episodes_to_batch': 3.856482706417225e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008643385035619941, 'timers': {'connectors': {'get_actions': 0.0004420178242495528, 'un_batch_to_individual_items': 6.542787138085289e-05, 'tensor_to_numpy': 0.00011769285200298341, 'module_to_agent_unmapping': 6.6394849022331985e-06, 'normalize_and_clip_actions': 7.154453352666807e-05, 'listify_data_for_vector_env': 2.3897044481353607e-05, 'remove_single_ts_time_rank_from_batch': 2.3167984888342152e-06}}}, 'sample': 91.33750110003166, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.900000000000205, 'blue_0': -33.320000000000235, 'blue_1': -39.94000000000029, 'red_0': -30.54000000000018}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 210.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1266000.0, 'blue_0': 1266000.0, 'blue_1': 1266000.0, 'red_0': 1266000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.900000000000205, 'blue_policy': -39.94000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 2532000.0, 'blue_policy': 2532000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034073533961354437, 'episode_return_mean': -137.7000000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -124.10000000000063, 'episode_duration_sec_mean': 18.13945104002487, 'episode_return_min': -154.9000000000011, 'rlmodule_inference_timer': 0.012997338093351934, 'num_episodes_lifetime': 1055.0, 'episode_len_min': 1200, 'time_between_sampling': 370.86209559999406, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.34551378391681, 'throughput_since_last_restore': 15.863484679468684}}, 'learners': {'red_policy': {'policy_loss': -0.15726064145565033, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.013379983603954315, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 211.0, 'num_module_steps_trained_lifetime': 76027520.0, 'curr_entropy_coeff': 0.03101, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00017339999999999999, 'vf_explained_var': -0.473352313041687, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.830743789672852, 'entropy': 1.120788812637329, 'vf_loss_unclipped': 681.9722900390625, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 921.5531212507468, 'throughput_since_last_restore': 952.6551289583357}}, 'blue_policy': {'weights_seq_no': 211.0, 'num_module_steps_trained_lifetime': 76027520.0, 'curr_entropy_coeff': 0.03101, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00017339999999999999, 'vf_explained_var': 0.16480910778045654, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 5.7434306144714355, 'total_loss': 1.3554095029830933, 'entropy': 1.6776916980743408, 'policy_loss': -0.22894416749477386, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.6130441427230835, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.015463877469301224, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 921.5517766064228, 'throughput_since_last_restore': 952.6551300744593}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.6999346017837524e-06, 'batch_individual_items': 1.132688399986364, 'add_time_dim_to_batch_and_zero_pad': 2.0200037397444248e-05, 'numpy_to_tensor': 0.15310849994421005, 'add_observations_from_episodes_to_batch': 0.0002655000425875187, 'agent_to_module_mapping': 0.020772599964402616, 'add_one_ts_to_episodes_and_truncate': 0.1867993000196293, 'add_columns_from_episodes_to_train_batch': 0.4866971999872476, 'general_advantage_estimation': 12.869147700024769}}, 'connector_pipeline_timer': 14.84994490002282}, 'num_module_steps_trained_lifetime': 152055040.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 3563790000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 43197.697931915, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 43197.69599847809, 'throughput_since_last_restore': 44655.709264542544}, 'num_module_steps_trained_throughput': 1843.101678493508, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1843.101654452443, 'throughput_since_last_restore': 1905.3102614930433}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1266000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 211, 'trial_id': 'default', 'date': '2026-01-26_13-28-58', 'timestamp': 1769430538, 'time_this_iter_s': 390.97221279144287, 'time_total_s': 79780.66629338264, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 79780.66629338264, 'iterations_since_restore': 211, 'perf': {'cpu_util_percent': 14.291577060931901, 'ram_util_percent': 85.30465949820788}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 754
(MultiAgentEnvRunner pid=37492) {'red_0': -15.29999999999998, 'red_1': -20.30000000000004, 'blue_0': -49.30000000000048, 'blue_1': -30.500000000000306}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 996
(MultiAgentEnvRunner pid=37492) {'red_0': -36.200000000000166, 'red_1': -53.600000000000406, 'blue_0': -18.90000000000015, 'blue_1': -33.100000000000264}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 390
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 540
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 957
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 1123
(MultiAgentEnvRunner pid=37492) {'red_0': -40.00000000000026, 'red_1': -30.599999999999998, 'blue_0': -22.90000000000013, 'blue_1': -16.40000000000008}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 389
(MultiAgentEnvRunner pid=37492) {'red_0': -10.09999999999998, 'red_1': -17.49999999999997, 'blue_0': -48.90000000000041, 'blue_1': -44.400000000000354}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 434
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 832
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 832
(MultiAgentEnvRunner pid=37492) {'red_0': -30.30000000000011, 'red_1': -12.999999999999986, 'blue_0': -37.100000000000215, 'blue_1': -25.300000000000075}
ITERATION 211: reward=-118.74000000000066, metadata={'num_env_steps_sampled_lifetime': 1272000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033988614162629366, 'timers': {'connectors': {'batch_individual_items': 0.00010068077544227036, 'add_states_from_episodes_to_batch': 6.977821135092318e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2787498978631302e-05, 'numpy_to_tensor': 7.148884787202562e-05, 'agent_to_module_mapping': 8.335760914906292e-06, 'add_observations_from_episodes_to_batch': 3.963845665197018e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008930086933236625, 'timers': {'connectors': {'get_actions': 0.00046292386370257405, 'un_batch_to_individual_items': 6.657864169057406e-05, 'tensor_to_numpy': 0.0001193937353217105, 'module_to_agent_unmapping': 7.502923494211932e-06, 'normalize_and_clip_actions': 7.362131113575142e-05, 'listify_data_for_vector_env': 2.423036693377053e-05, 'remove_single_ts_time_rank_from_batch': 2.3815706415072257e-06}}}, 'sample': 92.88810360000934, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.000000000000078, 'blue_0': -35.42000000000028, 'blue_1': -29.940000000000214, 'red_0': -26.3800000000001}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 211.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1272000.0, 'blue_0': 1272000.0, 'blue_1': 1272000.0, 'red_0': 1272000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.000000000000078, 'blue_policy': -29.940000000000214}, 'num_module_steps_sampled_lifetime': {'red_policy': 2544000.0, 'blue_policy': 2544000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003446175255696439, 'episode_return_mean': -118.74000000000066, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -105.70000000000039, 'episode_duration_sec_mean': 18.429703200003132, 'episode_return_min': -141.80000000000098, 'rlmodule_inference_timer': 0.013424991616781195, 'num_episodes_lifetime': 1060.0, 'episode_len_min': 1200, 'time_between_sampling': 299.7479379000142, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.28961687461749, 'throughput_since_last_restore': 15.860676170626485}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 553
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 863
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 994
(MultiAgentEnvRunner pid=37492) {'red_0': -36.700000000000244, 'red_1': -49.20000000000038, 'blue_0': -23.500000000000306, 'blue_1': -43.3000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1003
(MultiAgentEnvRunner pid=37492) {'red_0': -44.30000000000028, 'red_1': -55.300000000000416, 'blue_0': -17.70000000000013, 'blue_1': -35.70000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 733
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1170
(MultiAgentEnvRunner pid=37492) {'red_0': -21.900000000000016, 'red_1': -23.100000000000072, 'blue_0': -48.600000000000335, 'blue_1': -49.30000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 902
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 995
(MultiAgentEnvRunner pid=37492) {'red_0': -26.200000000000124, 'red_1': -33.000000000000206, 'blue_0': -28.90000000000021, 'blue_1': -20.200000000000166}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1165
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1190
(MultiAgentEnvRunner pid=37492) {'red_0': -43.700000000000266, 'red_1': -54.90000000000041, 'blue_0': -51.500000000000504, 'blue_1': -18.000000000000227}
ITERATION 212: reward=-145.00000000000108, metadata={'num_env_steps_sampled_lifetime': 1278000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000332845962163768, 'timers': {'connectors': {'batch_individual_items': 0.00010205885083070422, 'add_states_from_episodes_to_batch': 6.333641787378611e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2325362179188783e-05, 'numpy_to_tensor': 6.990717977157472e-05, 'agent_to_module_mapping': 7.982197659567247e-06, 'add_observations_from_episodes_to_batch': 3.8966982738837355e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008756962043914054, 'timers': {'connectors': {'get_actions': 0.0004503552208412074, 'un_batch_to_individual_items': 6.493397325680869e-05, 'tensor_to_numpy': 0.00011579761174338913, 'module_to_agent_unmapping': 6.4980798230062975e-06, 'normalize_and_clip_actions': 7.500488130701257e-05, 'listify_data_for_vector_env': 2.4402302892799656e-05, 'remove_single_ts_time_rank_from_batch': 2.305811481625302e-06}}}, 'sample': 92.51288430008572, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -43.10000000000029, 'blue_0': -34.0400000000003, 'blue_1': -33.30000000000028, 'red_0': -34.56000000000019}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 212.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1278000.0, 'blue_0': 1278000.0, 'blue_1': 1278000.0, 'red_0': 1278000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -43.10000000000029, 'blue_policy': -33.30000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 2556000.0, 'blue_policy': 2556000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003428636017384275, 'episode_return_mean': -145.00000000000108, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.3000000000007, 'episode_duration_sec_mean': 18.374473980022593, 'episode_return_min': -168.10000000000142, 'rlmodule_inference_timer': 0.013131733042353762, 'num_episodes_lifetime': 1065.0, 'episode_len_min': 1200, 'time_between_sampling': 299.44635970005766, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.380317031639594, 'throughput_since_last_restore': 15.858350442709408}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -3.3000000000000016, 'red_1': -11.199999999999976, 'blue_0': -70.50000000000024, 'blue_1': -39.20000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 357
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 647
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 865
(MultiAgentEnvRunner pid=37492) {'red_0': -46.20000000000031, 'red_1': -36.50000000000019, 'blue_0': -57.700000000000585, 'blue_1': -26.1000000000002}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 318
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 617
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 785
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 982
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 982
(MultiAgentEnvRunner pid=37492) {'red_0': -34.00000000000024, 'red_1': -35.70000000000019, 'blue_0': -37.80000000000024, 'blue_1': -37.300000000000296}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 878
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 899
(MultiAgentEnvRunner pid=37492) {'red_0': -22.099999999999977, 'red_1': -17.099999999999994, 'blue_0': -36.00000000000031, 'blue_1': -23.800000000000203}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 576
(MultiAgentEnvRunner pid=37492) {'red_0': -39.40000000000028, 'red_1': -59.90000000000048, 'blue_0': -42.20000000000028, 'blue_1': -28.700000000000138}
ITERATION 213: reward=-140.9400000000009, metadata={'num_env_steps_sampled_lifetime': 1284000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032312074312192216, 'timers': {'connectors': {'batch_individual_items': 9.642584227953096e-05, 'add_states_from_episodes_to_batch': 6.473553661570522e-06, 'add_time_dim_to_batch_and_zero_pad': 1.224800029359164e-05, 'numpy_to_tensor': 6.576861909994237e-05, 'agent_to_module_mapping': 8.000581995865117e-06, 'add_observations_from_episodes_to_batch': 3.904229846735953e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008555990142463736, 'timers': {'connectors': {'get_actions': 0.0004380046006722055, 'un_batch_to_individual_items': 6.37358997488117e-05, 'tensor_to_numpy': 0.00011444438021484887, 'module_to_agent_unmapping': 6.263317300871364e-06, 'normalize_and_clip_actions': 7.187737936811226e-05, 'listify_data_for_vector_env': 2.555040010300978e-05, 'remove_single_ts_time_rank_from_batch': 2.2572827807522476e-06}}}, 'sample': 90.64719499996863, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.08000000000017, 'blue_0': -48.84000000000033, 'blue_1': -31.020000000000227, 'red_0': -29.000000000000163}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 213.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1284000.0, 'blue_0': 1284000.0, 'blue_1': 1284000.0, 'red_0': 1284000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.08000000000017, 'blue_policy': -31.020000000000227}, 'num_module_steps_sampled_lifetime': {'red_policy': 2568000.0, 'blue_policy': 2568000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003362399138334082, 'episode_return_mean': -140.9400000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -99.00000000000048, 'episode_duration_sec_mean': 18.008193860016764, 'episode_return_min': -170.20000000000118, 'rlmodule_inference_timer': 0.012864202554748076, 'num_episodes_lifetime': 1070.0, 'episode_len_min': 1200, 'time_between_sampling': 297.60225879994687, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.575557251216276, 'throughput_since_last_restore': 15.857004480664571}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 489
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 523
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 561
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 561
(MultiAgentEnvRunner pid=37492) {'red_0': -18.6, 'red_1': -42.800000000000345, 'blue_0': -37.500000000000284, 'blue_1': -26.700000000000085}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 639
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 952
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 994
(MultiAgentEnvRunner pid=37492) {'red_0': -31.7000000000002, 'red_1': -32.90000000000018, 'blue_0': -61.70000000000057, 'blue_1': -36.4000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1156
(MultiAgentEnvRunner pid=37492) {'red_0': -21.800000000000033, 'red_1': -34.700000000000244, 'blue_0': -40.90000000000031, 'blue_1': -43.50000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 548
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 915
(MultiAgentEnvRunner pid=37492) {'red_0': -54.00000000000046, 'red_1': -41.400000000000304, 'blue_0': -42.40000000000038, 'blue_1': -48.100000000000406}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 678
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1037
(MultiAgentEnvRunner pid=37492) {'red_0': -39.10000000000024, 'red_1': -42.700000000000365, 'blue_0': -45.30000000000041, 'blue_1': -32.400000000000276}
ITERATION 214: reward=-154.92000000000115, metadata={'num_env_steps_sampled_lifetime': 1290000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003253389769697986, 'timers': {'connectors': {'batch_individual_items': 9.814016084425867e-05, 'add_states_from_episodes_to_batch': 6.321395294676974e-06, 'add_time_dim_to_batch_and_zero_pad': 1.231183995717868e-05, 'numpy_to_tensor': 6.717817582528144e-05, 'agent_to_module_mapping': 7.92535870526966e-06, 'add_observations_from_episodes_to_batch': 3.802764570459367e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008602217885293154, 'timers': {'connectors': {'get_actions': 0.00044627279539025886, 'un_batch_to_individual_items': 6.371421313383516e-05, 'tensor_to_numpy': 0.00011793936514195866, 'module_to_agent_unmapping': 6.18849436177105e-06, 'normalize_and_clip_actions': 7.024798278534306e-05, 'listify_data_for_vector_env': 2.346930264556136e-05, 'remove_single_ts_time_rank_from_batch': 2.3604948344187525e-06}}}, 'sample': 90.73485290003009, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -38.90000000000029, 'blue_0': -45.560000000000386, 'blue_1': -37.42000000000028, 'red_0': -33.040000000000184}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 214.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1290000.0, 'blue_0': 1290000.0, 'blue_1': 1290000.0, 'red_0': 1290000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -38.90000000000029, 'blue_policy': -37.42000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 2580000.0, 'blue_policy': 2580000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033598201132967995, 'episode_return_mean': -154.92000000000115, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -125.60000000000072, 'episode_duration_sec_mean': 18.019547719997355, 'episode_return_min': -185.90000000000157, 'rlmodule_inference_timer': 0.012787354652561493, 'num_episodes_lifetime': 1075.0, 'episode_len_min': 1200, 'time_between_sampling': 294.576478999923, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.428205385582821, 'throughput_since_last_restore': 15.85495443619158}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 985
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1056
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1192
(MultiAgentEnvRunner pid=37492) {'red_0': -28.600000000000065, 'red_1': -17.899999999999977, 'blue_0': -32.60000000000039, 'blue_1': -49.000000000000504}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.399999999999993, 'red_1': -16.199999999999964, 'blue_0': -40.900000000000304, 'blue_1': -53.400000000000475}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 234
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 269
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 833
(MultiAgentEnvRunner pid=37492) {'red_0': -11.999999999999979, 'red_1': -8.299999999999986, 'blue_0': -48.80000000000045, 'blue_1': -48.60000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 447
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 459
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 459
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 870
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 872
(MultiAgentEnvRunner pid=37492) {'red_0': -26.400000000000123, 'red_1': -44.60000000000036, 'blue_0': -27.400000000000087, 'blue_1': -48.60000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 857
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 924
(MultiAgentEnvRunner pid=37492) {'red_0': -30.600000000000172, 'red_1': -21.200000000000042, 'blue_0': -40.70000000000037, 'blue_1': -20.700000000000188}
ITERATION 215: reward=-124.58000000000087, metadata={'num_env_steps_sampled_lifetime': 1296000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032728798167649595, 'timers': {'connectors': {'batch_individual_items': 9.970966252783048e-05, 'add_states_from_episodes_to_batch': 6.462462415506446e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2290405032762576e-05, 'numpy_to_tensor': 6.692897036910113e-05, 'agent_to_module_mapping': 8.184350843492005e-06, 'add_observations_from_episodes_to_batch': 3.867622930571274e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008537230679694009, 'timers': {'connectors': {'get_actions': 0.0004415533504157405, 'un_batch_to_individual_items': 6.351619005900551e-05, 'tensor_to_numpy': 0.00011657112452049765, 'module_to_agent_unmapping': 6.265963551234942e-06, 'normalize_and_clip_actions': 7.056103905275913e-05, 'listify_data_for_vector_env': 2.3304838018177844e-05, 'remove_single_ts_time_rank_from_batch': 2.2291998347956814e-06}}}, 'sample': 91.79542520001996, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.640000000000065, 'blue_0': -38.080000000000325, 'blue_1': -44.060000000000414, 'red_0': -20.800000000000068}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 215.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1296000.0, 'blue_0': 1296000.0, 'blue_1': 1296000.0, 'red_0': 1296000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.640000000000065, 'blue_policy': -44.060000000000414}, 'num_module_steps_sampled_lifetime': {'red_policy': 2592000.0, 'blue_policy': 2592000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003410074236267325, 'episode_return_mean': -124.58000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -113.20000000000077, 'episode_duration_sec_mean': 18.215008560009302, 'episode_return_min': -147.00000000000105, 'rlmodule_inference_timer': 0.012846068496742047, 'num_episodes_lifetime': 1080.0, 'episode_len_min': 1200, 'time_between_sampling': 298.162629700033, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.381327696808839, 'throughput_since_last_restore': 15.852694167616214}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -30.50000000000016, 'red_1': -11.299999999999976, 'blue_0': -40.90000000000031, 'blue_1': -39.000000000000284}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 425
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 459
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 848
(MultiAgentEnvRunner pid=37492) {'red_0': -28.600000000000062, 'red_1': -59.00000000000047, 'blue_0': -38.80000000000035, 'blue_1': -27.300000000000328}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 937
(MultiAgentEnvRunner pid=37492) {'red_0': -21.400000000000013, 'red_1': -29.400000000000077, 'blue_0': -19.80000000000016, 'blue_1': -52.30000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -31.20000000000016, 'red_1': -35.80000000000024, 'blue_0': -35.600000000000236, 'blue_1': -37.20000000000025}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 544
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1014
(MultiAgentEnvRunner pid=37492) {'red_0': -37.10000000000026, 'red_1': -46.70000000000031, 'blue_0': -28.60000000000019, 'blue_1': -33.80000000000023}
ITERATION 216: reward=-136.8600000000009, metadata={'num_env_steps_sampled_lifetime': 1302000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003170267272248077, 'timers': {'connectors': {'batch_individual_items': 9.393665872357568e-05, 'add_states_from_episodes_to_batch': 6.358723508369378e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1940939100736064e-05, 'numpy_to_tensor': 6.69180462286484e-05, 'agent_to_module_mapping': 7.984417513430016e-06, 'add_observations_from_episodes_to_batch': 3.7436805766685497e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000857967870748259, 'timers': {'connectors': {'get_actions': 0.00044228060398369864, 'un_batch_to_individual_items': 6.397423558792052e-05, 'tensor_to_numpy': 0.00011623960664107893, 'module_to_agent_unmapping': 6.281711827882583e-06, 'normalize_and_clip_actions': 7.099450233459329e-05, 'listify_data_for_vector_env': 2.346720875027756e-05, 'remove_single_ts_time_rank_from_batch': 2.352217876321595e-06}}}, 'sample': 91.93085810006596, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -36.44000000000021, 'blue_0': -32.74000000000025, 'blue_1': -37.92000000000032, 'red_0': -29.760000000000126}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 216.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1302000.0, 'blue_0': 1302000.0, 'blue_1': 1302000.0, 'red_0': 1302000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -36.44000000000021, 'blue_policy': -37.92000000000032}, 'num_module_steps_sampled_lifetime': {'red_policy': 2604000.0, 'blue_policy': 2604000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003437621850290086, 'episode_return_mean': -136.8600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -121.70000000000073, 'episode_duration_sec_mean': 18.2585985600017, 'episode_return_min': -153.7000000000012, 'rlmodule_inference_timer': 0.013097124299658069, 'num_episodes_lifetime': 1085.0, 'episode_len_min': 1200, 'time_between_sampling': 298.2923932999838, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.356184330699964, 'throughput_since_last_restore': 15.850332027916346}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 540
(MultiAgentEnvRunner pid=37492) {'red_0': -33.700000000000216, 'red_1': -21.20000000000005, 'blue_0': -28.7000000000001, 'blue_1': -42.100000000000385}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 180
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 227
(MultiAgentEnvRunner pid=37492) {'red_0': -53.7000000000004, 'red_1': -71.80000000000018, 'blue_0': -17.199999999999953, 'blue_1': -43.300000000000324}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 612
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 944
(MultiAgentEnvRunner pid=37492) {'red_0': -15.499999999999961, 'red_1': -22.500000000000107, 'blue_0': -39.300000000000274, 'blue_1': -50.30000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 326
(MultiAgentEnvRunner pid=37492) {'red_0': -37.70000000000027, 'red_1': -56.400000000000475, 'blue_0': -40.70000000000028, 'blue_1': -25.500000000000064}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 415
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 703
(MultiAgentEnvRunner pid=37492) {'red_0': -35.100000000000186, 'red_1': -56.000000000000426, 'blue_0': -37.300000000000324, 'blue_1': -37.00000000000033}
ITERATION 217: reward=-153.00000000000097, metadata={'num_env_steps_sampled_lifetime': 1308000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003286246690534189, 'timers': {'connectors': {'batch_individual_items': 9.943012698770213e-05, 'add_states_from_episodes_to_batch': 6.431940805480799e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2210458356056094e-05, 'numpy_to_tensor': 6.748670834219584e-05, 'agent_to_module_mapping': 8.006233184993735e-06, 'add_observations_from_episodes_to_batch': 3.927546185790361e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008699128449632298, 'timers': {'connectors': {'get_actions': 0.00044837158517077537, 'un_batch_to_individual_items': 6.496958723028779e-05, 'tensor_to_numpy': 0.00011844181926332997, 'module_to_agent_unmapping': 6.452885877092222e-06, 'normalize_and_clip_actions': 7.250724286931928e-05, 'listify_data_for_vector_env': 2.394102890545083e-05, 'remove_single_ts_time_rank_from_batch': 2.303086999935921e-06}}}, 'sample': 91.437292000046, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -45.580000000000254, 'blue_0': -32.640000000000185, 'blue_1': -39.640000000000306, 'red_0': -35.14000000000021}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 217.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1308000.0, 'blue_0': 1308000.0, 'blue_1': 1308000.0, 'red_0': 1308000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -45.580000000000254, 'blue_policy': -39.640000000000306}, 'num_module_steps_sampled_lifetime': {'red_policy': 2616000.0, 'blue_policy': 2616000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003473973220207078, 'episode_return_mean': -153.00000000000097, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -125.70000000000076, 'episode_duration_sec_mean': 18.161564660002476, 'episode_return_min': -186.00000000000085, 'rlmodule_inference_timer': 0.013020136808271587, 'num_episodes_lifetime': 1090.0, 'episode_len_min': 1200, 'time_between_sampling': 298.7942723999731, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.357364646738516, 'throughput_since_last_restore': 15.847997960129682}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -28.60000000000014, 'red_1': -17.099999999999973, 'blue_0': -56.20000000000053, 'blue_1': -45.80000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -30.60000000000017, 'red_1': -13.999999999999966, 'blue_0': -47.20000000000039, 'blue_1': -38.50000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 308
(MultiAgentEnvRunner pid=37492) {'red_0': -50.2000000000004, 'red_1': -24.600000000000097, 'blue_0': -28.100000000000104, 'blue_1': -34.80000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 388
(MultiAgentEnvRunner pid=37492) {'red_0': -30.20000000000017, 'red_1': -33.30000000000022, 'blue_0': -32.80000000000014, 'blue_1': -35.100000000000215}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 951
(MultiAgentEnvRunner pid=37492) {'red_0': -11.799999999999974, 'red_1': -17.49999999999997, 'blue_0': -47.40000000000039, 'blue_1': -41.000000000000306}
ITERATION 218: reward=-132.9600000000008, metadata={'num_env_steps_sampled_lifetime': 1314000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00040814637406852693, 'timers': {'connectors': {'batch_individual_items': 0.00012171153060779018, 'add_states_from_episodes_to_batch': 7.382430345548004e-06, 'add_time_dim_to_batch_and_zero_pad': 1.627906365931999e-05, 'numpy_to_tensor': 9.052967089288035e-05, 'agent_to_module_mapping': 9.76374909654056e-06, 'add_observations_from_episodes_to_batch': 4.60043920884462e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0010412697406376064, 'timers': {'connectors': {'get_actions': 0.0005318430701182637, 'un_batch_to_individual_items': 7.705314323313551e-05, 'tensor_to_numpy': 0.00014270072403428665, 'module_to_agent_unmapping': 7.467592153737947e-06, 'normalize_and_clip_actions': 8.594235246243997e-05, 'listify_data_for_vector_env': 2.8027732087232064e-05, 'remove_single_ts_time_rank_from_batch': 2.8889771552264835e-06}}}, 'sample': 90.72684299992397, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.300000000000047, 'blue_0': -42.3400000000003, 'blue_1': -39.040000000000276, 'red_0': -30.28000000000017}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 218.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1314000.0, 'blue_0': 1314000.0, 'blue_1': 1314000.0, 'red_0': 1314000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.300000000000047, 'blue_policy': -39.040000000000276}, 'num_module_steps_sampled_lifetime': {'red_policy': 2628000.0, 'blue_policy': 2628000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0004016757708536328, 'episode_return_mean': -132.9600000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -117.70000000000064, 'episode_duration_sec_mean': 17.97330967998132, 'episode_return_min': -147.700000000001, 'rlmodule_inference_timer': 0.01581592441920721, 'num_episodes_lifetime': 1095.0, 'episode_len_min': 1200, 'time_between_sampling': 299.2638881999301, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.395975474138794, 'throughput_since_last_restore': 15.850416176043758}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 743
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 985
(MultiAgentEnvRunner pid=37492) {'red_0': -18.400000000000038, 'red_1': -12.29999999999999, 'blue_0': -36.90000000000031, 'blue_1': -32.00000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.699999999999989, 'red_1': -10.399999999999979, 'blue_0': -40.5000000000003, 'blue_1': -49.20000000000041}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 875
(MultiAgentEnvRunner pid=37492) {'red_0': -37.00000000000028, 'red_1': -13.799999999999967, 'blue_0': -46.500000000000384, 'blue_1': -45.600000000000364}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 483
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 553
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 567
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 708
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 995
(MultiAgentEnvRunner pid=37492) {'red_0': -28.000000000000018, 'red_1': -35.600000000000094, 'blue_0': -53.10000000000052, 'blue_1': -36.40000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1157
(MultiAgentEnvRunner pid=37492) {'red_0': -12.19999999999999, 'red_1': -12.49999999999999, 'blue_0': -39.60000000000036, 'blue_1': -29.20000000000035}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.79999999999747, 'red_1': 0, 'blue_0': 0, 'blue_1': 0}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.30000000000000004, 'blue_0': -0.9999999999999999, 'blue_1': -0.7999999999999999}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 14:24:05,133	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.2, 'blue_0': -0.1, 'blue_1': 0}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -0.7999999999999999, 'red_1': 0, 'blue_0': -1.4000000000000001, 'blue_1': -1.2}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -115.29999999999772, 'red_1': 0, 'blue_0': -0.7999999999999999, 'blue_1': -0.9999999999999999}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 219: reward=-119.38000000000082, metadata={'num_env_steps_sampled_lifetime': 1320000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032571367311864263, 'timers': {'connectors': {'batch_individual_items': 9.626593066127702e-05, 'add_states_from_episodes_to_batch': 6.467899918105009e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2686314561184375e-05, 'numpy_to_tensor': 6.897945684146275e-05, 'agent_to_module_mapping': 8.095106373908644e-06, 'add_observations_from_episodes_to_batch': 3.8228580707784126e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008705022275726716, 'timers': {'connectors': {'get_actions': 0.0004503001354397124, 'un_batch_to_individual_items': 6.509678581963513e-05, 'tensor_to_numpy': 0.00011830642751930822, 'module_to_agent_unmapping': 6.396113004764379e-06, 'normalize_and_clip_actions': 7.145117305037917e-05, 'listify_data_for_vector_env': 2.3725904412603132e-05, 'remove_single_ts_time_rank_from_batch': 2.2366251852305493e-06}}}, 'sample': 92.26146379997954, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -16.92, 'blue_0': -43.32000000000038, 'blue_1': -38.48000000000038, 'red_0': -20.66000000000006}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 219.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1320000.0, 'blue_0': 1320000.0, 'blue_1': 1320000.0, 'red_0': 1320000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -16.92, 'blue_policy': -38.48000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 2640000.0, 'blue_policy': 2640000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034004744000726446, 'episode_return_mean': -119.38000000000082, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -93.50000000000068, 'episode_duration_sec_mean': 18.30061842005234, 'episode_return_min': -153.10000000000107, 'rlmodule_inference_timer': 0.012789040651278684, 'num_episodes_lifetime': 1100.0, 'episode_len_min': 1200, 'time_between_sampling': 275.22089679993223, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.857628351087675, 'throughput_since_last_restore': 15.833663293683653}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 519
(MultiAgentEnvRunner pid=37492) {'red_0': -27.400000000000123, 'red_1': -29.600000000000183, 'blue_0': -54.80000000000048, 'blue_1': -46.900000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 456
(MultiAgentEnvRunner pid=37492) {'red_0': -18.900000000000006, 'red_1': -16.199999999999978, 'blue_0': -37.200000000000244, 'blue_1': -38.700000000000244}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 879
(MultiAgentEnvRunner pid=37492) {'red_0': -24.90000000000007, 'red_1': -23.90000000000007, 'blue_0': -56.20000000000053, 'blue_1': -40.60000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 625
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1191
(MultiAgentEnvRunner pid=37492) {'red_0': -35.900000000000176, 'red_1': -48.40000000000034, 'blue_0': -33.50000000000021, 'blue_1': -20.100000000000094}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 876
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 979
(MultiAgentEnvRunner pid=37492) {'red_0': -46.6000000000003, 'red_1': -50.80000000000035, 'blue_0': -39.00000000000035, 'blue_1': -25.20000000000033}
ITERATION 220: reward=-142.96000000000095, metadata={'num_env_steps_sampled_lifetime': 1326000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034307826105834164, 'timers': {'connectors': {'batch_individual_items': 9.96539306493628e-05, 'add_states_from_episodes_to_batch': 6.5978393023788645e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4714151972674743e-05, 'numpy_to_tensor': 7.244520557889735e-05, 'agent_to_module_mapping': 9.190562656667828e-06, 'add_observations_from_episodes_to_batch': 4.099175525947856e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000904906621071702, 'timers': {'connectors': {'get_actions': 0.00046568242768486715, 'un_batch_to_individual_items': 6.644756775623368e-05, 'tensor_to_numpy': 0.00012328156869237725, 'module_to_agent_unmapping': 6.544684340107365e-06, 'normalize_and_clip_actions': 7.514423396788829e-05, 'listify_data_for_vector_env': 2.445117654525407e-05, 'remove_single_ts_time_rank_from_batch': 2.4272548623126047e-06}}}, 'sample': 92.37880879989825, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.780000000000186, 'blue_0': -44.14000000000036, 'blue_1': -34.300000000000274, 'red_0': -30.740000000000133}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 220.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1326000.0, 'blue_0': 1326000.0, 'blue_1': 1326000.0, 'red_0': 1326000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.780000000000186, 'blue_policy': -34.300000000000274}, 'num_module_steps_sampled_lifetime': {'red_policy': 2652000.0, 'blue_policy': 2652000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036320058436315096, 'episode_return_mean': -142.96000000000095, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.00000000000047, 'episode_duration_sec_mean': 18.343296500016002, 'episode_return_min': -161.60000000000133, 'rlmodule_inference_timer': 0.013464409611749833, 'num_episodes_lifetime': 1105.0, 'episode_len_min': 1200, 'time_between_sampling': 374.3815663000569, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.00049278104034, 'throughput_since_last_restore': 15.83440967424606}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 374.95001809997484, 'restore_env_runners': 1.1599971912801266e-05, 'training_step': 374.9492018999299, 'env_runner_sampling_timer': 92.54105810006149, 'learner_update_timer': 282.2976622999413, 'synch_weights': 0.051177100045606494, 'synch_env_connectors': 0.0025337999686598778, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1326000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034307826105834164, 'timers': {'connectors': {'batch_individual_items': 9.96539306493628e-05, 'add_states_from_episodes_to_batch': 6.5978393023788645e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4714151972674743e-05, 'numpy_to_tensor': 7.244520557889735e-05, 'agent_to_module_mapping': 9.190562656667828e-06, 'add_observations_from_episodes_to_batch': 4.099175525947856e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000904906621071702, 'timers': {'connectors': {'get_actions': 0.00046568242768486715, 'un_batch_to_individual_items': 6.644756775623368e-05, 'tensor_to_numpy': 0.00012328156869237725, 'module_to_agent_unmapping': 6.544684340107365e-06, 'normalize_and_clip_actions': 7.514423396788829e-05, 'listify_data_for_vector_env': 2.445117654525407e-05, 'remove_single_ts_time_rank_from_batch': 2.4272548623126047e-06}}}, 'sample': 92.37880879989825, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.780000000000186, 'blue_0': -44.14000000000036, 'blue_1': -34.300000000000274, 'red_0': -30.740000000000133}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 220.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1326000.0, 'blue_0': 1326000.0, 'blue_1': 1326000.0, 'red_0': 1326000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.780000000000186, 'blue_policy': -34.300000000000274}, 'num_module_steps_sampled_lifetime': {'red_policy': 2652000.0, 'blue_policy': 2652000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036320058436315096, 'episode_return_mean': -142.96000000000095, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -111.00000000000047, 'episode_duration_sec_mean': 18.343296500016002, 'episode_return_min': -161.60000000000133, 'rlmodule_inference_timer': 0.013464409611749833, 'num_episodes_lifetime': 1105.0, 'episode_len_min': 1200, 'time_between_sampling': 374.3815663000569, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.00049278104034, 'throughput_since_last_restore': 15.83440967424606}}, 'learners': {'red_policy': {'policy_loss': -0.2169693261384964, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.008980159647762775, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 221.0, 'num_module_steps_trained_lifetime': 79630720.0, 'curr_entropy_coeff': 0.03011, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00016739999999999998, 'vf_explained_var': -0.1766585111618042, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.770297050476074, 'entropy': 0.9297020435333252, 'vf_loss_unclipped': 688.0391845703125, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 960.8866440704185, 'throughput_since_last_restore': 950.9090751456962}}, 'blue_policy': {'weights_seq_no': 221.0, 'num_module_steps_trained_lifetime': 79630720.0, 'curr_entropy_coeff': 0.03011, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00016739999999999998, 'vf_explained_var': 0.6219750642776489, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 1.623821496963501, 'total_loss': 0.7719283103942871, 'entropy': 1.6577953100204468, 'policy_loss': -0.18061447143554688, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.9771891832351685, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.016736797988414764, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 960.8852016674243, 'throughput_since_last_restore': 950.9090763209659}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 1.4899997040629387e-05, 'batch_individual_items': 0.782383400015533, 'add_time_dim_to_batch_and_zero_pad': 5.579995922744274e-05, 'numpy_to_tensor': 0.14775549992918968, 'add_observations_from_episodes_to_batch': 0.00041779992170631886, 'agent_to_module_mapping': 0.02373499993700534, 'add_one_ts_to_episodes_and_truncate': 0.20760209998115897, 'add_columns_from_episodes_to_train_batch': 0.518979600048624, 'general_advantage_estimation': 15.360875300015323}}, 'connector_pipeline_timer': 17.042334800004028}, 'num_module_steps_trained_lifetime': 159261440.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 3732690000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 45041.48432710653, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 45041.457373414705, 'throughput_since_last_restore': 44573.86301333147}, 'num_module_steps_trained_throughput': 1921.7688043705266, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1921.768736722161, 'throughput_since_last_restore': 1901.818154449691}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1326000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 221, 'trial_id': 'default', 'date': '2026-01-26_14-34-34', 'timestamp': 1769434474, 'time_this_iter_s': 374.9726104736328, 'time_total_s': 83716.0759241581, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 83716.0759241581, 'iterations_since_restore': 221, 'perf': {'cpu_util_percent': 23.208971962616822, 'ram_util_percent': 87.24822429906543}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 226
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 402
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 731
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 818
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 818
(MultiAgentEnvRunner pid=37492) {'red_0': -37.30000000000021, 'red_1': -38.20000000000028, 'blue_0': -23.60000000000006, 'blue_1': -39.80000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 863
(MultiAgentEnvRunner pid=37492) {'red_0': -11.29999999999998, 'red_1': -4.899999999999999, 'blue_0': -35.10000000000023, 'blue_1': -39.400000000000276}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 399
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1032
(MultiAgentEnvRunner pid=37492) {'red_0': -13.999999999999975, 'red_1': -18.1, 'blue_0': -54.20000000000053, 'blue_1': -31.100000000000126}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 686
(MultiAgentEnvRunner pid=37492) {'red_0': -49.80000000000038, 'red_1': -28.40000000000015, 'blue_0': -22.300000000000086, 'blue_1': -40.300000000000374}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 586
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 758
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1082
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1082
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1134
(MultiAgentEnvRunner pid=37492) {'red_0': -33.100000000000186, 'red_1': -29.10000000000012, 'blue_0': -25.200000000000035, 'blue_1': -22.000000000000046}
ITERATION 221: reward=-119.44000000000065, metadata={'num_env_steps_sampled_lifetime': 1332000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00037464700979106404, 'timers': {'connectors': {'batch_individual_items': 0.00010582163992811751, 'add_states_from_episodes_to_batch': 7.943046504950818e-06, 'add_time_dim_to_batch_and_zero_pad': 1.442730566408531e-05, 'numpy_to_tensor': 8.029375015784857e-05, 'agent_to_module_mapping': 9.13392219418989e-06, 'add_observations_from_episodes_to_batch': 4.3682968319519545e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009704300403489105, 'timers': {'connectors': {'get_actions': 0.0004973987717030502, 'un_batch_to_individual_items': 7.2021065345863e-05, 'tensor_to_numpy': 0.0001290801959426966, 'module_to_agent_unmapping': 7.265534578677679e-06, 'normalize_and_clip_actions': 8.014635948387743e-05, 'listify_data_for_vector_env': 2.654057747147415e-05, 'remove_single_ts_time_rank_from_batch': 2.6362854178808274e-06}}}, 'sample': 125.4398362999782, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -23.740000000000112, 'blue_0': -32.08000000000019, 'blue_1': -34.520000000000216, 'red_0': -29.100000000000147}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 221.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1332000.0, 'blue_0': 1332000.0, 'blue_1': 1332000.0, 'red_0': 1332000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -23.740000000000112, 'blue_policy': -34.520000000000216}, 'num_module_steps_sampled_lifetime': {'red_policy': 2664000.0, 'blue_policy': 2664000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00037159807314772155, 'episode_return_mean': -119.44000000000065, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -90.70000000000049, 'episode_duration_sec_mean': 24.911074939998798, 'episode_return_min': -140.80000000000098, 'rlmodule_inference_timer': 0.014438095582445023, 'num_episodes_lifetime': 1110.0, 'episode_len_min': 1200, 'time_between_sampling': 282.7848611000227, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.463033762186978, 'throughput_since_last_restore': 15.827648697162608}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 250
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 347
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 439
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1034
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1035
(MultiAgentEnvRunner pid=37492) {'red_0': -23.00000000000003, 'red_1': -18.600000000000012, 'blue_0': -24.200000000000102, 'blue_1': -19.500000000000178}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 461
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 529
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 529
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 857
(MultiAgentEnvRunner pid=37492) {'red_0': -31.00000000000016, 'red_1': -22.700000000000063, 'blue_0': -32.80000000000016, 'blue_1': -34.800000000000196}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 210
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 832
(MultiAgentEnvRunner pid=37492) {'red_0': -56.00000000000055, 'red_1': -35.90000000000027, 'blue_0': -27.300000000000104, 'blue_1': -56.500000000000476}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 396
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 415
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1043
(MultiAgentEnvRunner pid=37492) {'red_0': -22.30000000000004, 'red_1': -24.200000000000024, 'blue_0': -31.50000000000024, 'blue_1': -26.40000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 278
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 433
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 782
(MultiAgentEnvRunner pid=37492) {'red_0': -36.200000000000195, 'red_1': -63.50000000000061, 'blue_0': -28.70000000000012, 'blue_1': -49.000000000000455}
ITERATION 222: reward=-132.8200000000009, metadata={'num_env_steps_sampled_lifetime': 1338000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0004785950318695165, 'timers': {'connectors': {'batch_individual_items': 0.0001354808442572495, 'add_states_from_episodes_to_batch': 9.953498399180216e-06, 'add_time_dim_to_batch_and_zero_pad': 1.7987131863188865e-05, 'numpy_to_tensor': 9.980409539830598e-05, 'agent_to_module_mapping': 1.1607770094293987e-05, 'add_observations_from_episodes_to_batch': 5.373235456101205e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.001200649550665036, 'timers': {'connectors': {'get_actions': 0.0005985189169691635, 'un_batch_to_individual_items': 9.419977381757982e-05, 'tensor_to_numpy': 0.00015961605279781848, 'module_to_agent_unmapping': 9.079163021543051e-06, 'normalize_and_clip_actions': 0.00010516803008060064, 'listify_data_for_vector_env': 3.5699641970977786e-05, 'remove_single_ts_time_rank_from_batch': 3.4163145038603458e-06}}}, 'sample': 127.24088359996676, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -32.9800000000002, 'blue_0': -28.900000000000148, 'blue_1': -37.24000000000033, 'red_0': -33.700000000000195}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 222.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1338000.0, 'blue_0': 1338000.0, 'blue_1': 1338000.0, 'red_0': 1338000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -32.9800000000002, 'blue_policy': -37.24000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2676000.0, 'blue_policy': 2676000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0004732176672539159, 'episode_return_mean': -132.8200000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -85.30000000000032, 'episode_duration_sec_mean': 25.12258536000736, 'episode_return_min': -177.4000000000014, 'rlmodule_inference_timer': 0.019516291275283128, 'num_episodes_lifetime': 1115.0, 'episode_len_min': 1200, 'time_between_sampling': 289.2487855000654, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.501593464472728, 'throughput_since_last_restore': 15.815429837837241}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 341
(MultiAgentEnvRunner pid=37492) {'red_0': -29.400000000000166, 'red_1': -36.80000000000026, 'blue_0': -26.700000000000085, 'blue_1': -50.40000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.099999999999991, 'red_1': -15.299999999999962, 'blue_0': -52.20000000000046, 'blue_1': -51.00000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 629
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 749
(MultiAgentEnvRunner pid=37492) {'red_0': -31.100000000000176, 'red_1': -34.40000000000022, 'blue_0': -49.700000000000486, 'blue_1': -43.300000000000345}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 706
(MultiAgentEnvRunner pid=37492) {'red_0': -8.899999999999997, 'red_1': -22.90000000000007, 'blue_0': -24.00000000000007, 'blue_1': -47.80000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 369
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 529
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 773
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1049
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1112
(MultiAgentEnvRunner pid=37492) {'red_0': -15.799999999999962, 'red_1': -29.100000000000126, 'blue_0': -24.200000000000237, 'blue_1': -38.20000000000032}
ITERATION 223: reward=-127.66000000000085, metadata={'num_env_steps_sampled_lifetime': 1344000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0004036443800885237, 'timers': {'connectors': {'batch_individual_items': 0.00011421854081540987, 'add_states_from_episodes_to_batch': 8.036248405839244e-06, 'add_time_dim_to_batch_and_zero_pad': 1.5878055949245097e-05, 'numpy_to_tensor': 8.68065193902499e-05, 'agent_to_module_mapping': 9.874503867991417e-06, 'add_observations_from_episodes_to_batch': 4.713760692079037e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0010439330052935247, 'timers': {'connectors': {'get_actions': 0.0005308297346515632, 'un_batch_to_individual_items': 7.959347919526166e-05, 'tensor_to_numpy': 0.00014241269895520891, 'module_to_agent_unmapping': 8.06106811851995e-06, 'normalize_and_clip_actions': 8.646230133265304e-05, 'listify_data_for_vector_env': 3.0201585874150712e-05, 'remove_single_ts_time_rank_from_batch': 2.8663917177491275e-06}}}, 'sample': 110.8102213999955, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.70000000000013, 'blue_0': -35.36000000000027, 'blue_1': -46.14000000000041, 'red_0': -18.460000000000058}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 223.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1344000.0, 'blue_0': 1344000.0, 'blue_1': 1344000.0, 'red_0': 1344000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.70000000000013, 'blue_policy': -46.14000000000041}, 'num_module_steps_sampled_lifetime': {'red_policy': 2688000.0, 'blue_policy': 2688000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00041709853907121147, 'episode_return_mean': -127.66000000000085, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -103.60000000000062, 'episode_duration_sec_mean': 22.007638099999166, 'episode_return_min': -158.50000000000122, 'rlmodule_inference_timer': 0.01666155529156418, 'num_episodes_lifetime': 1120.0, 'episode_len_min': 1200, 'time_between_sampling': 317.1433436999796, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.36542452490312, 'throughput_since_last_restore': 15.813361809975692}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 413
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 699
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 744
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 895
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 915
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 915
(MultiAgentEnvRunner pid=37492) {'red_0': -43.100000000000286, 'red_1': -29.800000000000125, 'blue_0': -40.80000000000036, 'blue_1': -34.100000000000236}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 526
(MultiAgentEnvRunner pid=37492) {'red_0': -20.400000000000038, 'red_1': -14.099999999999982, 'blue_0': -35.1000000000002, 'blue_1': -21.40000000000002}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 547
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 810
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 810
(MultiAgentEnvRunner pid=37492) {'red_0': -11.699999999999955, 'red_1': -10.899999999999967, 'blue_0': -56.400000000000496, 'blue_1': -54.60000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 272
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 663
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_1 at STEP 964
(MultiAgentEnvRunner pid=37492) {'red_0': -16.799999999999983, 'red_1': -24.899999999999967, 'blue_0': -52.9000000000005, 'blue_1': -25.900000000000293}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 525
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1037
(MultiAgentEnvRunner pid=37492) {'red_0': -47.70000000000038, 'red_1': -34.90000000000023, 'blue_0': -19.79999999999998, 'blue_1': -39.1000000000003}
ITERATION 224: reward=-126.88000000000075, metadata={'num_env_steps_sampled_lifetime': 1350000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003376534056683637, 'timers': {'connectors': {'batch_individual_items': 9.928873330690726e-05, 'add_states_from_episodes_to_batch': 6.482817693930391e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3934018322727684e-05, 'numpy_to_tensor': 6.942671114958392e-05, 'agent_to_module_mapping': 8.423948311683187e-06, 'add_observations_from_episodes_to_batch': 4.056948376241822e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008870382440387207, 'timers': {'connectors': {'get_actions': 0.00045731320962855837, 'un_batch_to_individual_items': 6.529417190610497e-05, 'tensor_to_numpy': 0.00012179921797368412, 'module_to_agent_unmapping': 6.4481826001689246e-06, 'normalize_and_clip_actions': 7.485661583819961e-05, 'listify_data_for_vector_env': 2.41530207599405e-05, 'remove_single_ts_time_rank_from_batch': 2.3394794561432017e-06}}}, 'sample': 91.85175719996914, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.92000000000005, 'blue_0': -41.00000000000031, 'blue_1': -35.02000000000026, 'red_0': -27.94000000000013}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 224.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1350000.0, 'blue_0': 1350000.0, 'blue_1': 1350000.0, 'red_0': 1350000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.92000000000005, 'blue_policy': -35.02000000000026}, 'num_module_steps_sampled_lifetime': {'red_policy': 2700000.0, 'blue_policy': 2700000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034990467924868914, 'episode_return_mean': -126.88000000000075, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -91.00000000000024, 'episode_duration_sec_mean': 18.13351270002313, 'episode_return_min': -147.80000000000098, 'rlmodule_inference_timer': 0.013029759358976172, 'num_episodes_lifetime': 1125.0, 'episode_len_min': 1200, 'time_between_sampling': 279.6849861999508, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.96158140467885, 'throughput_since_last_restore': 15.818120537646513}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 516
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 563
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 989
(MultiAgentEnvRunner pid=37492) {'red_0': -13.599999999999978, 'red_1': -32.30000000000014, 'blue_0': -28.800000000000093, 'blue_1': -58.70000000000058}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 666
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1056
(MultiAgentEnvRunner pid=37492) {'red_0': -26.20000000000009, 'red_1': -11.899999999999974, 'blue_0': -49.900000000000425, 'blue_1': -40.50000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -22.200000000000045, 'red_1': -11.999999999999973, 'blue_0': -34.70000000000022, 'blue_1': -55.10000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 744
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 883
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1128
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1128
(MultiAgentEnvRunner pid=37492) {'red_0': -27.000000000000213, 'red_1': -15.199999999999992, 'blue_0': -41.200000000000344, 'blue_1': -31.200000000000262}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1045
(MultiAgentEnvRunner pid=37492) {'red_0': -11.999999999999991, 'red_1': -11.299999999999994, 'blue_0': -26.40000000000031, 'blue_1': -30.700000000000237}
ITERATION 225: reward=-116.18000000000072, metadata={'num_env_steps_sampled_lifetime': 1356000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.000326067511479131, 'timers': {'connectors': {'batch_individual_items': 9.432315474881126e-05, 'add_states_from_episodes_to_batch': 6.497861612964057e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2585022785606525e-05, 'numpy_to_tensor': 6.771123858699818e-05, 'agent_to_module_mapping': 8.236519245652238e-06, 'add_observations_from_episodes_to_batch': 3.919649645926956e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008787003951090223, 'timers': {'connectors': {'get_actions': 0.0004521228449778086, 'un_batch_to_individual_items': 6.663963063552307e-05, 'tensor_to_numpy': 0.00011967678687499764, 'module_to_agent_unmapping': 6.521015941318839e-06, 'normalize_and_clip_actions': 7.240010984575414e-05, 'listify_data_for_vector_env': 2.426526744130254e-05, 'remove_single_ts_time_rank_from_batch': 2.456582686548952e-06}}}, 'sample': 88.71262210002169, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -16.540000000000013, 'blue_0': -36.20000000000029, 'blue_1': -43.24000000000038, 'red_0': -20.200000000000063}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 225.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1356000.0, 'blue_0': 1356000.0, 'blue_1': 1356000.0, 'red_0': 1356000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -16.540000000000013, 'blue_policy': -43.24000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 2712000.0, 'blue_policy': 2712000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003428238818305564, 'episode_return_mean': -116.18000000000072, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -80.40000000000053, 'episode_duration_sec_mean': 17.611823740019464, 'episode_return_min': -133.4000000000008, 'rlmodule_inference_timer': 0.01290700953829095, 'num_episodes_lifetime': 1130.0, 'episode_len_min': 1200, 'time_between_sampling': 261.8841725999955, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.02619363488595, 'throughput_since_last_restore': 15.823087861128519}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 523
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 539
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 936
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1029
(MultiAgentEnvRunner pid=37492) {'red_0': -58.70000000000046, 'red_1': -51.70000000000045, 'blue_0': -16.19999999999992, 'blue_1': -46.80000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 316
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1086
(MultiAgentEnvRunner pid=37492) {'red_0': -85.09999999999965, 'red_1': -64.20000000000053, 'blue_0': -30.20000000000021, 'blue_1': -9.399999999999999}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 794
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1030
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1030
(MultiAgentEnvRunner pid=37492) {'red_0': -11.899999999999977, 'red_1': -26.300000000000065, 'blue_0': -55.20000000000053, 'blue_1': -28.100000000000172}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 408
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 640
(MultiAgentEnvRunner pid=37492) {'red_0': -27.30000000000014, 'red_1': -52.800000000000395, 'blue_0': -33.700000000000195, 'blue_1': -24.30000000000005}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 413
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 672
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 672
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 804
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1140
(MultiAgentEnvRunner pid=37492) {'red_0': -33.10000000000013, 'red_1': -33.7000000000002, 'blue_0': -18.50000000000019, 'blue_1': -32.40000000000021}
ITERATION 226: reward=-147.92000000000078, metadata={'num_env_steps_sampled_lifetime': 1362000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003506855398468426, 'timers': {'connectors': {'batch_individual_items': 0.00011091806041505443, 'add_states_from_episodes_to_batch': 6.706636849402048e-06, 'add_time_dim_to_batch_and_zero_pad': 1.304062913791394e-05, 'numpy_to_tensor': 7.101271157162658e-05, 'agent_to_module_mapping': 8.220970161577633e-06, 'add_observations_from_episodes_to_batch': 4.007955103249447e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008910792618832208, 'timers': {'connectors': {'get_actions': 0.0004604009069496284, 'un_batch_to_individual_items': 6.684231395384135e-05, 'tensor_to_numpy': 0.0001189012111212609, 'module_to_agent_unmapping': 6.668891503703181e-06, 'normalize_and_clip_actions': 7.367054488078817e-05, 'listify_data_for_vector_env': 2.475767041970137e-05, 'remove_single_ts_time_rank_from_batch': 2.4288628814373825e-06}}}, 'sample': 92.31410329998471, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -45.74000000000033, 'blue_0': -30.76000000000021, 'blue_1': -28.200000000000177, 'red_0': -43.22000000000007}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 226.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1362000.0, 'blue_0': 1362000.0, 'blue_1': 1362000.0, 'red_0': 1362000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -45.74000000000033, 'blue_policy': -28.200000000000177}, 'num_module_steps_sampled_lifetime': {'red_policy': 2724000.0, 'blue_policy': 2724000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000353839571650191, 'episode_return_mean': -147.92000000000078, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -117.70000000000073, 'episode_duration_sec_mean': 18.32996445999015, 'episode_return_min': -188.9000000000004, 'rlmodule_inference_timer': 0.013256891054295363, 'num_episodes_lifetime': 1135.0, 'episode_len_min': 1200, 'time_between_sampling': 263.6910160999978, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.48024155220589, 'throughput_since_last_restore': 15.821543721586902}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 335
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 593
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 660
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 679
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 999
(MultiAgentEnvRunner pid=37492) {'red_0': -38.700000000000266, 'red_1': -55.50000000000049, 'blue_0': -24.700000000000085, 'blue_1': -29.100000000000108}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 329
(MultiAgentEnvRunner pid=37492) {'red_0': -11.099999999999993, 'red_1': -19.200000000000006, 'blue_0': -39.40000000000026, 'blue_1': -54.500000000000554}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 518
(MultiAgentEnvRunner pid=37492) {'red_0': -33.9000000000002, 'red_1': -37.10000000000025, 'blue_0': -41.40000000000031, 'blue_1': -39.40000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1199
(MultiAgentEnvRunner pid=37492) {'red_0': -26.000000000000032, 'red_1': -64.10000000000055, 'blue_0': -39.200000000000344, 'blue_1': -19.300000000000217}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 379
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 646
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 646
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 649
(MultiAgentEnvRunner pid=37492) {'red_0': -25.200000000000063, 'red_1': -42.300000000000246, 'blue_0': -32.80000000000015, 'blue_1': -34.90000000000017}
ITERATION 227: reward=-141.5600000000009, metadata={'num_env_steps_sampled_lifetime': 1368000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003478021755612113, 'timers': {'connectors': {'batch_individual_items': 0.00010225038993907793, 'add_states_from_episodes_to_batch': 6.535482451529196e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2852268204787548e-05, 'numpy_to_tensor': 7.337863430892751e-05, 'agent_to_module_mapping': 8.930585122681756e-06, 'add_observations_from_episodes_to_batch': 4.185914766198816e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000911925337362799, 'timers': {'connectors': {'get_actions': 0.00047177929036578537, 'un_batch_to_individual_items': 6.743314089692864e-05, 'tensor_to_numpy': 0.0001230857715826183, 'module_to_agent_unmapping': 6.666519666627725e-06, 'normalize_and_clip_actions': 7.524659660267212e-05, 'listify_data_for_vector_env': 2.5452308630854332e-05, 'remove_single_ts_time_rank_from_batch': 2.3994264343648305e-06}}}, 'sample': 96.00550239998847, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -43.640000000000306, 'blue_0': -35.500000000000234, 'blue_1': -35.44000000000027, 'red_0': -26.98000000000011}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 227.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1368000.0, 'blue_0': 1368000.0, 'blue_1': 1368000.0, 'red_0': 1368000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -43.640000000000306, 'blue_policy': -35.44000000000027}, 'num_module_steps_sampled_lifetime': {'red_policy': 2736000.0, 'blue_policy': 2736000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036962415580169466, 'episode_return_mean': -141.5600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -124.20000000000081, 'episode_duration_sec_mean': 19.016994239995256, 'episode_return_min': -151.80000000000106, 'rlmodule_inference_timer': 0.013577845010120618, 'num_episodes_lifetime': 1140.0, 'episode_len_min': 1200, 'time_between_sampling': 295.2749634999782, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.043470601758608, 'throughput_since_last_restore': 15.817954999726702}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 671
(MultiAgentEnvRunner pid=37492) {'red_0': -29.400000000000176, 'red_1': -26.1000000000001, 'blue_0': -54.30000000000048, 'blue_1': -50.50000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -18.099999999999987, 'red_1': -19.400000000000006, 'blue_0': -46.500000000000384, 'blue_1': -36.200000000000244}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 339
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1006
(MultiAgentEnvRunner pid=37492) {'red_0': -8.499999999999979, 'red_1': -7.799999999999989, 'blue_0': -41.100000000000314, 'blue_1': -50.90000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 793
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1171
(MultiAgentEnvRunner pid=37492) {'red_0': -40.600000000000165, 'red_1': -37.600000000000165, 'blue_0': -15.40000000000022, 'blue_1': -26.00000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 492
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 595
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1103
(MultiAgentEnvRunner pid=37492) {'red_0': -22.00000000000002, 'red_1': -62.80000000000054, 'blue_0': -44.800000000000445, 'blue_1': -32.800000000000395}
ITERATION 228: reward=-134.160000000001, metadata={'num_env_steps_sampled_lifetime': 1374000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00030373550419645544, 'timers': {'connectors': {'batch_individual_items': 9.378473418990545e-05, 'add_states_from_episodes_to_batch': 5.8575879448227484e-06, 'add_time_dim_to_batch_and_zero_pad': 1.1304562588814644e-05, 'numpy_to_tensor': 6.236291078050663e-05, 'agent_to_module_mapping': 7.396858596750911e-06, 'add_observations_from_episodes_to_batch': 3.559430712175795e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008049070211486869, 'timers': {'connectors': {'get_actions': 0.0004170764466745002, 'un_batch_to_individual_items': 6.017933605533165e-05, 'tensor_to_numpy': 0.0001100005781298001, 'module_to_agent_unmapping': 6.08433856498501e-06, 'normalize_and_clip_actions': 6.563630592749955e-05, 'listify_data_for_vector_env': 2.1631409681959916e-05, 'remove_single_ts_time_rank_from_batch': 2.1551094904381666e-06}}}, 'sample': 86.3557099000318, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.74000000000016, 'blue_0': -40.42000000000037, 'blue_1': -39.28000000000037, 'red_0': -23.720000000000066}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 228.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1374000.0, 'blue_0': 1374000.0, 'blue_1': 1374000.0, 'red_0': 1374000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.74000000000016, 'blue_policy': -39.28000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 2748000.0, 'blue_policy': 2748000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003169818267313956, 'episode_return_mean': -134.160000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.30000000000072, 'episode_duration_sec_mean': 17.15835078000091, 'episode_return_min': -162.4000000000014, 'rlmodule_inference_timer': 0.011750374550405247, 'num_episodes_lifetime': 1145.0, 'episode_len_min': 1200, 'time_between_sampling': 302.8406610999955, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.592691057310493, 'throughput_since_last_restore': 15.821180461354919}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 261
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 417
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 1146
(MultiAgentEnvRunner pid=37492) {'red_0': -26.90000000000016, 'red_1': -54.300000000000495, 'blue_0': -34.6000000000002, 'blue_1': -27.600000000000087}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 363
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 792
(MultiAgentEnvRunner pid=37492) {'red_0': -38.20000000000022, 'red_1': -35.20000000000017, 'blue_0': -16.899999999999956, 'blue_1': -30.800000000000118}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 568
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 674
(MultiAgentEnvRunner pid=37492) {'red_0': -19.1, 'red_1': -11.79999999999997, 'blue_0': -37.90000000000027, 'blue_1': -38.400000000000254}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 531
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1077
(MultiAgentEnvRunner pid=37492) {'red_0': -34.40000000000022, 'red_1': -27.800000000000175, 'blue_0': -39.90000000000026, 'blue_1': -24.400000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 444
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 730
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 771
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 985
(MultiAgentEnvRunner pid=37492) {'red_0': -45.30000000000028, 'red_1': -40.20000000000024, 'blue_0': -28.9000000000003, 'blue_1': -44.50000000000041}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -6.5999999999999925, 'red_1': 0, 'blue_0': -111.09999999999796, 'blue_1': -111.19999999999796}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -118.89999999999752, 'blue_1': -118.89999999999752}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) TAGGED blue_0 -> red_1 at STEP 75
(MultiAgentEnvRunner pid=41856) PICKED UP by blue_0 at STEP 82
2026-01-26 15:29:52,267	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -9.799999999999983, 'red_1': -29.100000000000154, 'blue_0': -71.1000000000002, 'blue_1': -87.69999999999929}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) TAGGED blue_1 -> red_1 at STEP 138
(MultiAgentEnvRunner pid=41856) TAGGED red_0 -> blue_1 at STEP 143
(MultiAgentEnvRunner pid=41856) PICKED UP by blue_0 at STEP 150
(MultiAgentEnvRunner pid=41856) {'red_0': -4.199999999999999, 'red_1': -112.09999999999792, 'blue_0': 4.700000000000022, 'blue_1': -2.7000000000000037}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -0.2, 'blue_0': -0.7999999999999999, 'blue_1': -0.4}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 229: reward=-131.42000000000075, metadata={'num_env_steps_sampled_lifetime': 1380000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033467562272463964, 'timers': {'connectors': {'batch_individual_items': 0.00010568936949466758, 'add_states_from_episodes_to_batch': 6.359050564182355e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2189814862581279e-05, 'numpy_to_tensor': 6.790531397953268e-05, 'agent_to_module_mapping': 7.97891133914418e-06, 'add_observations_from_episodes_to_batch': 3.9020662018461754e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008637652083724457, 'timers': {'connectors': {'get_actions': 0.000444689198602373, 'un_batch_to_individual_items': 6.413411294559094e-05, 'tensor_to_numpy': 0.0001180688011757995, 'module_to_agent_unmapping': 6.4643441851493046e-06, 'normalize_and_clip_actions': 7.149129773862115e-05, 'listify_data_for_vector_env': 2.360024515645043e-05, 'remove_single_ts_time_rank_from_batch': 2.3171500270615985e-06}}}, 'sample': 93.72179559990764, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.86000000000021, 'blue_0': -31.640000000000196, 'blue_1': -33.14000000000018, 'red_0': -32.78000000000018}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 229.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1380000.0, 'blue_0': 1380000.0, 'blue_1': 1380000.0, 'red_0': 1380000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.86000000000021, 'blue_policy': -33.14000000000018}, 'num_module_steps_sampled_lifetime': {'red_policy': 2760000.0, 'blue_policy': 2760000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003456632223883589, 'episode_return_mean': -131.42000000000075, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -107.2000000000005, 'episode_duration_sec_mean': 18.616684239986352, 'episode_return_min': -158.90000000000123, 'rlmodule_inference_timer': 0.012729125610515606, 'num_episodes_lifetime': 1150.0, 'episode_len_min': 1200, 'time_between_sampling': 275.2623695000075, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 13.269885102799593, 'throughput_since_last_restore': 15.807965701554945}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 392
(MultiAgentEnvRunner pid=37492) {'red_0': -22.00000000000003, 'red_1': -14.699999999999982, 'blue_0': -37.00000000000024, 'blue_1': -25.900000000000077}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 505
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1076
(MultiAgentEnvRunner pid=37492) {'red_0': -28.80000000000015, 'red_1': -39.300000000000274, 'blue_0': -24.400000000000045, 'blue_1': -33.3000000000002}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1022
(MultiAgentEnvRunner pid=37492) {'red_0': -71.90000000000008, 'red_1': -32.000000000000114, 'blue_0': -22.80000000000026, 'blue_1': -48.50000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 573
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 593
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 595
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1069
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1152
(MultiAgentEnvRunner pid=37492) {'red_0': -44.80000000000035, 'red_1': -42.50000000000025, 'blue_0': -17.70000000000006, 'blue_1': -42.10000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 255
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 467
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 697
(MultiAgentEnvRunner pid=37492) {'red_0': -64.40000000000057, 'red_1': -47.60000000000032, 'blue_0': -25.40000000000009, 'blue_1': -48.700000000000514}
ITERATION 230: reward=-146.7600000000009, metadata={'num_env_steps_sampled_lifetime': 1386000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032302583587222555, 'timers': {'connectors': {'batch_individual_items': 9.614156332146327e-05, 'add_states_from_episodes_to_batch': 6.714003674007128e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3358973240750008e-05, 'numpy_to_tensor': 6.690189824681542e-05, 'agent_to_module_mapping': 7.911946564697396e-06, 'add_observations_from_episodes_to_batch': 3.798231026284104e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000866333087330693, 'timers': {'connectors': {'get_actions': 0.00044866365957454344, 'un_batch_to_individual_items': 6.433904639769102e-05, 'tensor_to_numpy': 0.00011872214306862943, 'module_to_agent_unmapping': 6.25438528614672e-06, 'normalize_and_clip_actions': 7.093645732888701e-05, 'listify_data_for_vector_env': 2.332244802678547e-05, 'remove_single_ts_time_rank_from_batch': 2.3483757557232684e-06}}}, 'sample': 91.42628200002946, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.220000000000184, 'blue_0': -25.460000000000143, 'blue_1': -39.70000000000033, 'red_0': -46.38000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 230.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1386000.0, 'blue_0': 1386000.0, 'blue_1': 1386000.0, 'red_0': 1386000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.220000000000184, 'blue_policy': -39.70000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2772000.0, 'blue_policy': 2772000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033318450668941394, 'episode_return_mean': -146.7600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -99.60000000000032, 'episode_duration_sec_mean': 18.15033827996813, 'episode_return_min': -186.1000000000015, 'rlmodule_inference_timer': 0.012558090821223458, 'num_episodes_lifetime': 1155.0, 'episode_len_min': 1200, 'time_between_sampling': 358.4319609000813, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.374158163967373, 'throughput_since_last_restore': 15.810331734466898}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 366.38751939998474, 'restore_env_runners': 9.70007386058569e-06, 'training_step': 366.38729399989825, 'env_runner_sampling_timer': 91.57040289998986, 'learner_update_timer': 274.7531855000416, 'synch_weights': 0.013441999908536673, 'synch_env_connectors': 0.0029847000259906054, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1386000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032302583587222555, 'timers': {'connectors': {'batch_individual_items': 9.614156332146327e-05, 'add_states_from_episodes_to_batch': 6.714003674007128e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3358973240750008e-05, 'numpy_to_tensor': 6.690189824681542e-05, 'agent_to_module_mapping': 7.911946564697396e-06, 'add_observations_from_episodes_to_batch': 3.798231026284104e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000866333087330693, 'timers': {'connectors': {'get_actions': 0.00044866365957454344, 'un_batch_to_individual_items': 6.433904639769102e-05, 'tensor_to_numpy': 0.00011872214306862943, 'module_to_agent_unmapping': 6.25438528614672e-06, 'normalize_and_clip_actions': 7.093645732888701e-05, 'listify_data_for_vector_env': 2.332244802678547e-05, 'remove_single_ts_time_rank_from_batch': 2.3483757557232684e-06}}}, 'sample': 91.42628200002946, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -35.220000000000184, 'blue_0': -25.460000000000143, 'blue_1': -39.70000000000033, 'red_0': -46.38000000000024}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 230.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1386000.0, 'blue_0': 1386000.0, 'blue_1': 1386000.0, 'red_0': 1386000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -35.220000000000184, 'blue_policy': -39.70000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2772000.0, 'blue_policy': 2772000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033318450668941394, 'episode_return_mean': -146.7600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -99.60000000000032, 'episode_duration_sec_mean': 18.15033827996813, 'episode_return_min': -186.1000000000015, 'rlmodule_inference_timer': 0.012558090821223458, 'num_episodes_lifetime': 1155.0, 'episode_len_min': 1200, 'time_between_sampling': 358.4319609000813, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.374158163967373, 'throughput_since_last_restore': 15.810331734466898}}, 'learners': {'red_policy': {'policy_loss': -0.08844339102506638, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.009001391008496284, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 231.0, 'num_module_steps_trained_lifetime': 83233920.0, 'curr_entropy_coeff': 0.029210000000000003, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001614, 'vf_explained_var': -0.5917357206344604, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.89572525024414, 'entropy': 1.065245270729065, 'vf_loss_unclipped': 640.7582397460938, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 983.3270617169396, 'throughput_since_last_restore': 949.4631181863939}}, 'blue_policy': {'weights_seq_no': 231.0, 'num_module_steps_trained_lifetime': 83233920.0, 'curr_entropy_coeff': 0.029210000000000003, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001614, 'vf_explained_var': 0.3603498339653015, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 3.8456172943115234, 'total_loss': 1.4144693613052368, 'entropy': 1.6851171255111694, 'policy_loss': -0.033554814755916595, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.4741005897521973, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.015340056270360947, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 983.3255726243311, 'throughput_since_last_restore': 949.4631189813657}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.500049494206905e-06, 'batch_individual_items': 1.0449179999995977, 'add_time_dim_to_batch_and_zero_pad': 2.0999927073717117e-05, 'numpy_to_tensor': 0.1462128000566736, 'add_observations_from_episodes_to_batch': 0.00024649989791214466, 'agent_to_module_mapping': 0.022344799945130944, 'add_one_ts_to_episodes_and_truncate': 0.1936982999322936, 'add_columns_from_episodes_to_train_batch': 0.4715756999794394, 'general_advantage_estimation': 13.438284100033343}}, 'connector_pipeline_timer': 15.317711999989115}, 'num_module_steps_trained_lifetime': 166467840.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 3901590000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 46093.356907626614, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 46093.354869808914, 'throughput_since_last_restore': 44506.083740125054}, 'num_module_steps_trained_throughput': 1966.6497895310272, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1966.6497600120242, 'throughput_since_last_restore': 1898.9262391757673}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1386000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 231, 'trial_id': 'default', 'date': '2026-01-26_15-39-56', 'timestamp': 1769438396, 'time_this_iter_s': 366.40411472320557, 'time_total_s': 87638.18039488792, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 87638.18039488792, 'iterations_since_restore': 231, 'perf': {'cpu_util_percent': 19.030268199233713, 'ram_util_percent': 91.09099616858238}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 372
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 384
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 658
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 671
(MultiAgentEnvRunner pid=37492) {'red_0': -27.60000000000006, 'red_1': -16.29999999999998, 'blue_0': -31.100000000000296, 'blue_1': -34.10000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 344
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 907
(MultiAgentEnvRunner pid=37492) {'red_0': -24.10000000000009, 'red_1': -25.90000000000013, 'blue_0': -26.50000000000008, 'blue_1': -56.50000000000049}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -5.4999999999999964, 'red_1': -5.999999999999995, 'blue_0': -44.70000000000035, 'blue_1': -61.700000000000585}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1134
(MultiAgentEnvRunner pid=37492) {'red_0': -19.800000000000015, 'red_1': -30.700000000000166, 'blue_0': -51.300000000000445, 'blue_1': -42.10000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 411
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 595
(MultiAgentEnvRunner pid=37492) {'red_0': -31.800000000000143, 'red_1': -33.800000000000196, 'blue_0': -38.80000000000025, 'blue_1': -37.60000000000024}
ITERATION 231: reward=-129.18000000000083, metadata={'num_env_steps_sampled_lifetime': 1392000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003386835728933272, 'timers': {'connectors': {'batch_individual_items': 9.707273090314368e-05, 'add_states_from_episodes_to_batch': 6.618972235861147e-06, 'add_time_dim_to_batch_and_zero_pad': 1.340315473865275e-05, 'numpy_to_tensor': 7.391516403094518e-05, 'agent_to_module_mapping': 8.423237739444744e-06, 'add_observations_from_episodes_to_batch': 4.0074939303417676e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009186313635545179, 'timers': {'connectors': {'get_actions': 0.00047547011163643504, 'un_batch_to_individual_items': 6.740283903157297e-05, 'tensor_to_numpy': 0.00012365756447678815, 'module_to_agent_unmapping': 7.338150320615607e-06, 'normalize_and_clip_actions': 7.677422418742101e-05, 'listify_data_for_vector_env': 2.4892162438780053e-05, 'remove_single_ts_time_rank_from_batch': 2.6822997149490625e-06}}}, 'sample': 96.73113259999081, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.54000000000009, 'blue_0': -38.48000000000028, 'blue_1': -46.40000000000039, 'red_0': -21.76000000000006}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 231.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1392000.0, 'blue_0': 1392000.0, 'blue_1': 1392000.0, 'red_0': 1392000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.54000000000009, 'blue_policy': -46.40000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 2784000.0, 'blue_policy': 2784000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003542958457727072, 'episode_return_mean': -129.18000000000083, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.1000000000006, 'episode_duration_sec_mean': 19.14618629999459, 'episode_return_min': -143.900000000001, 'rlmodule_inference_timer': 0.013454886575385686, 'num_episodes_lifetime': 1160.0, 'episode_len_min': 1200, 'time_between_sampling': 275.1303343999898, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.295286756607183, 'throughput_since_last_restore': 15.812359709008035}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 841
(MultiAgentEnvRunner pid=37492) {'red_0': -25.60000000000006, 'red_1': -35.100000000000215, 'blue_0': -35.00000000000029, 'blue_1': -28.700000000000227}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 693
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 785
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 785
(MultiAgentEnvRunner pid=37492) {'red_0': -38.80000000000026, 'red_1': -11.999999999999984, 'blue_0': -43.00000000000045, 'blue_1': -47.700000000000436}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1142
(MultiAgentEnvRunner pid=37492) {'red_0': -18.90000000000003, 'red_1': -4.799999999999999, 'blue_0': -42.100000000000314, 'blue_1': -46.80000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -14.599999999999964, 'red_1': -9.799999999999981, 'blue_0': -40.4000000000003, 'blue_1': -50.70000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 360
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 557
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 708
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 806
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1064
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1117
(MultiAgentEnvRunner pid=37492) {'red_0': -74.60000000000001, 'red_1': -37.3000000000002, 'blue_0': -33.10000000000046, 'blue_1': -31.50000000000027}
ITERATION 232: reward=-134.10000000000088, metadata={'num_env_steps_sampled_lifetime': 1398000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003475170381971936, 'timers': {'connectors': {'batch_individual_items': 9.959674067428017e-05, 'add_states_from_episodes_to_batch': 6.850574323901217e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4276468508954516e-05, 'numpy_to_tensor': 7.283723997980893e-05, 'agent_to_module_mapping': 8.738599118616397e-06, 'add_observations_from_episodes_to_batch': 4.210997042216003e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009382215018577453, 'timers': {'connectors': {'get_actions': 0.0004842442253720998, 'un_batch_to_individual_items': 7.086443963641365e-05, 'tensor_to_numpy': 0.0001255555213328746, 'module_to_agent_unmapping': 7.028527976765142e-06, 'normalize_and_clip_actions': 7.760835939158362e-05, 'listify_data_for_vector_env': 2.5902686748854656e-05, 'remove_single_ts_time_rank_from_batch': 2.5464698619354707e-06}}}, 'sample': 93.68227390001994, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -19.800000000000075, 'blue_0': -38.720000000000354, 'blue_1': -41.08000000000035, 'red_0': -34.50000000000007}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 232.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1398000.0, 'blue_0': 1398000.0, 'blue_1': 1398000.0, 'red_0': 1398000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -19.800000000000075, 'blue_policy': -41.08000000000035}, 'num_module_steps_sampled_lifetime': {'red_policy': 2796000.0, 'blue_policy': 2796000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003664677996263724, 'episode_return_mean': -134.10000000000088, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -112.60000000000073, 'episode_duration_sec_mean': 18.610824880027213, 'episode_return_min': -176.50000000000097, 'rlmodule_inference_timer': 0.013894860232191627, 'num_episodes_lifetime': 1165.0, 'episode_len_min': 1200, 'time_between_sampling': 271.35033319995273, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.81223848489447, 'throughput_since_last_restore': 15.812358685770157}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 351
(MultiAgentEnvRunner pid=37492) {'red_0': -25.500000000000025, 'red_1': -10.399999999999983, 'blue_0': -41.20000000000029, 'blue_1': -46.70000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 374
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 473
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 473
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 848
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1046
(MultiAgentEnvRunner pid=37492) {'red_0': -45.200000000000294, 'red_1': -76.60000000000021, 'blue_0': -25.100000000000147, 'blue_1': -11.39999999999992}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1127
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1128
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1148
(MultiAgentEnvRunner pid=37492) {'red_0': -27.900000000000045, 'red_1': -47.60000000000031, 'blue_0': -24.900000000000333, 'blue_1': -36.10000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 315
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1004
(MultiAgentEnvRunner pid=37492) {'red_0': -19.300000000000036, 'red_1': -18.399999999999995, 'blue_0': -64.10000000000059, 'blue_1': -41.900000000000325}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 321
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1130
(MultiAgentEnvRunner pid=37492) {'red_0': -4.700000000000001, 'red_1': -18.89999999999998, 'blue_0': -39.400000000000375, 'blue_1': -46.200000000000394}
ITERATION 233: reward=-134.30000000000078, metadata={'num_env_steps_sampled_lifetime': 1404000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003211481778855406, 'timers': {'connectors': {'batch_individual_items': 9.548011236417757e-05, 'add_states_from_episodes_to_batch': 6.280247714208443e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2352977139867229e-05, 'numpy_to_tensor': 6.61488006079685e-05, 'agent_to_module_mapping': 8.05068180888395e-06, 'add_observations_from_episodes_to_batch': 3.877005976641695e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008683444918550108, 'timers': {'connectors': {'get_actions': 0.0004497546853774915, 'un_batch_to_individual_items': 6.486795426825602e-05, 'tensor_to_numpy': 0.00011736575948224954, 'module_to_agent_unmapping': 6.6536902766809546e-06, 'normalize_and_clip_actions': 7.141615215129342e-05, 'listify_data_for_vector_env': 2.3663813321588223e-05, 'remove_single_ts_time_rank_from_batch': 2.327991975271398e-06}}}, 'sample': 87.68060510000214, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -34.380000000000095, 'blue_0': -38.94000000000035, 'blue_1': -36.460000000000264, 'red_0': -24.52000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 233.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1404000.0, 'blue_0': 1404000.0, 'blue_1': 1404000.0, 'red_0': 1404000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -34.380000000000095, 'blue_policy': -36.460000000000264}, 'num_module_steps_sampled_lifetime': {'red_policy': 2808000.0, 'blue_policy': 2808000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034034103542144873, 'episode_return_mean': -134.30000000000078, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.20000000000076, 'episode_duration_sec_mean': 17.39604439998511, 'episode_return_min': -158.30000000000055, 'rlmodule_inference_timer': 0.012776680431763037, 'num_episodes_lifetime': 1170.0, 'episode_len_min': 1200, 'time_between_sampling': 285.77356230001897, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.14278906914059, 'throughput_since_last_restore': 15.813741601228154}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 660
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 978
(MultiAgentEnvRunner pid=37492) {'red_0': -29.100000000000087, 'red_1': -36.40000000000014, 'blue_0': -29.400000000000304, 'blue_1': -14.100000000000072}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -22.50000000000005, 'red_1': -15.499999999999961, 'blue_0': -33.2000000000002, 'blue_1': -48.3000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 561
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1029
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1052
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1196
(MultiAgentEnvRunner pid=37492) {'red_0': -41.10000000000025, 'red_1': -52.60000000000043, 'blue_0': -39.9000000000003, 'blue_1': -28.30000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -0.7, 'red_1': -11.699999999999974, 'blue_0': -56.10000000000051, 'blue_1': -53.40000000000047}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 909
(MultiAgentEnvRunner pid=37492) {'red_0': -18.70000000000001, 'red_1': -29.90000000000017, 'blue_0': -52.500000000000526, 'blue_1': -18.900000000000112}
ITERATION 234: reward=-126.46000000000083, metadata={'num_env_steps_sampled_lifetime': 1410000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032688347991721684, 'timers': {'connectors': {'batch_individual_items': 9.419339010346397e-05, 'add_states_from_episodes_to_batch': 6.2496611299714345e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2980911394023117e-05, 'numpy_to_tensor': 6.828582446172059e-05, 'agent_to_module_mapping': 8.1890609459238e-06, 'add_observations_from_episodes_to_batch': 3.9168800321925134e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008645799923394077, 'timers': {'connectors': {'get_actions': 0.0004448892636762236, 'un_batch_to_individual_items': 6.484212262874773e-05, 'tensor_to_numpy': 0.0001182361977500534, 'module_to_agent_unmapping': 6.4256901752096785e-06, 'normalize_and_clip_actions': 7.076199627173837e-05, 'listify_data_for_vector_env': 2.3707436538832364e-05, 'remove_single_ts_time_rank_from_batch': 2.282046203648263e-06}}}, 'sample': 94.60528570006136, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -29.220000000000134, 'blue_0': -42.22000000000037, 'blue_1': -32.60000000000025, 'red_0': -22.42000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 234.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1410000.0, 'blue_0': 1410000.0, 'blue_1': 1410000.0, 'red_0': 1410000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -29.220000000000134, 'blue_policy': -32.60000000000025}, 'num_module_steps_sampled_lifetime': {'red_policy': 2820000.0, 'blue_policy': 2820000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033920669134214357, 'episode_return_mean': -126.46000000000083, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -109.0000000000006, 'episode_duration_sec_mean': 18.785045600007287, 'episode_return_min': -161.90000000000114, 'rlmodule_inference_timer': 0.012863597393602974, 'num_episodes_lifetime': 1175.0, 'episode_len_min': 1200, 'time_between_sampling': 284.003328199964, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.35938936056356, 'throughput_since_last_restore': 15.815985791285042}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 636
(MultiAgentEnvRunner pid=37492) {'red_0': -44.70000000000029, 'red_1': -46.40000000000031, 'blue_0': -32.30000000000022, 'blue_1': -42.300000000000395}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 301
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 475
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 681
(MultiAgentEnvRunner pid=37492) {'red_0': -34.000000000000135, 'red_1': -38.6000000000002, 'blue_0': -30.700000000000276, 'blue_1': -64.00000000000067}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 538
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 623
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 661
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 661
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 752
(MultiAgentEnvRunner pid=37492) {'red_0': -23.20000000000012, 'red_1': -9.399999999999983, 'blue_0': -52.20000000000049, 'blue_1': -37.10000000000024}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 884
(MultiAgentEnvRunner pid=37492) {'red_0': -35.30000000000016, 'red_1': -55.90000000000043, 'blue_0': -27.600000000000335, 'blue_1': -34.000000000000284}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 683
(MultiAgentEnvRunner pid=37492) {'red_0': -43.40000000000035, 'red_1': -24.10000000000009, 'blue_0': -31.400000000000247, 'blue_1': -35.70000000000035}
ITERATION 235: reward=-148.46000000000112, metadata={'num_env_steps_sampled_lifetime': 1416000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032204682807028973, 'timers': {'connectors': {'batch_individual_items': 9.541859658453251e-05, 'add_states_from_episodes_to_batch': 6.331890550831341e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2438667630675032e-05, 'numpy_to_tensor': 6.702813237442824e-05, 'agent_to_module_mapping': 7.982218793544757e-06, 'add_observations_from_episodes_to_batch': 3.836100112212493e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008653090464232712, 'timers': {'connectors': {'get_actions': 0.00044761633561343935, 'un_batch_to_individual_items': 6.550498616602548e-05, 'tensor_to_numpy': 0.00011699173228676338, 'module_to_agent_unmapping': 6.424761739286237e-06, 'normalize_and_clip_actions': 7.083847673161998e-05, 'listify_data_for_vector_env': 2.3792840698161767e-05, 'remove_single_ts_time_rank_from_batch': 2.4058938202120388e-06}}}, 'sample': 90.41572150005959, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -34.8800000000002, 'blue_0': -34.840000000000316, 'blue_1': -42.62000000000039, 'red_0': -36.12000000000021}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 235.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1416000.0, 'blue_0': 1416000.0, 'blue_1': 1416000.0, 'red_0': 1416000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -34.8800000000002, 'blue_policy': -42.62000000000039}, 'num_module_steps_sampled_lifetime': {'red_policy': 2832000.0, 'blue_policy': 2832000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034390393987665145, 'episode_return_mean': -148.46000000000112, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -121.90000000000083, 'episode_duration_sec_mean': 17.95929579997901, 'episode_return_min': -167.30000000000126, 'rlmodule_inference_timer': 0.012728614715749405, 'num_episodes_lifetime': 1180.0, 'episode_len_min': 1200, 'time_between_sampling': 272.1571180000901, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.569363287876946, 'throughput_since_last_restore': 15.819033074007185}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 442
(MultiAgentEnvRunner pid=37492) {'red_0': -28.50000000000014, 'red_1': -25.600000000000094, 'blue_0': -43.100000000000335, 'blue_1': -43.70000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 298
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1154
(MultiAgentEnvRunner pid=37492) {'red_0': -10.199999999999992, 'red_1': -10.899999999999995, 'blue_0': -53.800000000000544, 'blue_1': -43.30000000000054}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 457
(MultiAgentEnvRunner pid=37492) {'red_0': -12.79999999999997, 'red_1': -22.70000000000005, 'blue_0': -43.90000000000034, 'blue_1': -45.900000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 434
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 449
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 550
(MultiAgentEnvRunner pid=37492) {'red_0': -22.29999999999998, 'red_1': -26.500000000000043, 'blue_0': -34.60000000000017, 'blue_1': -37.80000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 962
(MultiAgentEnvRunner pid=37492) {'red_0': -44.80000000000038, 'red_1': -25.600000000000094, 'blue_0': -55.70000000000049, 'blue_1': -50.600000000000435}
ITERATION 236: reward=-136.46000000000092, metadata={'num_env_steps_sampled_lifetime': 1422000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003441748909162036, 'timers': {'connectors': {'batch_individual_items': 9.982884324548207e-05, 'add_states_from_episodes_to_batch': 6.543146408965666e-06, 'add_time_dim_to_batch_and_zero_pad': 1.336226418968654e-05, 'numpy_to_tensor': 7.492516555054765e-05, 'agent_to_module_mapping': 8.44760798698361e-06, 'add_observations_from_episodes_to_batch': 4.0405910264622925e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008904599362160452, 'timers': {'connectors': {'get_actions': 0.0004558770276143404, 'un_batch_to_individual_items': 6.746290475897161e-05, 'tensor_to_numpy': 0.00012124179620154438, 'module_to_agent_unmapping': 6.65729665785541e-06, 'normalize_and_clip_actions': 7.433253109670984e-05, 'listify_data_for_vector_env': 2.490043257493069e-05, 'remove_single_ts_time_rank_from_batch': 2.4086814965828558e-06}}}, 'sample': 91.1797690000385, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -22.260000000000055, 'blue_0': -46.22000000000038, 'blue_1': -44.2600000000004, 'red_0': -23.72000000000009}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 236.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1422000.0, 'blue_0': 1422000.0, 'blue_1': 1422000.0, 'red_0': 1422000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -22.260000000000055, 'blue_policy': -44.2600000000004}, 'num_module_steps_sampled_lifetime': {'red_policy': 2844000.0, 'blue_policy': 2844000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000347465914796112, 'episode_return_mean': -136.46000000000092, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -118.20000000000107, 'episode_duration_sec_mean': 18.090065199998207, 'episode_return_min': -176.7000000000014, 'rlmodule_inference_timer': 0.013475436159370111, 'num_episodes_lifetime': 1185.0, 'episode_len_min': 1200, 'time_between_sampling': 271.7007955000736, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.036327614661065, 'throughput_since_last_restore': 15.819937088283561}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1136
(MultiAgentEnvRunner pid=37492) {'red_0': -11.399999999999993, 'red_1': -11.799999999999992, 'blue_0': -37.300000000000466, 'blue_1': -35.00000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 396
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 979
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1022
(MultiAgentEnvRunner pid=37492) {'red_0': -22.000000000000007, 'red_1': -14.399999999999999, 'blue_0': -37.60000000000031, 'blue_1': -37.90000000000056}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 395
(MultiAgentEnvRunner pid=37492) {'red_0': -30.30000000000009, 'red_1': -15.19999999999998, 'blue_0': -29.400000000000123, 'blue_1': -42.8000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 502
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 736
(MultiAgentEnvRunner pid=37492) {'red_0': -37.500000000000306, 'red_1': -20.199999999999992, 'blue_0': -46.500000000000306, 'blue_1': -40.10000000000023}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -53.50000000000047, 'red_1': -36.30000000000024, 'blue_0': -38.90000000000028, 'blue_1': -50.20000000000042}
ITERATION 237: reward=-129.6600000000009, metadata={'num_env_steps_sampled_lifetime': 1428000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003448869819252061, 'timers': {'connectors': {'batch_individual_items': 0.00010348319263051427, 'add_states_from_episodes_to_batch': 6.557109645622073e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3100002703053754e-05, 'numpy_to_tensor': 7.359068802097211e-05, 'agent_to_module_mapping': 8.400932631401043e-06, 'add_observations_from_episodes_to_batch': 3.9951149990562435e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008977555016518217, 'timers': {'connectors': {'get_actions': 0.00046810794947040084, 'un_batch_to_individual_items': 6.644620430969401e-05, 'tensor_to_numpy': 0.00012220990087154484, 'module_to_agent_unmapping': 6.682700126961098e-06, 'normalize_and_clip_actions': 7.209816101534426e-05, 'listify_data_for_vector_env': 2.408009386690474e-05, 'remove_single_ts_time_rank_from_batch': 2.4717169917996706e-06}}}, 'sample': 91.76214989996515, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -19.58000000000004, 'blue_0': -37.940000000000296, 'blue_1': -41.20000000000038, 'red_0': -30.94000000000017}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 237.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1428000.0, 'blue_0': 1428000.0, 'blue_1': 1428000.0, 'red_0': 1428000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -19.58000000000004, 'blue_policy': -41.20000000000038}, 'num_module_steps_sampled_lifetime': {'red_policy': 2856000.0, 'blue_policy': 2856000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003487265253358472, 'episode_return_mean': -129.6600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -95.50000000000074, 'episode_duration_sec_mean': 18.227040679985656, 'episode_return_min': -178.9000000000014, 'rlmodule_inference_timer': 0.013284549179676768, 'num_episodes_lifetime': 1190.0, 'episode_len_min': 1200, 'time_between_sampling': 282.976137700025, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 16.765050371594516, 'throughput_since_last_restore': 15.823684706976662}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -6.199999999999994, 'red_1': -4.899999999999999, 'blue_0': -60.60000000000057, 'blue_1': -53.900000000000475}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 288
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 310
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 470
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 700
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 1143
(MultiAgentEnvRunner pid=37492) {'red_0': -43.70000000000034, 'red_1': -38.40000000000022, 'blue_0': -44.100000000000406, 'blue_1': -24.80000000000002}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1123
(MultiAgentEnvRunner pid=37492) {'red_0': -36.900000000000176, 'red_1': -42.000000000000256, 'blue_0': -41.400000000000375, 'blue_1': -18.3000000000002}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 582
(MultiAgentEnvRunner pid=37492) {'red_0': -16.699999999999992, 'red_1': -12.89999999999997, 'blue_0': -43.60000000000034, 'blue_1': -57.700000000000514}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 495
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 540
(MultiAgentEnvRunner pid=37492) {'red_0': -38.00000000000029, 'red_1': -40.0000000000003, 'blue_0': -42.90000000000033, 'blue_1': -43.70000000000034}
ITERATION 238: reward=-142.140000000001, metadata={'num_env_steps_sampled_lifetime': 1434000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00031510953288361397, 'timers': {'connectors': {'batch_individual_items': 9.191297461465363e-05, 'add_states_from_episodes_to_batch': 6.189733632687024e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2189671881465369e-05, 'numpy_to_tensor': 6.599812622426123e-05, 'agent_to_module_mapping': 7.957741478306775e-06, 'add_observations_from_episodes_to_batch': 3.758966985632426e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008520828197233553, 'timers': {'connectors': {'get_actions': 0.00044166125945979104, 'un_batch_to_individual_items': 6.309584481362688e-05, 'tensor_to_numpy': 0.00011570048065783305, 'module_to_agent_unmapping': 6.19025940764459e-06, 'normalize_and_clip_actions': 7.003214901695926e-05, 'listify_data_for_vector_env': 2.317181542702258e-05, 'remove_single_ts_time_rank_from_batch': 2.2881212356179815e-06}}}, 'sample': 92.32877120003104, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.640000000000146, 'blue_0': -46.5200000000004, 'blue_1': -39.680000000000305, 'red_0': -28.30000000000016}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 238.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1434000.0, 'blue_0': 1434000.0, 'blue_1': 1434000.0, 'red_0': 1434000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.640000000000146, 'blue_policy': -39.680000000000305}, 'num_module_steps_sampled_lifetime': {'red_policy': 2868000.0, 'blue_policy': 2868000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033393512776050914, 'episode_return_mean': -142.140000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -125.60000000000103, 'episode_duration_sec_mean': 18.30873417998664, 'episode_return_min': -164.60000000000127, 'rlmodule_inference_timer': 0.012428155519006532, 'num_episodes_lifetime': 1195.0, 'episode_len_min': 1200, 'time_between_sampling': 266.1305381999118, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 17.044181592039216, 'throughput_since_last_restore': 15.828426654533093}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 211
(MultiAgentEnvRunner pid=37492) {'red_0': -34.400000000000226, 'red_1': -37.30000000000027, 'blue_0': -39.70000000000028, 'blue_1': -29.000000000000117}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 258
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 319
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 622
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 741
(MultiAgentEnvRunner pid=37492) {'red_0': -43.90000000000038, 'red_1': -32.20000000000016, 'blue_0': -60.50000000000058, 'blue_1': -46.700000000000365}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 232
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1027
(MultiAgentEnvRunner pid=37492) {'red_0': -29.500000000000085, 'red_1': -28.10000000000006, 'blue_0': -31.7000000000004, 'blue_1': -53.60000000000054}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 581
(MultiAgentEnvRunner pid=37492) {'red_0': -10.099999999999994, 'red_1': -17.900000000000006, 'blue_0': -23.40000000000003, 'blue_1': -30.500000000000153}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 445
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 600
(MultiAgentEnvRunner pid=37492) {'red_0': -4.099999999999998, 'red_1': -13.59999999999996, 'blue_0': -47.00000000000038, 'blue_1': -62.700000000000585}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.49999999999748, 'red_1': -119.49999999999748, 'blue_0': -119.39999999999749, 'blue_1': -119.39999999999749}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.7, 'blue_1': -0.30000000000000004}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 16:32:18,564	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': -0.1, 'red_1': -6.5999999999999925, 'blue_0': -1.4000000000000001, 'blue_1': -112.59999999999788}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.2, 'blue_1': -0.5}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -0.4, 'blue_1': -119.2999999999975}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 239: reward=-135.18000000000092, metadata={'num_env_steps_sampled_lifetime': 1440000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003352384776589982, 'timers': {'connectors': {'batch_individual_items': 9.8193175989465e-05, 'add_states_from_episodes_to_batch': 6.6356866462056315e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2796104915528384e-05, 'numpy_to_tensor': 6.96265554978666e-05, 'agent_to_module_mapping': 8.465398969116826e-06, 'add_observations_from_episodes_to_batch': 4.0051897019689966e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008977588043135327, 'timers': {'connectors': {'get_actions': 0.0004603739802860932, 'un_batch_to_individual_items': 6.825041347683303e-05, 'tensor_to_numpy': 0.00012222408681708834, 'module_to_agent_unmapping': 6.607840905075509e-06, 'normalize_and_clip_actions': 7.395067754501516e-05, 'listify_data_for_vector_env': 2.4836631922734542e-05, 'remove_single_ts_time_rank_from_batch': 2.49376854980263e-06}}}, 'sample': 89.07740800001193, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -25.82000000000009, 'blue_0': -40.460000000000335, 'blue_1': -44.500000000000355, 'red_0': -24.400000000000137}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 239.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1440000.0, 'blue_0': 1440000.0, 'blue_1': 1440000.0, 'red_0': 1440000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -25.82000000000009, 'blue_policy': -44.500000000000355}, 'num_module_steps_sampled_lifetime': {'red_policy': 2880000.0, 'blue_policy': 2880000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003427402010535845, 'episode_return_mean': -135.18000000000092, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -81.90000000000019, 'episode_duration_sec_mean': 17.696807700023054, 'episode_return_min': -183.30000000000146, 'rlmodule_inference_timer': 0.013082411983269268, 'num_episodes_lifetime': 1200.0, 'episode_len_min': 1200, 'time_between_sampling': 259.6997318000067, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.882796752196745, 'throughput_since_last_restore': 15.813360657638318}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 528
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 653
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 727
(MultiAgentEnvRunner pid=37492) {'red_0': -22.700000000000113, 'red_1': -42.00000000000039, 'blue_0': -56.70000000000044, 'blue_1': -59.000000000000476}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 506
(MultiAgentEnvRunner pid=37492) {'red_0': -13.999999999999966, 'red_1': -11.599999999999977, 'blue_0': -42.600000000000335, 'blue_1': -44.30000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 892
(MultiAgentEnvRunner pid=37492) {'red_0': -19.40000000000001, 'red_1': -14.299999999999983, 'blue_0': -30.700000000000284, 'blue_1': -37.20000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -35.60000000000023, 'red_1': -23.90000000000007, 'blue_0': -43.800000000000345, 'blue_1': -44.10000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 515
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 808
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 808
(MultiAgentEnvRunner pid=37492) {'red_0': -17.000000000000018, 'red_1': -42.10000000000031, 'blue_0': -26.700000000000085, 'blue_1': -42.20000000000034}
ITERATION 240: reward=-133.98000000000087, metadata={'num_env_steps_sampled_lifetime': 1446000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00037757247841988526, 'timers': {'connectors': {'batch_individual_items': 0.00010583612589554485, 'add_states_from_episodes_to_batch': 7.715225430231065e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4464961202927833e-05, 'numpy_to_tensor': 7.923712469296464e-05, 'agent_to_module_mapping': 9.503265229444343e-06, 'add_observations_from_episodes_to_batch': 4.5993254746550445e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009793293699023463, 'timers': {'connectors': {'get_actions': 0.0004979353909951239, 'un_batch_to_individual_items': 7.371935018459263e-05, 'tensor_to_numpy': 0.00013349993262460938, 'module_to_agent_unmapping': 7.4687610788203514e-06, 'normalize_and_clip_actions': 8.219764683384972e-05, 'listify_data_for_vector_env': 2.773577921334524e-05, 'remove_single_ts_time_rank_from_batch': 2.6960306739870906e-06}}}, 'sample': 92.33285020000767, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.780000000000143, 'blue_0': -40.10000000000029, 'blue_1': -45.36000000000037, 'red_0': -21.74000000000007}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 240.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1446000.0, 'blue_0': 1446000.0, 'blue_1': 1446000.0, 'red_0': 1446000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.780000000000143, 'blue_policy': -45.36000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 2892000.0, 'blue_policy': 2892000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00038115083780740946, 'episode_return_mean': -133.98000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.6000000000006, 'episode_duration_sec_mean': 18.334679760015568, 'episode_return_min': -180.40000000000143, 'rlmodule_inference_timer': 0.01532132442011625, 'num_episodes_lifetime': 1205.0, 'episode_len_min': 1200, 'time_between_sampling': 376.6595884000417, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.326644513195696, 'throughput_since_last_restore': 15.811276808711327}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 391.4358346000081, 'restore_env_runners': 1.1399970389902592e-05, 'training_step': 391.435555000091, 'env_runner_sampling_timer': 92.48028829996474, 'learner_update_timer': 298.8988659000024, 'synch_weights': 0.01254500006325543, 'synch_env_connectors': 0.0023207999765872955, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1446000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00037757247841988526, 'timers': {'connectors': {'batch_individual_items': 0.00010583612589554485, 'add_states_from_episodes_to_batch': 7.715225430231065e-06, 'add_time_dim_to_batch_and_zero_pad': 1.4464961202927833e-05, 'numpy_to_tensor': 7.923712469296464e-05, 'agent_to_module_mapping': 9.503265229444343e-06, 'add_observations_from_episodes_to_batch': 4.5993254746550445e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009793293699023463, 'timers': {'connectors': {'get_actions': 0.0004979353909951239, 'un_batch_to_individual_items': 7.371935018459263e-05, 'tensor_to_numpy': 0.00013349993262460938, 'module_to_agent_unmapping': 7.4687610788203514e-06, 'normalize_and_clip_actions': 8.219764683384972e-05, 'listify_data_for_vector_env': 2.773577921334524e-05, 'remove_single_ts_time_rank_from_batch': 2.6960306739870906e-06}}}, 'sample': 92.33285020000767, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -26.780000000000143, 'blue_0': -40.10000000000029, 'blue_1': -45.36000000000037, 'red_0': -21.74000000000007}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 240.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1446000.0, 'blue_0': 1446000.0, 'blue_1': 1446000.0, 'red_0': 1446000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -26.780000000000143, 'blue_policy': -45.36000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 2892000.0, 'blue_policy': 2892000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00038115083780740946, 'episode_return_mean': -133.98000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.6000000000006, 'episode_duration_sec_mean': 18.334679760015568, 'episode_return_min': -180.40000000000143, 'rlmodule_inference_timer': 0.01532132442011625, 'num_episodes_lifetime': 1205.0, 'episode_len_min': 1200, 'time_between_sampling': 376.6595884000417, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.326644513195696, 'throughput_since_last_restore': 15.811276808711327}}, 'learners': {'red_policy': {'policy_loss': -0.27385246753692627, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.008108319714665413, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 241.0, 'num_module_steps_trained_lifetime': 86837120.0, 'curr_entropy_coeff': 0.028310000000000002, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00015539999999999998, 'vf_explained_var': -0.46842384338378906, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.717422485351562, 'entropy': 0.7950559854507446, 'vf_loss_unclipped': 733.860107421875, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 920.4195017155268, 'throughput_since_last_restore': 949.5198731991431}}, 'blue_policy': {'weights_seq_no': 241.0, 'num_module_steps_trained_lifetime': 86837120.0, 'curr_entropy_coeff': 0.028310000000000002, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.00015539999999999998, 'vf_explained_var': 0.7778517603874207, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 0.7333155274391174, 'total_loss': 0.46858328580856323, 'entropy': 1.6593029499053955, 'policy_loss': -0.23497433960437775, 'module_train_batch_size_mean': 128.0, 'vf_loss': 0.7263746857643127, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.016004720702767372, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 920.4183458838313, 'throughput_since_last_restore': 949.5198739331868}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 7.0999376475811005e-06, 'batch_individual_items': 0.8033294000197202, 'add_time_dim_to_batch_and_zero_pad': 2.8999987989664078e-05, 'numpy_to_tensor': 0.13453739997930825, 'add_observations_from_episodes_to_batch': 0.0003880000440403819, 'agent_to_module_mapping': 0.021538699977099895, 'add_one_ts_to_episodes_and_truncate': 0.20795229997020215, 'add_columns_from_episodes_to_train_batch': 0.4985661000246182, 'general_advantage_estimation': 12.411363899940625}}, 'connector_pipeline_timer': 14.07813019992318}, 'num_module_steps_trained_lifetime': 173674240.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 4070490000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 43144.58306090951, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 43144.58134161295, 'throughput_since_last_restore': 44508.744131255924}, 'num_module_steps_trained_throughput': 1840.8354531773412, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1840.835422612619, 'throughput_since_last_restore': 1899.0397491496528}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1446000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 241, 'trial_id': 'default', 'date': '2026-01-26_16-43-06', 'timestamp': 1769442186, 'time_this_iter_s': 391.45071244239807, 'time_total_s': 91427.30604600906, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 91427.30604600906, 'iterations_since_restore': 241, 'perf': {'cpu_util_percent': 14.611270125223614, 'ram_util_percent': 84.75169946332737}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 709
(MultiAgentEnvRunner pid=37492) {'red_0': -37.00000000000018, 'red_1': -46.80000000000032, 'blue_0': -27.500000000000284, 'blue_1': -43.4000000000004}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 870
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1010
(MultiAgentEnvRunner pid=37492) {'red_0': -22.300000000000068, 'red_1': -42.30000000000024, 'blue_0': -43.50000000000056, 'blue_1': -34.60000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -4.000000000000002, 'red_1': -30.50000000000016, 'blue_0': -40.200000000000294, 'blue_1': -42.80000000000034}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 571
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1116
(MultiAgentEnvRunner pid=37492) {'red_0': -67.30000000000037, 'red_1': -37.50000000000027, 'blue_0': -31.800000000000153, 'blue_1': -38.50000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -14.399999999999965, 'red_1': -9.499999999999982, 'blue_0': -43.60000000000034, 'blue_1': -42.80000000000033}
ITERATION 241: reward=-140.060000000001, metadata={'num_env_steps_sampled_lifetime': 1452000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003248059967545939, 'timers': {'connectors': {'batch_individual_items': 9.514307750999707e-05, 'add_states_from_episodes_to_batch': 6.456856559554899e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2511399089656065e-05, 'numpy_to_tensor': 6.72624586758698e-05, 'agent_to_module_mapping': 7.993234915606896e-06, 'add_observations_from_episodes_to_batch': 3.921228706883146e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008632486627930876, 'timers': {'connectors': {'get_actions': 0.00044093192076475224, 'un_batch_to_individual_items': 6.787412682395994e-05, 'tensor_to_numpy': 0.00011585861636415282, 'module_to_agent_unmapping': 6.333348467537882e-06, 'normalize_and_clip_actions': 7.302089170586049e-05, 'listify_data_for_vector_env': 2.3709537223503218e-05, 'remove_single_ts_time_rank_from_batch': 2.3193515274469926e-06}}}, 'sample': 96.06280770001467, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -33.32000000000019, 'blue_0': -37.32000000000033, 'blue_1': -40.42000000000034, 'red_0': -29.00000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 241.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1452000.0, 'blue_0': 1452000.0, 'blue_1': 1452000.0, 'red_0': 1452000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -33.32000000000019, 'blue_policy': -40.42000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 2904000.0, 'blue_policy': 2904000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003343702338720153, 'episode_return_mean': -140.060000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -110.30000000000061, 'episode_duration_sec_mean': 19.019768239976838, 'episode_return_min': -175.10000000000116, 'rlmodule_inference_timer': 0.013088915761678668, 'num_episodes_lifetime': 1210.0, 'episode_len_min': 1200, 'time_between_sampling': 299.2288465000456, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.195230940702746, 'throughput_since_last_restore': 15.808628050748885}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1158
(MultiAgentEnvRunner pid=37492) {'red_0': -16.29999999999998, 'red_1': -13.799999999999985, 'blue_0': -40.000000000000504, 'blue_1': -34.40000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -21.000000000000032, 'red_1': -22.700000000000053, 'blue_0': -51.50000000000045, 'blue_1': -51.90000000000045}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 411
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 629
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 814
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 884
(MultiAgentEnvRunner pid=37492) {'red_0': -56.100000000000435, 'red_1': -52.50000000000037, 'blue_0': -44.300000000000445, 'blue_1': -27.600000000000254}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 866
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 874
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1086
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1181
(MultiAgentEnvRunner pid=37492) {'red_0': -54.100000000000435, 'red_1': -49.20000000000034, 'blue_0': -35.200000000000315, 'blue_1': -19.5000000000001}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -3.3000000000000016, 'red_1': -12.299999999999972, 'blue_0': -42.50000000000033, 'blue_1': -46.10000000000037}
ITERATION 242: reward=-138.86000000000104, metadata={'num_env_steps_sampled_lifetime': 1458000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033241045779165956, 'timers': {'connectors': {'batch_individual_items': 0.0001007580030610726, 'add_states_from_episodes_to_batch': 6.391879879966465e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3014950609208784e-05, 'numpy_to_tensor': 6.913320403295765e-05, 'agent_to_module_mapping': 7.981262996262865e-06, 'add_observations_from_episodes_to_batch': 3.883515849965208e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008738383769415993, 'timers': {'connectors': {'get_actions': 0.0004476823952710708, 'un_batch_to_individual_items': 6.64061438753908e-05, 'tensor_to_numpy': 0.00011799205818579375, 'module_to_agent_unmapping': 7.154417095428808e-06, 'normalize_and_clip_actions': 7.245488068068282e-05, 'listify_data_for_vector_env': 2.39335082948014e-05, 'remove_single_ts_time_rank_from_batch': 2.4011392895230567e-06}}}, 'sample': 92.81723679997958, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.100000000000144, 'blue_0': -42.70000000000041, 'blue_1': -35.9000000000003, 'red_0': -30.160000000000178}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 242.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1458000.0, 'blue_0': 1458000.0, 'blue_1': 1458000.0, 'red_0': 1458000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.100000000000144, 'blue_policy': -35.9000000000003}, 'num_module_steps_sampled_lifetime': {'red_policy': 2916000.0, 'blue_policy': 2916000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003403063012594125, 'episode_return_mean': -138.86000000000104, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -104.20000000000067, 'episode_duration_sec_mean': 18.436666219984183, 'episode_return_min': -180.5000000000015, 'rlmodule_inference_timer': 0.01303818265073536, 'num_episodes_lifetime': 1215.0, 'episode_len_min': 1200, 'time_between_sampling': 298.70857359992806, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.44968804055594, 'throughput_since_last_restore': 15.807116389631426}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 740
(MultiAgentEnvRunner pid=37492) {'red_0': -31.400000000000173, 'red_1': -26.800000000000082, 'blue_0': -25.500000000000142, 'blue_1': -34.60000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 737
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1182
(MultiAgentEnvRunner pid=37492) {'red_0': -38.1000000000002, 'red_1': -55.50000000000041, 'blue_0': -51.20000000000051, 'blue_1': -20.200000000000117}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -2.1000000000000005, 'red_1': -3.5000000000000018, 'blue_0': -37.700000000000266, 'blue_1': -47.00000000000039}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 620
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 798
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 825
(MultiAgentEnvRunner pid=37492) {'red_0': -52.2000000000004, 'red_1': -43.800000000000296, 'blue_0': -37.40000000000031, 'blue_1': -32.20000000000017}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 755
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 993
(MultiAgentEnvRunner pid=37492) {'red_0': -55.300000000000416, 'red_1': -43.800000000000296, 'blue_0': -29.200000000000202, 'blue_1': -31.400000000000382}
ITERATION 243: reward=-139.780000000001, metadata={'num_env_steps_sampled_lifetime': 1464000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00034448875430055696, 'timers': {'connectors': {'batch_individual_items': 9.753017159674846e-05, 'add_states_from_episodes_to_batch': 6.8526646607277705e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2948309171513581e-05, 'numpy_to_tensor': 7.307774724987667e-05, 'agent_to_module_mapping': 8.618600822990389e-06, 'add_observations_from_episodes_to_batch': 4.113264164352345e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009146854704264913, 'timers': {'connectors': {'get_actions': 0.00046128826796916744, 'un_batch_to_individual_items': 6.98957009173796e-05, 'tensor_to_numpy': 0.00012636523949446219, 'module_to_agent_unmapping': 6.940905131160756e-06, 'normalize_and_clip_actions': 7.7783120347854e-05, 'listify_data_for_vector_env': 2.596318487184067e-05, 'remove_single_ts_time_rank_from_batch': 2.5514590031006255e-06}}}, 'sample': 91.91580740001518, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -34.68000000000022, 'blue_0': -36.20000000000029, 'blue_1': -33.08000000000027, 'red_0': -35.820000000000235}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 243.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1464000.0, 'blue_0': 1464000.0, 'blue_1': 1464000.0, 'red_0': 1464000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -34.68000000000022, 'blue_policy': -33.08000000000027}, 'num_module_steps_sampled_lifetime': {'red_policy': 2928000.0, 'blue_policy': 2928000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003655788803404465, 'episode_return_mean': -139.780000000001, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -90.30000000000067, 'episode_duration_sec_mean': 18.261541620013304, 'episode_return_min': -165.60000000000116, 'rlmodule_inference_timer': 0.014142413761052504, 'num_episodes_lifetime': 1220.0, 'episode_len_min': 1200, 'time_between_sampling': 295.5454067999963, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.400449740051693, 'throughput_since_last_restore': 15.805405413286747}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1156
(MultiAgentEnvRunner pid=37492) {'red_0': -14.399999999999983, 'red_1': -15.599999999999978, 'blue_0': -39.80000000000035, 'blue_1': -28.400000000000347}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 301
(MultiAgentEnvRunner pid=37492) {'red_0': -57.700000000000536, 'red_1': -42.900000000000325, 'blue_0': -44.30000000000034, 'blue_1': -39.000000000000284}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 479
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 578
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 593
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 884
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 893
(MultiAgentEnvRunner pid=37492) {'red_0': -37.90000000000044, 'red_1': -33.90000000000029, 'blue_0': -61.000000000000504, 'blue_1': -32.50000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 291
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 313
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 496
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 1032
(MultiAgentEnvRunner pid=37492) {'red_0': -18.499999999999943, 'red_1': -12.399999999999984, 'blue_0': -42.70000000000029, 'blue_1': -48.10000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 855
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 990
(MultiAgentEnvRunner pid=37492) {'red_0': -24.200000000000085, 'red_1': -32.00000000000011, 'blue_0': -35.2000000000003, 'blue_1': -28.900000000000382}
ITERATION 244: reward=-137.88000000000102, metadata={'num_env_steps_sampled_lifetime': 1470000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003223812812679745, 'timers': {'connectors': {'batch_individual_items': 9.566017089482176e-05, 'add_states_from_episodes_to_batch': 6.3080923278547185e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2002550151942675e-05, 'numpy_to_tensor': 6.736195839783003e-05, 'agent_to_module_mapping': 7.990177225623655e-06, 'add_observations_from_episodes_to_batch': 3.826850671451266e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008611576559813759, 'timers': {'connectors': {'get_actions': 0.0004472593813928918, 'un_batch_to_individual_items': 6.388003667430372e-05, 'tensor_to_numpy': 0.00011484769226570301, 'module_to_agent_unmapping': 6.23148180699987e-06, 'normalize_and_clip_actions': 6.994719945072688e-05, 'listify_data_for_vector_env': 2.3628802828182556e-05, 'remove_single_ts_time_rank_from_batch': 2.3057719062204958e-06}}}, 'sample': 92.07856679998804, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.360000000000138, 'blue_0': -44.60000000000036, 'blue_1': -35.38000000000034, 'red_0': -30.540000000000198}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 244.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1470000.0, 'blue_0': 1470000.0, 'blue_1': 1470000.0, 'red_0': 1470000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.360000000000138, 'blue_policy': -35.38000000000034}, 'num_module_steps_sampled_lifetime': {'red_policy': 2940000.0, 'blue_policy': 2940000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003376272087194077, 'episode_return_mean': -137.88000000000102, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -98.20000000000066, 'episode_duration_sec_mean': 18.292404460022226, 'episode_return_min': -183.90000000000148, 'rlmodule_inference_timer': 0.012955167334097961, 'num_episodes_lifetime': 1225.0, 'episode_len_min': 1200, 'time_between_sampling': 297.6839242000133, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.408058983408086, 'throughput_since_last_restore': 15.803741551769}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 283
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 544
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 647
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 647
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 719
(MultiAgentEnvRunner pid=37492) {'red_0': -43.30000000000031, 'red_1': -30.40000000000015, 'blue_0': -24.600000000000048, 'blue_1': -24.200000000000053}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 342
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 460
(MultiAgentEnvRunner pid=37492) {'red_0': -37.100000000000264, 'red_1': -32.40000000000021, 'blue_0': -33.500000000000185, 'blue_1': -20.90000000000001}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 404
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 570
(MultiAgentEnvRunner pid=37492) {'red_0': -62.70000000000059, 'red_1': -23.40000000000008, 'blue_0': -37.90000000000025, 'blue_1': -12.599999999999953}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.79999999999996, 'red_1': -12.199999999999973, 'blue_0': -64.80000000000057, 'blue_1': -39.40000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 564
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 886
(MultiAgentEnvRunner pid=37492) {'red_0': -35.80000000000016, 'red_1': -36.90000000000018, 'blue_0': -38.70000000000036, 'blue_1': -20.70000000000016}
ITERATION 245: reward=-129.46000000000078, metadata={'num_env_steps_sampled_lifetime': 1476000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033288556913882043, 'timers': {'connectors': {'batch_individual_items': 9.813654375180977e-05, 'add_states_from_episodes_to_batch': 6.632001341475542e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3293133940555536e-05, 'numpy_to_tensor': 6.914946286281272e-05, 'agent_to_module_mapping': 8.046463289886711e-06, 'add_observations_from_episodes_to_batch': 3.910557313965418e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008733407815567944, 'timers': {'connectors': {'get_actions': 0.00044869144440785327, 'un_batch_to_individual_items': 6.454131412183208e-05, 'tensor_to_numpy': 0.00011768744427741729, 'module_to_agent_unmapping': 6.71304722143942e-06, 'normalize_and_clip_actions': 7.23068196349951e-05, 'listify_data_for_vector_env': 2.4946326006208094e-05, 'remove_single_ts_time_rank_from_batch': 2.867425189681352e-06}}}, 'sample': 91.87363689998165, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.060000000000123, 'blue_0': -39.900000000000276, 'blue_1': -23.560000000000095, 'red_0': -38.94000000000025}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 245.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1476000.0, 'blue_0': 1476000.0, 'blue_1': 1476000.0, 'red_0': 1476000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.060000000000123, 'blue_policy': -23.560000000000095}, 'num_module_steps_sampled_lifetime': {'red_policy': 2952000.0, 'blue_policy': 2952000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003438343844157806, 'episode_return_mean': -129.46000000000078, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -122.50000000000057, 'episode_duration_sec_mean': 18.232338499976322, 'episode_return_min': -136.60000000000088, 'rlmodule_inference_timer': 0.013100115579100841, 'num_episodes_lifetime': 1230.0, 'episode_len_min': 1200, 'time_between_sampling': 297.32842759997584, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.468561335399313, 'throughput_since_last_restore': 15.802349239861343}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 415
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 468
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 681
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1139
(MultiAgentEnvRunner pid=37492) {'red_0': -32.200000000000124, 'red_1': -26.90000000000005, 'blue_0': -27.900000000000304, 'blue_1': -31.700000000000284}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 597
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 954
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1075
(MultiAgentEnvRunner pid=37492) {'red_0': -37.60000000000026, 'red_1': -47.40000000000031, 'blue_0': -44.90000000000049, 'blue_1': -20.90000000000001}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 683
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1023
(MultiAgentEnvRunner pid=37492) {'red_0': -34.40000000000024, 'red_1': -28.100000000000126, 'blue_0': -43.80000000000034, 'blue_1': -39.700000000000315}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 508
(MultiAgentEnvRunner pid=37492) {'red_0': -23.50000000000008, 'red_1': -31.30000000000013, 'blue_0': -34.10000000000019, 'blue_1': -16.99999999999994}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 638
(MultiAgentEnvRunner pid=37492) {'red_0': -4.5, 'red_1': -4.700000000000001, 'blue_0': -46.60000000000037, 'blue_1': -54.20000000000048}
ITERATION 246: reward=-126.2800000000008, metadata={'num_env_steps_sampled_lifetime': 1482000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032116080024874365, 'timers': {'connectors': {'batch_individual_items': 9.545022445242087e-05, 'add_states_from_episodes_to_batch': 6.455838536965469e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2138855350587667e-05, 'numpy_to_tensor': 6.656567837163355e-05, 'agent_to_module_mapping': 8.030968339258592e-06, 'add_observations_from_episodes_to_batch': 3.827535151317469e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008521395062829728, 'timers': {'connectors': {'get_actions': 0.0004375782345690096, 'un_batch_to_individual_items': 6.368117054259242e-05, 'tensor_to_numpy': 0.00011446117138763123, 'module_to_agent_unmapping': 6.190150112497547e-06, 'normalize_and_clip_actions': 7.095003353175522e-05, 'listify_data_for_vector_env': 2.382278510373026e-05, 'remove_single_ts_time_rank_from_batch': 2.303063105421918e-06}}}, 'sample': 91.22492399998009, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.68000000000012, 'blue_0': -39.460000000000335, 'blue_1': -32.7000000000002, 'red_0': -26.44000000000014}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 246.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1482000.0, 'blue_0': 1482000.0, 'blue_1': 1482000.0, 'red_0': 1482000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.68000000000012, 'blue_policy': -32.7000000000002}, 'num_module_steps_sampled_lifetime': {'red_policy': 2964000.0, 'blue_policy': 2964000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003300283980106426, 'episode_return_mean': -126.2800000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -105.90000000000035, 'episode_duration_sec_mean': 18.113323260005565, 'episode_return_min': -150.80000000000106, 'rlmodule_inference_timer': 0.01263164806776087, 'num_episodes_lifetime': 1235.0, 'episode_len_min': 1200, 'time_between_sampling': 296.02444740000647, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.519521122040441, 'throughput_since_last_restore': 15.80118296312001}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -40.5000000000003, 'red_1': -48.400000000000404, 'blue_0': -32.9000000000002, 'blue_1': -42.00000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 846
(MultiAgentEnvRunner pid=37492) {'red_0': -19.100000000000072, 'red_1': -20.00000000000012, 'blue_0': -48.90000000000035, 'blue_1': -44.20000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 782
(MultiAgentEnvRunner pid=37492) {'red_0': -30.8000000000001, 'red_1': -39.70000000000021, 'blue_0': -29.200000000000216, 'blue_1': -19.100000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -3.3000000000000016, 'red_1': -8.599999999999985, 'blue_0': -42.20000000000032, 'blue_1': -60.50000000000056}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -7.099999999999992, 'red_1': -7.699999999999989, 'blue_0': -60.100000000000556, 'blue_1': -51.10000000000045}
ITERATION 247: reward=-131.0800000000009, metadata={'num_env_steps_sampled_lifetime': 1488000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003237230255523801, 'timers': {'connectors': {'batch_individual_items': 9.53032810471167e-05, 'add_states_from_episodes_to_batch': 6.600903953119331e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2007074447573207e-05, 'numpy_to_tensor': 6.728079540731322e-05, 'agent_to_module_mapping': 8.146635353605906e-06, 'add_observations_from_episodes_to_batch': 3.899457888067872e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008611667746397766, 'timers': {'connectors': {'get_actions': 0.0004461562375481275, 'un_batch_to_individual_items': 6.312528646964308e-05, 'tensor_to_numpy': 0.00011618795694102008, 'module_to_agent_unmapping': 6.302454121635087e-06, 'normalize_and_clip_actions': 7.025043805644889e-05, 'listify_data_for_vector_env': 2.510981110392106e-05, 'remove_single_ts_time_rank_from_batch': 2.3157233361886737e-06}}}, 'sample': 91.45581389998551, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -24.88000000000014, 'blue_0': -42.66000000000033, 'blue_1': -43.38000000000033, 'red_0': -20.160000000000093}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 247.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1488000.0, 'blue_0': 1488000.0, 'blue_1': 1488000.0, 'red_0': 1488000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -24.88000000000014, 'blue_policy': -43.38000000000033}, 'num_module_steps_sampled_lifetime': {'red_policy': 2976000.0, 'blue_policy': 2976000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003356170973808685, 'episode_return_mean': -131.0800000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -114.60000000000088, 'episode_duration_sec_mean': 18.15110420002602, 'episode_return_min': -163.8000000000012, 'rlmodule_inference_timer': 0.012981485839298626, 'num_episodes_lifetime': 1240.0, 'episode_len_min': 1200, 'time_between_sampling': 295.382882899954, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.52537324726713, 'throughput_since_last_restore': 15.800050843270942}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 533
(MultiAgentEnvRunner pid=37492) {'red_0': -12.199999999999987, 'red_1': -18.300000000000008, 'blue_0': -31.000000000000217, 'blue_1': -31.700000000000166}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 629
(MultiAgentEnvRunner pid=37492) {'red_0': -29.400000000000077, 'red_1': -19.300000000000022, 'blue_0': -38.50000000000035, 'blue_1': -19.800000000000008}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 998
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1068
(MultiAgentEnvRunner pid=37492) {'red_0': -2.1999999999999726, 'red_1': -12.599999999999955, 'blue_0': -52.20000000000039, 'blue_1': -54.700000000000436}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 359
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 960
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 1147
(MultiAgentEnvRunner pid=37492) {'red_0': -59.800000000000544, 'red_1': -30.60000000000014, 'blue_0': -43.60000000000037, 'blue_1': -35.90000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 674
(MultiAgentEnvRunner pid=37492) {'red_0': -18.700000000000014, 'red_1': -24.70000000000006, 'blue_0': -36.000000000000306, 'blue_1': -26.800000000000228}
ITERATION 248: reward=-119.6000000000007, metadata={'num_env_steps_sampled_lifetime': 1494000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033079464047805403, 'timers': {'connectors': {'batch_individual_items': 9.797877647138075e-05, 'add_states_from_episodes_to_batch': 6.516636769762282e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2618689181934595e-05, 'numpy_to_tensor': 6.814405190558443e-05, 'agent_to_module_mapping': 8.207689049942043e-06, 'add_observations_from_episodes_to_batch': 3.9285106513673106e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000886313043438321, 'timers': {'connectors': {'get_actions': 0.00045266350178329386, 'un_batch_to_individual_items': 6.61421195864106e-05, 'tensor_to_numpy': 0.00012286791320152216, 'module_to_agent_unmapping': 6.522361355296742e-06, 'normalize_and_clip_actions': 7.31109277919127e-05, 'listify_data_for_vector_env': 2.454763627278159e-05, 'remove_single_ts_time_rank_from_batch': 2.3588633581334537e-06}}}, 'sample': 90.73392719996627, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -21.100000000000037, 'blue_0': -40.260000000000325, 'blue_1': -33.78000000000022, 'red_0': -24.46000000000012}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 248.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1494000.0, 'blue_0': 1494000.0, 'blue_1': 1494000.0, 'red_0': 1494000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -21.100000000000037, 'blue_policy': -33.78000000000022}, 'num_module_steps_sampled_lifetime': {'red_policy': 2988000.0, 'blue_policy': 2988000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003429767644268434, 'episode_return_mean': -119.6000000000007, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -93.20000000000039, 'episode_duration_sec_mean': 18.018698320025578, 'episode_return_min': -169.9000000000013, 'rlmodule_inference_timer': 0.013432015381818, 'num_episodes_lifetime': 1245.0, 'episode_len_min': 1200, 'time_between_sampling': 295.00535179988947, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.543691174099507, 'throughput_since_last_restore': 15.799003998273715}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_1 at STEP 334
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 633
(MultiAgentEnvRunner pid=37492) {'red_0': -25.800000000000033, 'red_1': -25.90000000000003, 'blue_0': -34.90000000000024, 'blue_1': -28.900000000000095}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 768
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 985
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1051
(MultiAgentEnvRunner pid=37492) {'red_0': -16.799999999999972, 'red_1': -21.600000000000108, 'blue_0': -49.20000000000051, 'blue_1': -47.400000000000325}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -20.000000000000018, 'red_1': -7.999999999999988, 'blue_0': -32.30000000000019, 'blue_1': -40.200000000000294}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.599999999999985, 'red_1': -23.300000000000065, 'blue_0': -37.70000000000026, 'blue_1': -41.50000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1162
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 1173
(MultiAgentEnvRunner pid=37492) {'red_0': -6.599999999999992, 'red_1': -16.099999999999966, 'blue_0': -36.3000000000003, 'blue_1': -45.20000000000036}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -118.59999999999754, 'blue_1': -118.59999999999754}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': 0, 'blue_0': -1.0999999999999999, 'blue_1': -39.60000000000029}
(MultiAgentEnvRunner pid=41856) RESET
2026-01-26 17:38:28,159	WARNING algorithm.py:2131 -- This evaluation iteration resulted in an empty set of episode summary results! It's possible that your configured duration timesteps are not enough to finish even a single episode. You have configured 5 episodes. For 'timesteps', try increasing this value via the `config.evaluation(evaluation_duration=...)` OR change the unit to 'episodes' via `config.evaluation(evaluation_duration_unit='episodes')` OR try increasing the timeout threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR you can also set `config.evaluation_force_reset_envs_before_iteration` to False. However, keep in mind that in the latter case, the evaluation results may contain some episode stats generated with earlier weights versions.
(MultiAgentEnvRunner pid=41856) {'red_0': 0, 'red_1': -13.599999999999971, 'blue_0': -105.49999999999828, 'blue_1': -105.59999999999827}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -115.7999999999977, 'red_1': 0, 'blue_0': -113.49999999999783, 'blue_1': -113.8999999999978}
(MultiAgentEnvRunner pid=41856) RESET
(MultiAgentEnvRunner pid=41856) {'red_0': -119.49999999999748, 'red_1': -119.09999999999751, 'blue_0': -119.09999999999751, 'blue_1': -118.79999999999752}
C:\Users\haslh\AppData\Roaming\Python\Python312\site-packages\ray\rllib\utils\metrics\stats\ema.py:124: RuntimeWarning: Mean of empty slice
  return np.nanmean(self._values_to_merge)
ITERATION 249: reward=-113.2600000000006, metadata={'num_env_steps_sampled_lifetime': 1500000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032854450693523305, 'timers': {'connectors': {'batch_individual_items': 9.768618269396528e-05, 'add_states_from_episodes_to_batch': 6.3477716054807935e-06, 'add_time_dim_to_batch_and_zero_pad': 1.226820640518069e-05, 'numpy_to_tensor': 6.781719061145422e-05, 'agent_to_module_mapping': 8.101458730700011e-06, 'add_observations_from_episodes_to_batch': 3.9412174027274086e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008663442101295385, 'timers': {'connectors': {'get_actions': 0.0004443833857081845, 'un_batch_to_individual_items': 6.531156127560844e-05, 'tensor_to_numpy': 0.00011737253876122571, 'module_to_agent_unmapping': 7.642848280506132e-06, 'normalize_and_clip_actions': 7.158813899565339e-05, 'listify_data_for_vector_env': 2.3681154253082362e-05, 'remove_single_ts_time_rank_from_batch': 2.3327993757653945e-06}}}, 'sample': 92.17556280002464, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -18.980000000000032, 'blue_0': -38.080000000000304, 'blue_1': -40.64000000000028, 'red_0': -15.559999999999999}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 249.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1500000.0, 'blue_0': 1500000.0, 'blue_1': 1500000.0, 'red_0': 1500000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -18.980000000000032, 'blue_policy': -40.64000000000028}, 'num_module_steps_sampled_lifetime': {'red_policy': 3000000.0, 'blue_policy': 3000000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.000336242530773825, 'episode_return_mean': -113.2600000000006, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -100.50000000000048, 'episode_duration_sec_mean': 18.312958419998175, 'episode_return_min': -135.0000000000009, 'rlmodule_inference_timer': 0.013139499397469716, 'num_episodes_lifetime': 1250.0, 'episode_len_min': 1200, 'time_between_sampling': 295.2783270999789, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 12.75492047842734, 'throughput_since_last_restore': 15.78393565716509}}
EVAL DONE
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 543
(MultiAgentEnvRunner pid=37492) {'red_0': -28.400000000000134, 'red_1': -24.90000000000005, 'blue_0': -24.600000000000048, 'blue_1': -30.70000000000016}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 810
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 1131
(MultiAgentEnvRunner pid=37492) {'red_0': -29.000000000000377, 'red_1': -10.199999999999962, 'blue_0': -46.90000000000032, 'blue_1': -48.80000000000033}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 839
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 959
(MultiAgentEnvRunner pid=37492) {'red_0': -30.800000000000153, 'red_1': -22.900000000000052, 'blue_0': -57.70000000000056, 'blue_1': -52.10000000000044}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 747
(MultiAgentEnvRunner pid=37492) {'red_0': -24.200000000000067, 'red_1': -21.30000000000005, 'blue_0': -26.700000000000248, 'blue_1': -45.200000000000436}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1085
(MultiAgentEnvRunner pid=37492) {'red_0': -10.699999999999996, 'red_1': -10.199999999999998, 'blue_0': -49.60000000000048, 'blue_1': -38.000000000000476}
ITERATION 250: reward=-126.58000000000087, metadata={'num_env_steps_sampled_lifetime': 1506000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033064121081324296, 'timers': {'connectors': {'batch_individual_items': 0.00010078935465333868, 'add_states_from_episodes_to_batch': 6.486978586837087e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2186913324959551e-05, 'numpy_to_tensor': 6.794341065158524e-05, 'agent_to_module_mapping': 7.964121995964783e-06, 'add_observations_from_episodes_to_batch': 3.9011765173249865e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008541565618266927, 'timers': {'connectors': {'get_actions': 0.00043987485670252303, 'un_batch_to_individual_items': 6.3879819508071e-05, 'tensor_to_numpy': 0.00011503113222655185, 'module_to_agent_unmapping': 6.32490689225059e-06, 'normalize_and_clip_actions': 6.964282477547579e-05, 'listify_data_for_vector_env': 2.408026400775074e-05, 'remove_single_ts_time_rank_from_batch': 2.464118688906084e-06}}}, 'sample': 91.7551514999941, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -17.900000000000023, 'blue_0': -41.10000000000033, 'blue_1': -42.96000000000037, 'red_0': -24.620000000000147}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 250.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1506000.0, 'blue_0': 1506000.0, 'blue_1': 1506000.0, 'red_0': 1506000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -17.900000000000023, 'blue_policy': -42.96000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 3012000.0, 'blue_policy': 3012000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034127081738976434, 'episode_return_mean': -126.58000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.50000000000094, 'episode_duration_sec_mean': 18.209015240031295, 'episode_return_min': -163.5000000000012, 'rlmodule_inference_timer': 0.013069431307555628, 'num_episodes_lifetime': 1255.0, 'episode_len_min': 1200, 'time_between_sampling': 378.23579730000347, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.388803976361915, 'throughput_since_last_restore': 15.782320628763566}}
Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=C:\Users\haslh\Documents\JKU\13.Semester\PrinciplesOfCooperation\CaptureTheFlag\ctf-env\ppo_v16/ppo_ctf_training), metrics={'timers': {'training_iteration': 389.8561267000623, 'restore_env_runners': 8.900067768990993e-06, 'training_step': 389.85589629993774, 'env_runner_sampling_timer': 91.89484389999416, 'learner_update_timer': 297.90834140009247, 'synch_weights': 0.015100799966603518, 'synch_env_connectors': 0.0021942000603303313, 'restore_eval_env_runners': nan, 'evaluation_iteration': nan, 'synch_eval_env_connectors': nan}, 'env_runners': {'num_env_steps_sampled_lifetime': 1506000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00033064121081324296, 'timers': {'connectors': {'batch_individual_items': 0.00010078935465333868, 'add_states_from_episodes_to_batch': 6.486978586837087e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2186913324959551e-05, 'numpy_to_tensor': 6.794341065158524e-05, 'agent_to_module_mapping': 7.964121995964783e-06, 'add_observations_from_episodes_to_batch': 3.9011765173249865e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008541565618266927, 'timers': {'connectors': {'get_actions': 0.00043987485670252303, 'un_batch_to_individual_items': 6.3879819508071e-05, 'tensor_to_numpy': 0.00011503113222655185, 'module_to_agent_unmapping': 6.32490689225059e-06, 'normalize_and_clip_actions': 6.964282477547579e-05, 'listify_data_for_vector_env': 2.408026400775074e-05, 'remove_single_ts_time_rank_from_batch': 2.464118688906084e-06}}}, 'sample': 91.7551514999941, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -17.900000000000023, 'blue_0': -41.10000000000033, 'blue_1': -42.96000000000037, 'red_0': -24.620000000000147}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 250.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1506000.0, 'blue_0': 1506000.0, 'blue_1': 1506000.0, 'red_0': 1506000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -17.900000000000023, 'blue_policy': -42.96000000000037}, 'num_module_steps_sampled_lifetime': {'red_policy': 3012000.0, 'blue_policy': 3012000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034127081738976434, 'episode_return_mean': -126.58000000000087, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -108.50000000000094, 'episode_duration_sec_mean': 18.209015240031295, 'episode_return_min': -163.5000000000012, 'rlmodule_inference_timer': 0.013069431307555628, 'num_episodes_lifetime': 1255.0, 'episode_len_min': 1200, 'time_between_sampling': 378.23579730000347, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.388803976361915, 'throughput_since_last_restore': 15.782320628763566}}, 'learners': {'red_policy': {'policy_loss': -0.1358741670846939, 'module_train_batch_size_mean': 128.0, 'vf_loss': 10.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.012373222969472408, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'weights_seq_no': 251.0, 'num_module_steps_trained_lifetime': 90440320.0, 'curr_entropy_coeff': 0.02741, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001494, 'vf_explained_var': -0.31279516220092773, 'curr_kl_coeff': 1.7085938453674316, 'total_loss': 9.863856315612793, 'entropy': 0.7785589694976807, 'vf_loss_unclipped': 727.816162109375, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 924.1530879207716, 'throughput_since_last_restore': 947.7809582000937}}, 'blue_policy': {'weights_seq_no': 251.0, 'num_module_steps_trained_lifetime': 90440320.0, 'curr_entropy_coeff': 0.02741, 'num_module_steps_trained': 360320, 'default_optimizer_learning_rate': 0.0001494, 'vf_explained_var': 0.5302956700325012, 'curr_kl_coeff': 1.5187500715255737, 'vf_loss_unclipped': 2.080960512161255, 'total_loss': 1.0512492656707764, 'entropy': 1.6154543161392212, 'policy_loss': -0.17092077434062958, 'module_train_batch_size_mean': 128.0, 'vf_loss': 1.2409343719482422, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'num_trainable_parameters': nan, 'mean_kl_loss': 0.01689593307673931, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 924.1518477924009, 'throughput_since_last_restore': 947.7809589271445}}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 6.900052540004253e-06, 'batch_individual_items': 1.1257890000706539, 'add_time_dim_to_batch_and_zero_pad': 2.2099935449659824e-05, 'numpy_to_tensor': 0.12821830005850643, 'add_observations_from_episodes_to_batch': 0.0003622999647632241, 'agent_to_module_mapping': 0.02268950000870973, 'add_one_ts_to_episodes_and_truncate': 0.17554100009147078, 'add_columns_from_episodes_to_train_batch': 0.5191993999760598, 'general_advantage_estimation': 12.438034399994649}}, 'connector_pipeline_timer': 14.410338099929504}, 'num_module_steps_trained_lifetime': 180880640.0, 'num_module_steps_trained': 720640, 'num_env_steps_trained': 16890000, 'num_env_steps_trained_lifetime': 4239390000.0, 'learner_connector_sum_episodes_length_out': 6000.0, 'num_non_trainable_parameters': nan, 'num_trainable_parameters': nan, 'learner_connector_sum_episodes_length_in': 6000.0, 'num_env_steps_trained_throughput': 43319.589599790655, 'num_env_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 43319.58799984659, 'throughput_since_last_restore': 44427.23248467502}, 'num_module_steps_trained_throughput': 1848.3024071056154, 'num_module_steps_trained_lifetime_throughput': {'throughput_since_last_reduce': 1848.3023800837877, 'throughput_since_last_restore': 1895.5619189508243}}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 1506000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 1}, 'done': False, 'training_iteration': 251, 'trial_id': 'default', 'date': '2026-01-26_17-49-16', 'timestamp': 1769446156, 'time_this_iter_s': 389.871657371521, 'time_total_s': 95396.50342535973, 'pid': 22428, 'hostname': 'DESKTOP-CM2GASU', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'ctf_env', 'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'non_human', 'max_steps': 1200}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 240, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': [[0, 0.0003], [2000000, 0.0001], [10000000, 5e-05]], 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 6000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function init.<locals>.<lambda> at 0x0000017A247CB060>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'env_config': {'width': 42, 'height': 42, 'num_of_team_agents': 2, 'render_mode': 'human', 'max_steps': 1200}, 'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'stats_cls_lookup': {'mean': <class 'ray.rllib.utils.metrics.stats.mean.MeanStats'>, 'ema': <class 'ray.rllib.utils.metrics.stats.ema.EmaStats'>, 'min': <class 'ray.rllib.utils.metrics.stats.min.MinStats'>, 'max': <class 'ray.rllib.utils.metrics.stats.max.MaxStats'>, 'sum': <class 'ray.rllib.utils.metrics.stats.sum.SumStats'>, 'lifetime_sum': <class 'ray.rllib.utils.metrics.stats.lifetime_sum.LifetimeSumStats'>, 'percentiles': <class 'ray.rllib.utils.metrics.stats.percentiles.PercentilesStats'>, 'item': <class 'ray.rllib.utils.metrics.stats.item.ItemStats'>, 'item_series': <class 'ray.rllib.utils.metrics.stats.item_series.ItemSeriesStats'>}, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': [[0, 0.05], [2000000, 0.02], [6000000, 0.01], [20000000, 0.0]], 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'red_policy': (None, None, None, {}), 'blue_policy': (None, None, None, {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 95396.50342535973, 'iterations_since_restore': 251, 'perf': {'cpu_util_percent': 14.007553956834531, 'ram_util_percent': 91.30665467625899}})
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -15.99999999999996, 'red_1': -15.899999999999961, 'blue_0': -47.60000000000039, 'blue_1': -44.10000000000035}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 177
(MultiAgentEnvRunner pid=37492) {'red_0': -14.099999999999977, 'red_1': -15.199999999999978, 'blue_0': -21.300000000000015, 'blue_1': -34.10000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 1182
(MultiAgentEnvRunner pid=37492) {'red_0': -12.899999999999974, 'red_1': -21.900000000000045, 'blue_0': -41.600000000000314, 'blue_1': -44.400000000000375}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 1056
(MultiAgentEnvRunner pid=37492) {'red_0': -12.099999999999966, 'red_1': -36.600000000000236, 'blue_0': -45.80000000000036, 'blue_1': -40.4000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 696
(MultiAgentEnvRunner pid=37492) {'red_0': -15.19999999999998, 'red_1': -26.500000000000117, 'blue_0': -34.30000000000029, 'blue_1': -28.700000000000212}
ITERATION 251: reward=-113.7400000000006, metadata={'num_env_steps_sampled_lifetime': 1512000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00035315221661453173, 'timers': {'connectors': {'batch_individual_items': 9.872805081123111e-05, 'add_states_from_episodes_to_batch': 7.323448231072687e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3490954425221637e-05, 'numpy_to_tensor': 7.5952959140114e-05, 'agent_to_module_mapping': 9.114047124113132e-06, 'add_observations_from_episodes_to_batch': 4.233985869235419e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0009364539142713757, 'timers': {'connectors': {'get_actions': 0.0004799215898652562, 'un_batch_to_individual_items': 7.167781162468102e-05, 'tensor_to_numpy': 0.00012685591303475437, 'module_to_agent_unmapping': 6.964203497706301e-06, 'normalize_and_clip_actions': 7.702870068671904e-05, 'listify_data_for_vector_env': 2.601702638281391e-05, 'remove_single_ts_time_rank_from_batch': 2.5190851945486512e-06}}}, 'sample': 92.88129140005913, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -23.220000000000066, 'blue_0': -38.120000000000275, 'blue_1': -38.34000000000029, 'red_0': -14.05999999999997}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 251.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1512000.0, 'blue_0': 1512000.0, 'blue_1': 1512000.0, 'red_0': 1512000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -23.220000000000066, 'blue_policy': -38.34000000000029}, 'num_module_steps_sampled_lifetime': {'red_policy': 3024000.0, 'blue_policy': 3024000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00036717018539708283, 'episode_return_mean': -113.7400000000006, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -84.70000000000016, 'episode_duration_sec_mean': 18.45579069997184, 'episode_return_min': -134.90000000000086, 'rlmodule_inference_timer': 0.014120128460737734, 'num_episodes_lifetime': 1260.0, 'episode_len_min': 1200, 'time_between_sampling': 298.23029039998073, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.320770500863391, 'throughput_since_last_restore': 15.780433753873357}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 922
(MultiAgentEnvRunner pid=37492) {'red_0': -26.800000000000047, 'red_1': -30.00000000000009, 'blue_0': -45.10000000000043, 'blue_1': -24.100000000000204}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 358
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1107
(MultiAgentEnvRunner pid=37492) {'red_0': -28.000000000000146, 'red_1': -38.60000000000024, 'blue_0': -26.500000000000096, 'blue_1': -39.70000000000027}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 817
(MultiAgentEnvRunner pid=37492) {'red_0': -17.09999999999997, 'red_1': -33.00000000000019, 'blue_0': -44.200000000000344, 'blue_1': -47.100000000000385}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 379
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_0 at STEP 794
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 803
(MultiAgentEnvRunner pid=37492) {'red_0': -60.70000000000054, 'red_1': -35.7000000000002, 'blue_0': -23.800000000000136, 'blue_1': -33.30000000000019}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 827
(MultiAgentEnvRunner pid=37492) {'red_0': -29.300000000000164, 'red_1': -12.799999999999972, 'blue_0': -42.700000000000315, 'blue_1': -56.30000000000051}
ITERATION 252: reward=-138.9600000000009, metadata={'num_env_steps_sampled_lifetime': 1518000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003363852977491032, 'timers': {'connectors': {'batch_individual_items': 0.00010065408473604677, 'add_states_from_episodes_to_batch': 6.582065862747461e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2662240691697765e-05, 'numpy_to_tensor': 6.905954796051255e-05, 'agent_to_module_mapping': 8.371834634295132e-06, 'add_observations_from_episodes_to_batch': 3.945444875870117e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008980732715744795, 'timers': {'connectors': {'get_actions': 0.0004630296021225063, 'un_batch_to_individual_items': 6.795475901869972e-05, 'tensor_to_numpy': 0.0001211103329679748, 'module_to_agent_unmapping': 6.622345008525265e-06, 'normalize_and_clip_actions': 7.440231506095357e-05, 'listify_data_for_vector_env': 2.4415712140699525e-05, 'remove_single_ts_time_rank_from_batch': 2.4092634835031267e-06}}}, 'sample': 92.14749320002738, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -30.020000000000145, 'blue_0': -36.460000000000264, 'blue_1': -40.10000000000031, 'red_0': -32.38000000000017}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 252.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1518000.0, 'blue_0': 1518000.0, 'blue_1': 1518000.0, 'red_0': 1518000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -30.020000000000145, 'blue_policy': -40.10000000000031}, 'num_module_steps_sampled_lifetime': {'red_policy': 3036000.0, 'blue_policy': 3036000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003415768906633878, 'episode_return_mean': -138.9600000000009, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -126.00000000000078, 'episode_duration_sec_mean': 18.282026440021582, 'episode_return_min': -153.50000000000108, 'rlmodule_inference_timer': 0.013614562856889681, 'num_episodes_lifetime': 1265.0, 'episode_len_min': 1200, 'time_between_sampling': 298.65357650001533, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.38168098189224, 'throughput_since_last_restore': 15.778816583929952}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -11.999999999999975, 'red_1': -6.499999999999993, 'blue_0': -55.300000000000495, 'blue_1': -44.70000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 695
(MultiAgentEnvRunner pid=37492) {'red_0': -27.500000000000153, 'red_1': -19.70000000000001, 'blue_0': -36.50000000000025, 'blue_1': -54.60000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 527
(MultiAgentEnvRunner pid=37492) {'red_0': -40.20000000000022, 'red_1': -39.50000000000021, 'blue_0': -24.000000000000036, 'blue_1': -29.900000000000137}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1169
(MultiAgentEnvRunner pid=37492) {'red_0': -16.199999999999978, 'red_1': -20.19999999999998, 'blue_0': -31.800000000000253, 'blue_1': -26.20000000000031}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 739
(MultiAgentEnvRunner pid=37492) {'red_0': -23.30000000000008, 'red_1': -49.200000000000344, 'blue_0': -33.900000000000276, 'blue_1': -20.100000000000055}
ITERATION 253: reward=-122.26000000000074, metadata={'num_env_steps_sampled_lifetime': 1524000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003278205130965644, 'timers': {'connectors': {'batch_individual_items': 9.854889636728268e-05, 'add_states_from_episodes_to_batch': 6.589880754987914e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2258664102015516e-05, 'numpy_to_tensor': 6.661596360479222e-05, 'agent_to_module_mapping': 8.052886571011697e-06, 'add_observations_from_episodes_to_batch': 3.858832471942825e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008689791925639754, 'timers': {'connectors': {'get_actions': 0.0004488163985231529, 'un_batch_to_individual_items': 6.608423338236179e-05, 'tensor_to_numpy': 0.00011765243029182984, 'module_to_agent_unmapping': 6.693565760536422e-06, 'normalize_and_clip_actions': 7.138967534511829e-05, 'listify_data_for_vector_env': 2.344639230032202e-05, 'remove_single_ts_time_rank_from_batch': 2.3174778933285306e-06}}}, 'sample': 92.40390209993348, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.020000000000106, 'blue_0': -36.30000000000026, 'blue_1': -35.100000000000264, 'red_0': -23.84000000000008}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 253.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1524000.0, 'blue_0': 1524000.0, 'blue_1': 1524000.0, 'red_0': 1524000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.020000000000106, 'blue_policy': -35.100000000000264}, 'num_module_steps_sampled_lifetime': {'red_policy': 3048000.0, 'blue_policy': 3048000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003443235002341295, 'episode_return_mean': -122.26000000000074, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -94.40000000000052, 'episode_duration_sec_mean': 18.351304640015588, 'episode_return_min': -138.3000000000009, 'rlmodule_inference_timer': 0.012935727600319797, 'num_episodes_lifetime': 1270.0, 'episode_len_min': 1200, 'time_between_sampling': 297.93033210001886, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.457678316777942, 'throughput_since_last_restore': 15.777525658182693}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 883
(MultiAgentEnvRunner pid=37492) {'red_0': -17.900000000000002, 'red_1': -19.1, 'blue_0': -37.10000000000033, 'blue_1': -27.200000000000284}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 727
(MultiAgentEnvRunner pid=37492) {'red_0': -17.399999999999977, 'red_1': -10.39999999999998, 'blue_0': -39.30000000000028, 'blue_1': -46.70000000000038}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1049
(MultiAgentEnvRunner pid=37492) {'red_0': -4.4, 'red_1': -21.300000000000036, 'blue_0': -49.20000000000044, 'blue_1': -45.40000000000037}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 252
(MultiAgentEnvRunner pid=37492) {'red_0': -24.00000000000008, 'red_1': -21.70000000000005, 'blue_0': -43.10000000000032, 'blue_1': -21.800000000000022}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -18.699999999999996, 'red_1': -25.500000000000092, 'blue_0': -54.70000000000049, 'blue_1': -39.40000000000029}
ITERATION 254: reward=-116.86000000000067, metadata={'num_env_steps_sampled_lifetime': 1530000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003355797088899631, 'timers': {'connectors': {'batch_individual_items': 9.908896547671322e-05, 'add_states_from_episodes_to_batch': 6.442779826126761e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2762008423479537e-05, 'numpy_to_tensor': 6.966365509318258e-05, 'agent_to_module_mapping': 8.286283722554531e-06, 'add_observations_from_episodes_to_batch': 3.8968782181283736e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008717657889105178, 'timers': {'connectors': {'get_actions': 0.0004502074439909115, 'un_batch_to_individual_items': 6.482328424539869e-05, 'tensor_to_numpy': 0.00011787942928639292, 'module_to_agent_unmapping': 6.494243511062618e-06, 'normalize_and_clip_actions': 7.137253106745346e-05, 'listify_data_for_vector_env': 2.3925264920333685e-05, 'remove_single_ts_time_rank_from_batch': 2.3103245911768153e-06}}}, 'sample': 90.93625730008353, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -19.600000000000033, 'blue_0': -44.68000000000037, 'blue_1': -36.100000000000264, 'red_0': -16.48000000000001}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 254.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1530000.0, 'blue_0': 1530000.0, 'blue_1': 1530000.0, 'red_0': 1530000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -19.600000000000033, 'blue_policy': -36.100000000000264}, 'num_module_steps_sampled_lifetime': {'red_policy': 3060000.0, 'blue_policy': 3060000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00033286583950846443, 'episode_return_mean': -116.86000000000067, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -101.30000000000062, 'episode_duration_sec_mean': 18.065872580045834, 'episode_return_min': -138.30000000000086, 'rlmodule_inference_timer': 0.012855445255482373, 'num_episodes_lifetime': 1275.0, 'episode_len_min': 1200, 'time_between_sampling': 295.75668039999437, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.375434350061962, 'throughput_since_last_restore': 15.775907181368362}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 937
(MultiAgentEnvRunner pid=37492) {'red_0': -37.900000000000205, 'red_1': -17.699999999999996, 'blue_0': -32.000000000000256, 'blue_1': -19.100000000000144}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 478
(MultiAgentEnvRunner pid=37492) {'red_0': -15.399999999999979, 'red_1': -13.099999999999987, 'blue_0': -39.30000000000029, 'blue_1': -20.299999999999986}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 391
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 587
(MultiAgentEnvRunner pid=37492) {'red_0': -30.00000000000018, 'red_1': -32.900000000000205, 'blue_0': -20.90000000000001, 'blue_1': -33.9000000000002}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -21.80000000000004, 'red_1': -34.500000000000206, 'blue_0': -40.1000000000003, 'blue_1': -50.90000000000043}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 365
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 1050
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 1115
(MultiAgentEnvRunner pid=37492) {'red_0': -44.1000000000003, 'red_1': -61.20000000000052, 'blue_0': -38.00000000000027, 'blue_1': -21.100000000000012}
ITERATION 255: reward=-124.84000000000069, metadata={'num_env_steps_sampled_lifetime': 1536000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032545182019456687, 'timers': {'connectors': {'batch_individual_items': 9.744237263598519e-05, 'add_states_from_episodes_to_batch': 6.291611821151151e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2211214133897704e-05, 'numpy_to_tensor': 6.79937660867229e-05, 'agent_to_module_mapping': 7.959027965924396e-06, 'add_observations_from_episodes_to_batch': 3.842021815355651e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008727411789391142, 'timers': {'connectors': {'get_actions': 0.0004517964393700262, 'un_batch_to_individual_items': 6.53845814533983e-05, 'tensor_to_numpy': 0.00011693448178731274, 'module_to_agent_unmapping': 6.344277810942567e-06, 'normalize_and_clip_actions': 7.089101237114035e-05, 'listify_data_for_vector_env': 2.5093679858581824e-05, 'remove_single_ts_time_rank_from_batch': 2.3145597907877735e-06}}}, 'sample': 91.89471270004287, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -31.880000000000184, 'blue_0': -34.06000000000023, 'blue_1': -29.060000000000155, 'red_0': -29.84000000000014}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 255.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1536000.0, 'blue_0': 1536000.0, 'blue_1': 1536000.0, 'red_0': 1536000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -31.880000000000184, 'blue_policy': -29.060000000000155}, 'num_module_steps_sampled_lifetime': {'red_policy': 3072000.0, 'blue_policy': 3072000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.0003436667114809479, 'episode_return_mean': -124.84000000000069, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -88.10000000000024, 'episode_duration_sec_mean': 18.235835040034726, 'episode_return_min': -164.4000000000011, 'rlmodule_inference_timer': 0.013227014534552764, 'num_episodes_lifetime': 1280.0, 'episode_len_min': 1200, 'time_between_sampling': 299.30027400003746, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.351872654756093, 'throughput_since_last_restore': 15.77420462308119}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 675
(MultiAgentEnvRunner pid=37492) {'red_0': -15.099999999999955, 'red_1': -8.599999999999985, 'blue_0': -35.00000000000023, 'blue_1': -45.70000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 631
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 799
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 852
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 852
(MultiAgentEnvRunner pid=37492) {'red_0': -63.40000000000057, 'red_1': -39.40000000000013, 'blue_0': -16.49999999999992, 'blue_1': -29.20000000000032}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_0 at STEP 298
(MultiAgentEnvRunner pid=37492) PICKED UP by red_1 at STEP 344
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by blue_0 at STEP 615
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 754
(MultiAgentEnvRunner pid=37492) {'red_0': -17.600000000000005, 'red_1': -15.599999999999952, 'blue_0': -42.90000000000038, 'blue_1': -25.700000000000273}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 247
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 916
(MultiAgentEnvRunner pid=37492) {'red_0': -19.100000000000012, 'red_1': -17.89999999999999, 'blue_0': -20.500000000000163, 'blue_1': -33.80000000000028}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 713
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 742
(MultiAgentEnvRunner pid=37492) {'red_0': -31.800000000000196, 'red_1': -23.40000000000008, 'blue_0': -26.60000000000016, 'blue_1': -21.700000000000166}
ITERATION 256: reward=-109.90000000000062, metadata={'num_env_steps_sampled_lifetime': 1542000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.00032633536328814173, 'timers': {'connectors': {'batch_individual_items': 9.757832306858021e-05, 'add_states_from_episodes_to_batch': 6.247455880851612e-06, 'add_time_dim_to_batch_and_zero_pad': 1.2152887447194122e-05, 'numpy_to_tensor': 6.695834062859723e-05, 'agent_to_module_mapping': 8.32721587116238e-06, 'add_observations_from_episodes_to_batch': 3.9361819128637706e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.000865010868963604, 'timers': {'connectors': {'get_actions': 0.0004443948761599645, 'un_batch_to_individual_items': 6.481699400225907e-05, 'tensor_to_numpy': 0.00011768137865267623, 'module_to_agent_unmapping': 6.328489640809017e-06, 'normalize_and_clip_actions': 7.107854598381163e-05, 'listify_data_for_vector_env': 2.4660868108823322e-05, 'remove_single_ts_time_rank_from_batch': 2.3268953558662935e-06}}}, 'sample': 92.31210059998557, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -20.980000000000025, 'blue_0': -28.30000000000017, 'blue_1': -31.220000000000276, 'red_0': -29.400000000000148}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 256.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1542000.0, 'blue_0': 1542000.0, 'blue_1': 1542000.0, 'red_0': 1542000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -20.980000000000025, 'blue_policy': -31.220000000000276}, 'num_module_steps_sampled_lifetime': {'red_policy': 3084000.0, 'blue_policy': 3084000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034026943835902256, 'episode_return_mean': -109.90000000000062, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -91.30000000000044, 'episode_duration_sec_mean': 18.337391800014302, 'episode_return_min': -148.50000000000094, 'rlmodule_inference_timer': 0.01287300856227221, 'num_episodes_lifetime': 1285.0, 'episode_len_min': 1200, 'time_between_sampling': 298.93881520000286, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 15.299228916950856, 'throughput_since_last_restore': 15.77229889315183}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 469
(MultiAgentEnvRunner pid=37492) {'red_0': -60.20000000000057, 'red_1': -38.80000000000028, 'blue_0': -28.00000000000009, 'blue_1': -33.20000000000018}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) {'red_0': -8.899999999999984, 'red_1': -7.39999999999999, 'blue_0': -42.20000000000032, 'blue_1': -40.8000000000003}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_1 at STEP 651
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_0 at STEP 705
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1025
(MultiAgentEnvRunner pid=37492) {'red_0': -26.900000000000038, 'red_1': -35.400000000000155, 'blue_0': -39.00000000000038, 'blue_1': -30.80000000000036}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 390
(MultiAgentEnvRunner pid=37492) {'red_0': -24.500000000000096, 'red_1': -28.600000000000158, 'blue_0': -37.60000000000023, 'blue_1': -18.89999999999998}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 830
(MultiAgentEnvRunner pid=37492) {'red_0': -32.40000000000012, 'red_1': -28.600000000000065, 'blue_0': -38.60000000000034, 'blue_1': -29.20000000000032}
ITERATION 257: reward=-126.0000000000008, metadata={'num_env_steps_sampled_lifetime': 1548000.0, 'env_to_module_connector': {'connector_pipeline_timer': 0.0003223237139703997, 'timers': {'connectors': {'batch_individual_items': 9.552150598635429e-05, 'add_states_from_episodes_to_batch': 6.314345697519828e-06, 'add_time_dim_to_batch_and_zero_pad': 1.3407883216983259e-05, 'numpy_to_tensor': 6.580060955278668e-05, 'agent_to_module_mapping': 7.99574394928404e-06, 'add_observations_from_episodes_to_batch': 3.8781125157559094e-05}}}, 'module_to_env_connector': {'connector_pipeline_timer': 0.0008726738273485211, 'timers': {'connectors': {'get_actions': 0.00045123387035396527, 'un_batch_to_individual_items': 6.455564154249798e-05, 'tensor_to_numpy': 0.00011815004708313058, 'module_to_agent_unmapping': 6.319841725824626e-06, 'normalize_and_clip_actions': 7.178393775824432e-05, 'listify_data_for_vector_env': 2.403098032663684e-05, 'remove_single_ts_time_rank_from_batch': 2.9719555900177484e-06}}}, 'sample': 92.59058880002704, 'num_agent_steps_sampled': {'blue_1': 6000, 'red_0': 6000, 'red_1': 6000, 'blue_0': 6000}, 'agent_episode_returns_mean': {'red_1': -27.760000000000126, 'blue_0': -37.08000000000027, 'blue_1': -30.58000000000023, 'red_0': -30.580000000000165}, 'episode_len_mean': 1200.0, 'agent_steps': {'red_1': 1200.0, 'blue_0': 1200.0, 'blue_1': 1200.0, 'red_0': 1200.0}, 'weights_seq_no': 257.0, 'num_module_steps_sampled': {'blue_policy': 12000, 'red_policy': 12000}, 'env_to_module_sum_episodes_length_in': 1089.0068085252615, 'num_agent_steps_sampled_lifetime': {'red_1': 1548000.0, 'blue_0': 1548000.0, 'blue_1': 1548000.0, 'red_0': 1548000.0}, 'connector_pipeline_timer': nan, 'env_reset_timer': nan, 'num_env_steps_sampled': 6000, 'module_episode_returns_mean': {'red_policy': -27.760000000000126, 'blue_policy': -30.58000000000023}, 'num_module_steps_sampled_lifetime': {'red_policy': 3096000.0, 'blue_policy': 3096000.0}, 'env_to_module_sum_episodes_length_out': 1089.0068085252615, 'timers': {'connectors': {'batch_individual_items': nan, 'add_states_from_episodes_to_batch': nan, 'add_time_dim_to_batch_and_zero_pad': nan, 'numpy_to_tensor': nan, 'agent_to_module_mapping': nan, 'add_observations_from_episodes_to_batch': nan}}, 'env_step_timer': 0.00034511103581976084, 'episode_return_mean': -126.0000000000008, 'num_episodes': 5, 'episode_len_max': 1200, 'episode_return_max': -99.30000000000061, 'episode_duration_sec_mean': 18.372705800016412, 'episode_return_min': -160.20000000000113, 'rlmodule_inference_timer': 0.01291179375309114, 'num_episodes_lifetime': 1290.0, 'episode_len_min': 1200, 'time_between_sampling': 299.86631109996233, 'num_env_steps_sampled_lifetime_throughput': {'throughput_since_last_reduce': 14.939925800158264, 'throughput_since_last_restore': 15.768893253746036}}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_1 -> red_0 at STEP 461
(MultiAgentEnvRunner pid=37492) {'red_0': -24.40000000000008, 'red_1': -40.40000000000029, 'blue_0': -35.500000000000234, 'blue_1': -38.40000000000026}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 305
(MultiAgentEnvRunner pid=37492) TAGGED red_0 -> blue_0 at STEP 634
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 773
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 773
(MultiAgentEnvRunner pid=37492) PICKED UP by red_0 at STEP 795
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1182
(MultiAgentEnvRunner pid=37492) {'red_0': -30.900000000000368, 'red_1': -22.900000000000194, 'blue_0': -41.90000000000034, 'blue_1': -44.90000000000029}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) TAGGED blue_0 -> red_1 at STEP 231
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 267
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 1026
(MultiAgentEnvRunner pid=37492) TAGGED red_1 -> blue_1 at STEP 1118
(MultiAgentEnvRunner pid=37492) FLAG RETURNED by red_1 at STEP 1118
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_0 at STEP 1120
(MultiAgentEnvRunner pid=37492) {'red_0': -62.000000000000405, 'red_1': -56.50000000000044, 'blue_0': -6.699999999999947, 'blue_1': -35.50000000000048}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 631
(MultiAgentEnvRunner pid=37492) {'red_0': -30.50000000000017, 'red_1': -34.90000000000021, 'blue_0': -34.20000000000029, 'blue_1': -28.500000000000107}
(MultiAgentEnvRunner pid=37492) RESET
(MultiAgentEnvRunner pid=37492) PICKED UP by blue_1 at STEP 475
(MultiAgentEnvRunner pid=37492) {'red_0': -23.700000000000077, 'red_1': -16.499999999999982, 'blue_0': -33.20000000000018, 'blue_1': -27.60000000000009}